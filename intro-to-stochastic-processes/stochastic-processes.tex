\input pdfToolbox

\setlayout{horizontal margin=1.5cm, vertical margin=1.5cm}
\parindent=0cm
\parskip=3pt plus 2pt minus 2pt

\input preamble

\footline={}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pageborder{rgb{1 .5 .5}}{rgb{1 0 0}}{5}

\color rgb{1 0 0}

{\def\boxshadowcolor{rgb{.8 0 0}}
\bppbox{rgb{1 .7 .7}}{rgb{1 0 0}}{rgb{.8 0 0}}

    \centerline{\setfontandscale{bf}{20pt}Introduction to Stochastic Processes}
    \smallskip
    \centerline{\setfont{it}Dr. Naomi Feldheim, \setfont{tt}naomi.feldheim@biu.ac.il}
    \centerline{\setfont{it}Summary by Ari Feiglin}

\eppbox

\bigskip

\bppbox{rgb{1 .7 .7}}{rgb{1 0 0}}{rgb{.8 0 0}}
\section*{Contents}

\tableofcontents
\eppbox

}

\vfill\break

\color{black}

\pageborder{rgb{1 .5 .5}}{rgb{1 0 0}}{5}
\null
\vfill\break

\pageno=1
\newif\ifpageodd
\pageoddtrue
\headline={%
    \hbox to \hsize{\color{black}%
        \ifpageodd\hfil{\it\currsubsection\quad\bf\folio}\global\pageoddfalse%
        \else{\bf\folio\quad\it\currsubsection}\hfil\global\pageoddtrue\fi%
    }%
}

\section{Introduction}

This course will focus on tools which can be used to study random processes.
A random process is a sequence of random variables which represent measurements of the process.
Examples of random processes are random walks (these are commonly described as the path a drunk man would take while trying to get home), card shuffles (which can be viewed as choosing a card and placing
it randomly in the deck), and branching (for example the population of bunnies in a specific area: the random variable being the number of bunnies in each generation).

\section{Markov Chains}

\bdefn

    A {\emphcolor discrete-time Markov process}\addtoindex{markov process}[discrete-time] is a sequence of random variables $\set{X_n}_{n\geq0}$.
    This sequence is called a {\emphcolor Markov chain}\addtoindex{markov chain} on a set of states $S$ if:
    \benum
        \item For every $n$, $X_n\in S$ almost surely (meaning $\probof{X_n\in S}=1$),
        \item For every $n\geq0$ and for every $s_0,\dots,s_{n+1}\in S$,
        $$\probof{X_{n+1}=s_{n+1}}[X_0=s_0,\dots,X_n=s_n]=\probof{X_{n+1}=s_{n+1}}[X_n=s_n]$$
        ie. the probability of the next measurement being some arbitrary value is dependent only on the previous measurement.
        This is only necessary if $\probof{X_0=s_0,\dots,X_n=s_n}>0$.
    \eenum

\edefn

In this course $S$ will always be countable.
We can also write the second condition using distributive equivalence:
$$ X_{n+1}\vert X_0,\dots,X_n \deq X_{n+1}\vert X_n $$

Notice how the Markov property can be strengthened in various ways, for example if $n>m$ then

\medskip
{\tabskip=0pt
\openup1\jot\halign to\hsize{$\displaystyle#$\hfil\tabskip=0pt plus 1fil&$\displaystyle#$\hfil\tabskip=0pt\cr
    \omit\span\omit$\displaystyle\probof{X_n=s_n}[X_{n-1}=s_{n-1},\dots,X_m=s_m]$\cr
    &= \sum_{s_m,\dots,s_0}\probof{X_n=s_n}[X_{n-1}=s_{n-1},\dots,X_0=s_0]\cdot\probof{X_{m-1}=s_{m-1},\dots,X_0=s_0}[X_{n-1}=s_{n-1},\dots,X_m=s_m]\cr
    &= \probof{X_n=s_n}[X_{n-1}=s_{n-1}]\cdot\sum\probof{X_{m-1}=s_{m-1},\dots,X_0=s_0}[X_{n-1}=s_{n-1},\dots,X_m=s_m]\cr
    &= \probof{X_n=s_n}[X_{n-1}=s_{n-1}]\cr
}}
\medskip

This can be viewed as the base case for
$$ \probof{X_{n+k}=s_{n+k}}[X_n=s_n,\dots,X_m=s_m] = \probof{X_{n+k}=s_{n+k}}[X_n=s_n,\dots,X_{m'}=s_{m'}] $$
where $m'<m$.
This is since for $k=1$, both of these are equal to $\probof{X_{n+1}=s_{n+1}}[X_n=s_n]$.
The induction step follows by

\medskip
{\tabskip=0pt
\openup1\jot\halign to\hsize{$\displaystyle#$\hfil\tabskip=0pt plus 1fil&$\displaystyle#$\hfil\tabskip=0pt\cr
    \omit\span\omit$\displaystyle\probof{X_{n+k+1}=s_{n+k+1}}[X_n=s_n,\dots,X_m=s_m]$\cr
    &= \sum_{s_{n+1}}\probof{X_{n+k+1}=s_{n+k+1}}[X_{n+1}=s_{n+1},\dots,X_m=s_m]\cdot\probof{X_{n+1}=s_{n+1}}[X_n=s_n,\dots,X_m=s_m]\cr
    &= \sum_{s_{n+1}}\probof{X_{n+k+1}=s_{n+k+1}}[X_{n+1}=s_{n+1},\dots,X_{m'}=s_{m'}]\cdot\probof{X_{n+1}=s_{n+1}}[X_n=s_n,\dots,X_{m'}=s_{m'}]\cr
    &= \probof{X_{n+k+1}=s_{n+k+1}}[X_n=s_n,\dots,X_{m'}=s_{m'}]\cr
}}
\medskip

By taking $m'=0$ and $m=n$ we get $\probof{X_{n+k}=s_{n+k}}[X_n=s_n] = \probof{X_{n+k}=s_{n+k}}[X_n=s_n,\dots,X_0=s_0]$, or in other words for all $m<n$,
$$ \probof{X_n=s_n}[X_m=s_m,\dots,X_0=s_0] = \probof{X_n=s_n}[X_m=s_m] $$

This can be even further strengthened: let $\varnothing\neq B\subseteq\set{0,\dots,n-1}$ and $m=\max B$ then
$$ \probof{X_n=s_n}[\forall i\in B\colon X_i=s_i] = \probof{X_n=s_n}[X_m=s_m] $$
To prove this let $C=\set{0,\dots,m}\setminus B$ then
$$ \eqalign{
    \probof{X_n=s_n}[\forall i\in B\colon X_i=s_i] &= \sum_{(s_i)_{i\in C}\in S^C}\probof{X_n=s_n}[X_m=s_m,\dots,X_0=s_0]\cdot\probof{\forall i\in C\colon X_i=s_i}[\forall i\in B\colon X_i=s_i]\cr
    &= \probof{X_n=s_n}[X_m=s_m]\cdot\sum\probof{\forall i\in C\colon X_i=s_i}[\forall i\in B\colon X_i=s_i]\cr
    &= \probof{X_n=s_n}[X_m=s_m]\cr
} $$

A consequence of this is that if $\set{X_n}_{n\geq0}$ is a Markov chain and $\set{a_n}_{n\geq0}$ is strictly monotonic then $Y_n=X_{a_n}$ is also a Markov chain.
After all if we let $B=\set{a_{n-1},\dots,a_0}$ then $\max B=a_{n-1}$ and so
\multlines{
    \probof{Y_n=s_{a_n}}[Y_{n-1}=s_{a_{n-1}},\dots,Y_0=s_{a_0}] = \probof{X_{a_n}=s_{a_n}}[\forall i\in B\colon X_i=s_i] = \probof{X_{a_n}=s_{a_n}}[X_{a_{n-1}}=s_{a_{n-1}}]\cr
    &= \probof{Y_n=s_{a_n}}[Y_{n-1}=s_{a_{n-1}}]
}
as required.

\bdefn

    For a Markov chain $\set{X_n}_{n\geq0}$ on a finite set of states $S$, we define the {\emphcolor adjacency matrix} at the $n$th measurement by
    $$ P^{(n)}_{ij} = \probof{X_n=j}[X_{n-1}=i] $$
    for $i,j\in S$.
    This is also sometimes written as $P_n(i\to j)$ (the probability measuring $i$ on the $n-1$th measurement gives $j$ on the next).
    If $P^{(n)}$ is the same for all $n$, then we say that the chain is {\emphcolor homogeneus in time}, and we generally write $P$ in place of $P^{(n)}$.

\edefn

For example, suppose a frog is hopping between $N$ leaves.
The frog can hopping from every leaf to every other leaf, and it always chooses a leaf in an independent and uniform manner.
This defines a Markov chain where the states are the leaves, and $X_n$ is the leaf the frog is on after $n$ hops.
This Markov chain is even homogeneus since the frog makes its choices in a manner which does not take the current number of hops into account.
The adjacency matrix is defined by
$$ P_{ij} = \cases{\frac1{N-1} & $i\neq j$\cr 0 & $i=j$} $$
This is the simple random process on the complete graph of $N$ vertices, $K_N$.

Suppose $N=4$, and supppose that at the beginning the frog is on either the first or second leaf with equal probability.
What is the probability that after one hop the frog is on the fourth leaf?
The following notation will be used: $X\sim(a_0,\dots,a_n)$ will be used to mean $\probof{X=s_i}=a_i$, where $s_i$ is some understood ordering of the set of states $S$.
Then
$$ \probof{X_1=j}[X_0\sim\parens{\frac12,\frac12,0,0}] = \probof{X_1=j}[X_0=1]\cdot\frac12 + \probof{X_1=j}[X_0=2]\cdot\frac12 $$
as the rest of the terms are zero.
For $j=4$ we get that this is equal to $\frac13$.
Notice that we can generalize this and get
$$ \probof{X_{n+1}=j}[X_n\sim\vec v] = \sum_{i\in S}\probof{X_{n+1}=j}[X_n=i]\cdot\probof{X_n=i} = \sum_{i\in S}P^{(n+1)}_{ij}\vec v_i = (\vec v\cdot P^{(n+1)})_j $$
So we have proven the following:

\bprop

    If $X_n\sim\vec v$ then $X_{n+1}\vert X_n\sim\vec v\cdot P^{(n+1)}$, and so $X_n\vert X_0\sim\vec v\cdot P^{(n)}\cdots P^{(1)}$.
    In particular if the Markov chain is homogeneus, $X_n\vert X_0\sim\vec v\cdot P^n$.

\eprop

This simplifies dealing with Markov chains, especially homogeneus ones.

\bexam

    Suppose $\set{Y_n}_{n=1}^\infty$ is a sequence of random variables which have the distribution $Y_n\sim\Berof{\frac1n}$ (recall that $X\sim\Berof p$ means that $X$ is $1$ with probability $p$ and zero
    otherwise).
    And we define $X_n=\chi\set{(\exists m\leq n)\,Y_m=1}$, the indicator of the set of all values such that there is an index before $n$ where $Y_m=1$ ($\chi_S$ is the {\it indicator function} of the set
    $S$, defined by $\chi_S(x)=1$ for $x\in S$ and zero otherwise).
    We will prove $X_n$ is a Markov chain.
    Notice that
    $$ X_n = \chi\set{(\exists m\leq n)\,Y_m=1} = \chi\set{(\exists m\leq n-1)\,Y_m=1}\lor\chi\set{Y_n=1} = X_{n-1}\lor\chi\set{Y_n=1} $$
    $\lor$ is bitwise or, or equivalently the maximum.
    And therefore we get that $X_n=\bigvee_{i=1}^n\chi\set{Y_i=1}$.
    This means that if $X_{n-1}=1$ then $X_n=1$, and if $X_{n-1}=0$ then $X_n=1$ if and only if $Y_n=1$.
    And so $X_n$'s value depends only on $X_{n-1}$'s and not any previous $X_i$.
    So $\set{X_n}_{n=1}^\infty$ is indeed a Markov chain.

    Notice that
    $$ \displaylines{
        \probof{X_n=0}[X_{n-1}=0] = \probof{Y_n=0} = \frac{n-1}n,\quad \probof{X_n=1}[X_{n-1}=0] = \probof{Y_n=1} = \frac1n,\quad\cr
        \probof{X_n=0}[X_{n-1}=1] = 0,\quad \probof{X_n=1}[X_{n-1}=1] = 1
    } $$
    And so we get that
    $$ P^{(n)} = \pmatrix{\frac{n-1}n & \frac1n \cr 0 & 1} $$

\eexam

\bdefn

    A real $n\times n$ matrix $P$ such that $P_{ij}\geq0$ for every $i,j$, and for every row $i$ we have $\sum_{j=1}^n P_{ij}=1$ then $P$ is called an {\emphcolor stochastic matrix}.

\edefn

Notice that we can draw a diagram for every stochastic matrix and it will be the transition matrix of a Markov chain.
Meaning every stochastic matrix is the transition matrix of some Markov chain, and every transition matrix is stochastic.
Notice that the second condition for a matrix to be stochastic can be written as $P{\bf1}={\bf1}$ where ${\bf1}=(1,\dots,1)^\top$.

\subsection{Hitting Times and Classifying States}

\bdefn

    Let $\set{X_n}_{n\geq0}$ be a Markov chain over a state space $S$, and let $A\subseteq S$.
    Then we define the {\emphcolor hitting time} to $A$ to be the random variable
    $$ T_A = \minof{t\geq1}[X_t\in A] $$
    Note that if $X_t$ is never in $A$ then $T_A$ can be $\infty$, and so $T_A$ is a function from the probability space to the extended reals: $\Omega\longto{\bb R}\cup\set\infty$.
    This means that $T_A^{-1}\set\infty$ must also be measurable (an event).

\edefn

In the case that $A$ is a singleton $A=\set a$ then we write $T_a$ in place of $T_A$.
Notice that $T_A$ measures starting from $t=1$, while it is possible that the initial condition is in $A$, ie. $X_0\in A$.
So in the case that $X_0\in A$, $T_A$ measures the {\it return time} to $A$, in particular if $X_0\sim\delta_a$ where $\delta_a=(0,\dots,1,\dots,0)$ ($1$ is at the index corresponding to the state $a$).
We also use the following notation
$$ \probof[V]E = \probof E[X_0\sim V],\qquad \probof[\delta_a]E = \probof[a]E = \probof{E}[X_0=a] $$

If $P$ is the transition matrix of a homogeneus Markov chain, then $P^n(a\to b)$ means $P^n_{ba}=\probof{X_n=b}[X_0=a]$.

\blemm

    If $\set{X_n}$ is a homogeneus Markov chain, then
    $$ P^n(a\to b)=\sum_{m=1}^n\probof[a]{T_b=m}P^{n-m}(b\to b) $$

\elemm

\multlines{%
    P^n(a\to b) = \probof[a]{X_n=b} = \probof{\bigdcup_{m=1}^n\set{T_b=m},X_n=b}[X_0=b] = \sum_{m=1}\probof{T_b=m,X_n=b}[X_0=b]\cr
    &= \sum_{m=1}^n\probof{X_n=b}[T_b=m,X_0=a]\cdot\probof{T_b=m}[X_0=a]\cr
}
Now, $\probof{X_n=b}[T_b=m,X_0=a]=\probof{X_n=b}[X_m=b,X_{m-1}\neq b,\dots,X_1\neq b,X_0=a]=\probof{X_n=b}[X_m=b]$ by the Markov property.
Since $\set{X_n}$ is homogeneus this is just equal to $P^{n-m}(b\to b)$.
Thus this formula is equal to
$$ \sum_{m=1}^b\probof{X_n=b}[X_m=b]\cdot\probof[a]{T_b=m} = \sum_{m=1}^b P^{n-m}(b\to b)\cdot\probof[a]{T_b=m} \qed $$

Let us introduce some more notation:
$$ f_{a\to b} = \probof{T_b<\infty}[X_0=a],\qquad f_{a\to a}=f_a=\probof{T_a<\infty}[X_0=a] $$
thus $f_{a\to b}$ is the probability that if we start at $a$, we eventually reach $b$.

\blemm

    $f_{a\to c}\geq f_{a\to b}\cdot f_{b\to c}$

\elemm

Notice that $\set{T_c<\infty}=\set{(\exists t>0)X_t=c}\supseteq\bigdcup_{k>0}\set{T_b=k,(\exists t>k)X_t=c}$.
Thus we get
\lmultlines{
    f_{a\to c} = \probof{T_c<\infty}[X_0=a] \geq \sum_{k=1}^\infty\probof{T_b=k,(\exists t>k)X_t=c}[X_0=a]\cr
    &= \sum_{k=1}^\infty\probof{T_b=k}[X_0=a]\cdot\probof{(\exists t>k)X_t=c}[T_b=k,X_0=a]\cr
    &= \sum_{k=1}^\infty\probof{T_b=k}[X_0=a]\cdot\probof{(\exists t>k)X_t=c}[X_k=b,X_{k-1}\neq b,\dots,X_1\neq b,X_0=a]\cr
    &\llap{(Markov property) }= \sum_{k=1}^\infty\probof{T_b=k}[X_0=a]\cdot\probof{(\exists t>k)X_t=c}[X_k=b]\cr
    &\llap{(homogeneity) }= \sum_{k=1}^\infty\probof{T_b=k}[X_0=a]\cdot\probof{(\exists t>0)X_t=c}[X_0=b]\cr
    &= \sum_{k=1}^\infty\probof{T_b=k}[X_0=a]\cdot f_{b\to c} = f_{a\to b}\cdot f_{b\to c}\hfill\mathchar"404\cr
}

In particular this means
$$ f_a\geq f_{a\to b}\cdot f_{b\to a} $$

For every $a\in S$ we define the random variable $N(a)=\sum_{n=1}^\infty\chi\set{X_n=a}$, which is the number of times the state $a$ is visited from time $1$ and onward.
When $X_0\sim V$ we write $N_V(a)$.
Notice then that $f_{a\to b}=\probof{N(b)\geq1}[X_0=a]$ and so $f_a=\probof{N(a)\geq1}[X_0=a]$.

\bprop

    $\probof{N(a)\geq k}[X_0=a]=f_a^k$

\eprop

We prove this by induction, for $k=1$ this is simply what we just said.
Now
\lmultlines{
    \probof{N(a)\geq k+1}[X_0=a] = \sum_{m=1}^\infty\probof{T_a=m,\abs{\set{j>m}[X_j=a]}\geq k}[X_0=a]\cr
    &\llap{(Markov property) }= \sum_{m=1}^\infty\probof{T_a=m}[X_0=a]\cdot\probof{\abs{\set{j>m}[X_j=a]}\geq k}[X_m=a]\cr
    &\llap{(homogeneity) }=\sum_{m=1}^\infty\probof{T_a=m}[X_0=a]\cdot\probof[a]{N(a)\geq k}\cr
    &\llap{(induction) }=f_a^k\sum_{m=1}^\infty\probof{T_a=m}[X_0=a] = f_a^{k+1}\hfill\mathchar"404\cr
}

Notice then that
$$ \probof{N(a)=k}[X_0=a] = \probof[a]{N(a)\geq k} - \probof[a]{N(a)\geq k+1} = f_a^k - f_a^{k+1} = f_a^k(1-f_a) $$
Thus $N_a(a)\sim\Geoof{1-f_a}-1$ (the $+1$ is since $X\sim\Geoof p$ means $\probof{X=k}=p(1-p)^{k-1}$).
Thus
$$ \expecof{N_a(a)} = \frac1{1-f_a} - 1 = \frac{f_a}{1-f_a} $$

\bdefn

    A state $b\in S$ is {\emphcolor recurrent} if $f_b=1$, equivalently if $\probof[b]{T_b<\infty}$ (the probability of returning to $b$ is $1$).
    A non-recurrent state is called {\emphcolor transient}.
    $b$ is {\emphcolor absorbing} if $P(b\to b)=1$.

\edefn

Notice that if $b$ is recurrent then if $f_b=1$, $N_b(b)\sim\Geoof{0}-1$, meaning $\probof[b]{N(b)=\infty}=1$.
And if $b$ is transient then $N_b(b)$ is a finite geometric variable and so $\probof[b]{N(b)<\infty}=1$.
And so
$$ \eqalign{
    \hbox{$b$ is recurrent} &\iff \probof{N(b)=\infty}[X_0=b] = 1,\cr
    \hbox{$b$ is transient} &\iff \probof{N(b)<\infty}[X_0=b] = 1 \iff \probof{N(b)<\infty}[X_0\sim v] = 1
} $$

\bdefn

    Let $a,b\in S$ be states.
    Then $b$ is {\emphcolor reachable} from $a$ if $f_{a\to b}\neq0$ or $a=b$, this is denoted $a\to b$.
    $a$ and $b$ are {\emphcolor connected} if both $a\to b$ and $b\to a$, this is denoted $a\oto b$.

\edefn

This means that $a\to b$ if and only if there exists some $n\geq0$ such that $P^n(a\to b)>0$.
Furthermore, connectivity is an equivalence relation: it is obviously reflexive and symmetric and if $a\to b$ and $b\to c$, since $f_{a\to c}\geq f_{a\to b}\cdot f_{b\to c}>0$, we get that reachability
and therefore connectivity is transitive.
Thus $S$ can be partitioned into {\it connectivity classes}.

\blemm

    If $a\to b$ and $a\neq b$ then $\probof{T_b<T_a}[X_0=a]>0$.

\elemm

Since $a\to b$, there exists a sequence of states $a=s_0,\dots,s_m=b$ such that $P_{s_is_{i+1}}>0$ for all $i$.
We can assume that for every $i>0$, $a\neq s_i$.
So we have a sequence whose probability is positive and where the hitting time of $b$ is before that of $a$, so the probability that $T_b<T_a$ must be positive.
\qed

\bdefn

    $A\subseteq S$ is {\emphcolor closed} if for every $a\in A$ and every $b\notin A$, $b$ is not reachable from $a$.
    $A$ is also called {\emphcolor irreducible} if it is closed and connected.

\edefn

\bthrm

    If $a$ is recurrent and $a\to b$, then also $b\to a$ and $b$ is recurrent.

\ethrm

We know
$$ f_{a\to b} = \probof[a]{T_a>T_b} + \probof[a]{T_a<T_b}\cdot\probof{T_b<\infty}[T_a<T_b] $$
by the above lemma $p=\probof[a]{T_b<T_a}>0$ and so by homogeneity
$$ = p + (1-p)\cdot\probof{T_b<\infty}[X_0=a] = p + (1-p)f_{a\to b} $$
Thus we get that $p\cdot f_{a\to b}=p$ and since $p\neq0$, $f_{a\to b}=1$.
Now
$$ f_{a\to b}(1-f_{b\to a}) = \probof{\hbox{$X_n$ hits $b$ and never returns to $a$}}[X_0=a] \leq \probof[a]{N(a)<\infty} = 0 $$
Thus $f_{b\to a}=1$.
Now $f_b\geq f_{b\to a}\cdot f_{a\to b}=1$ so $b$ is also recurrent.
\qed

So if $a\oto b$, then $a$ is recurrent if and only if $b$ is.
If $b$ is reachable from $a$ but $a$ is not reachable from $b$, then $a$ is transient.
And if $a$ is recurrent and $a\to b$ then $\probof[b]{N(a)=\infty}=1$.

\bthrm

    A finite closed set of states $A\subseteq S$ contains a recurrent state.

\ethrm

Suppose $A$ has only transient states.
This means that $\probof[v]{N(a)<\infty}=1$ for every $a\in A$, and so we get that $\probof[v]{(\forall a\in A)N(a)<\infty}=1$ (as the intersection of a countable number of events with probability one).
And this means $\probof[v]{\sum_{a\in A}N(a)<\infty}=1$ since $A$ is finite.
But since $A$ is closed, we can never leave $A$ and so if $v$'s support is in $A$ then $\sum_{a\in A}N_v(a)=\infty$.
\qed

In particular, since $S$ is closed, if $S$ is finite it contains a recurrent state.

\bthrm

    If $S$ is a finite state space, then it can be uniquely partitioned into
    $$ S = T\dcup C_1\dcup\cdots\dcup C_k $$
    where $T$ is the set of all transient states, and $C_i$ are all disjoint irreducible (closed and connected) sets.

\ethrm

So $T$ is the set of all transient states, and for every recurrent state $a\in S\setminus T$ let $C_a=\set{b}[a\to b]$.
By a previous theorem, for every $b\in C_a$, $b\to a$ so and if $b\to b'$ then $a\to b'$ meaning $b'\in C_a$, so $C_a$ is closed.
And if $b,b'\in C_a$ then $a\to b$ and $a\to b'\implies b'\to a$ and therefore $b'\to b$, so $C_a$ is connected and therefore irreducible.
By taking representatives of each $C_a$, let $C_i=C_{a_i}$, we get the partition.

This partition is unique: since if $C_1\dcup\cdots\dcup C_k=C'_1\dcup\cdots\dcup C'_m$ let $a\in C_1$ then $a\in C'_i$ for some $i$, without loss of generality assume $a\in C'_1$.
Then for every $b\in C_1$, since $C_1$ is connected $a\to b$ and so $b\in C'_1$ since $C'_1$ is closed, thus $C_1=C'_1$.
Continuing inductively we get $k=m$ and $C_i=C'_i$ as required.
\qed

\bexam

    Suppose Elise is in a room $0$, and can either stay in the room with probability $1-p_1-p_2$, go to room $1$ with probability $p_1$ or go to room $2$ with probability $p_2$.
    If she goes to a new room, she stays there forever.
    Knowing that ends up in room $2$, what is the expected amount of time she spends waiting in room $0$?

    So we want to find the expected value of $N_0(0)$ knowing that $T_2<\infty$.
    So we will compute
    $$ \probof{N_0(0)=k}[T_2<\infty] = \frac{\probof{N_0(0)=k,T_2<\infty}}{\probof{T_2<\infty}} = \frac{\probof{X_1=\cdots=X_k=0,X_{k+1}=2}}{\probof{T_2<\infty}} $$
    Now, utilizing conditional probability and the Markov property (this is all done under the assumption $X_0=0$),
    $$ \probof{X_1=\cdots=X_k=0,X_{k+1}=2} = \probof{X_{k+1}=2}[X_k=0]\cdot\probof{X_k=0}[X_{k-1}=0]\cdots\probof{X_1=0} = p_2\cdot(1-p_1-p_2)^k $$
    And $\probof{T_2<\infty}=\frac{p_2}{p_1+p_2}$ since to get to room $2$ we must visit room $0$ an arbitrary number of times, and then go to room $2$, so
    $$ \probof{T_2<\infty} = \sum_{n=0}^\infty p_2\cdot(1-p_1-p_2)^n = \frac{p_2}{p_1+p_2} $$
    Thus
    $$ \probof{N_0(0)=k}[T_2<\infty] = (p_1+p_2)\cdot(1-p_1-p_2)^k $$
    Which means that
    $$ (N_0(0)\,|\,T_2<\infty) \sim \Geoof{p_1+p_2} - 1 \implies \expecof{N_0(0)}[T_2<\infty] = \frac{1-p_1-p_2}{p_1+p_2} $$

    Notice two things: firstly, by symmetry this means that $(N_0(0)\,|\,T_1<\infty)\sim\Geoof{p_1+p_2}-1$ which is the same distribution.
    And secondly, this is the same distribution as $N_0(0)$, so the expected time Elise waits at room $0$ does not change if we know which room she ends up in.

\eexam

\bdefn

    Let $a\in S\setminus T$ be a recurrent state, then we define its {\emphcolor period} to be
    $$ d(a) = \gcd\set{n\geq 1}[P^n(a\to a)>0] $$
    An irreducible Markov chain is called {\emphcolor periodic} if every state is recurrent and has the same period greater than $1$, which is the {\emphcolor period} of the Markov chain.

\edefn

Notice that if $P(a\to a)>0$ then $d(a)=1$, and so a periodic chain can be made aperiodic by adding a self-edge whose probability is nonzero.

\bprop

    If $P$ is the transition matrix of some periodic chain with a period of $d$, then $P^d$ is reducible.

\eprop

\bprop

    If the Markov chain is irreducible then every state has the same period.

\eprop

Let $P$ be the transition matrix of the chain.
Since $x\oto y$, there exist natural $r,\ell$ such that $P^r(x,y),P^\ell(y,x)>0$.
So let $m=r+\ell$ and so
$$ P^m(x,x) \geq P^r(x,y)\cdot P^\ell(y,x) > 0,\qquad P^m(y,y) \geq P^\ell(y,x)\cdot P^r(x,y) > 0 $$
So let $\tau(a)=\set{n\geq1}[P^n(a,a)>0]$, and by above we have shown that $m\in\tau(x)\cap\tau(y)$.
Now for every $n\in\tau(x)$ we have that $P^{\ell+n+r}(y,y)\geq P^\ell(y,x)P^n(x,x)P^r(x,y)>0$ and so $n+m\in\tau(y)$.
Thus $m+\tau(x)\subseteq\tau(y)$.
By definition we have $d(y)=\gcd(\tau(y))$ and since $m\in\tau(y)$ we have $d(y)\divides m$ and since $m+\tau(x)\subseteq\tau(y)$ we must have that $d(y)\divides\tau(x)$.
Thus $d(y)\divides d(x)$, and since $x,y$ are arbitrary we get $d(x)\divides d(y)$ and so $d(x)=d(y)$ as required.
\qed

This means that every irreducible Markov chain has a period, and if the period is $>1$, it is periodic.
So in order for an irreducible Markov chain to be periodic, it is sufficient for there to exist a state $a$ with $d(a)>1$.

A common Markov chain is a random walk on ${\bb Z}$, where
$$ P(i,i+1) = p,\quad P(i,i-1) = 1-p,\quad P(i,j)=0\hbox{ for $j\notin\set{i\pm 1}$} $$
Another way of representing $X_n$ is by $X_n=\sum_{k=1}^nB_k$ where $B_k=1$ with probability $p$ and $B_k=-1$ with probability $1-p$.
$\set{B_k}$ is independent.
If $p=\frac12$, the walk is called {\it fair}.

\bthrm

    If $p\neq\frac12$, every state in ${\bb Z}$ is transient.

\ethrm

Since all the states are connected, it is sufficient to show that $0$ is transient.
So we set $X_0=0$ and notice that $\frac{B_k+1}2\sim\Berof p$ and thus $\frac{X_n+n}2\sim\Binof{n,p}$ thus
$$ \probof{X_{2n}=0} = \probof{\frac{X_{2n}+2n}2=n} = {2n\choose n}p^n(1-p)^n $$
and $\probof{X_{2n+1}=0}=\probof{\frac{X_{2n+1}+2n+1}2=n+\frac12}=0$ since binomial distributions take on only integer values.
By Stirling's approximation: $k!\in\Theta(k^{k+1/2}e^{-k})$, we get that there exists some $c>0$ such that
$$ \probof{X_{2n}=0} = \frac{(2n)!}{n!n!}p^n(1-p)^n \leq cp^n(1-p)^n\frac{(2n)^{2n+1/2}e^{-2n}}{n^{2n+1}e^{-2n}} = cp^n(1-p)^n\frac{2^{2n+1/2}}{\sqrt n} = c'\frac{\bigl(4p(1-p)\bigr)^n}{\sqrt n} $$
This can be bound by a $q^n$ where $q\in[0,1)$, since $4p(1-p)<1$ for $p\neq\frac12$.
Thus we get that $\sum_{k=1}^\infty\probof{X_k=0}[X_0=0]$ and so by Borel-Cantelli we then get that $\probof{X_k=0\ \hbox{i.o.}}[X_0=0]=0$, meaning that the probability $X_k=0$ an infinite number of times
is zero.
Thus $\probof{N(0)=\infty}[X_0=0]=0$, and so this means $0$ is transient as required.
\qed

If we have a Markov chain, and $A\subseteq S$, we can ask questions about hitting times in $A$ by removing all the states in $A$ and adding a new state $\hat A$.
This can only be done if for every $a,a'\in A$ and $b\notin A$, $P(a\to b)=P(a'\to b)$, and we define that the probability $P(\hat A\to b)=P(a\to b)$.
And $P(b\to\hat A)=\sum_{a\in A}P(b\to a)$.
In particular this can be done if $A$ is closed.

\bexam

    Suppose we have the following Markov chain:
    $$ P = \pmatrix{1 & 0 & 0\cr p & q & r\cr 0 & 0 & 1} $$
    where $p,q,r\geq0$ and $p+q+r=1$.
    If we know that $X_0=2$, what is the probability that the chain will be absorbed into $1$ or $3$?

    Let us define
    $$ \ell_j = \probof{T_1<\infty}[X_0=j] $$
    since $1$ and $3$ are absorbing states, $\ell_1=1$ and $\ell_3=0$.
    Now, we want to compute $\ell_2$:
    \multlines{
        \ell_2 = \probof{T_1<\infty}[X_0=2] = \sum_{j=1}^3\probof{T_1<\infty}[X_1=j,X_0=2]\cdot\probof{X_1=j}[X_0=2]\cr\noalign{\kern-.4cm}
        &= \sum_{j=1}^3\probof{T_1<\infty}[X_1=j]\cdot P_{2j}
    }
    where the last step is due to homogeneity.
    This is equal to $\sum_{j=1}^3\ell_jP_{2j}=\ell_1p+\ell_2q+\ell_3r=p+\ell_2r$.
    Thus we get that $\ell_2=p+\ell_2r$ and so $\ell_2=\frac p{1-r}$.
    Thus the probability that starting from $X_0=2$ we are absorbed into $1$ (meaning $T_1=\infty$) is $1-\ell_2=\frac q{1-r}$.
    Since $2$ is transient, we are either absorbed into $1$ or $3$, so the probability of being absorbed into $3$ is $\frac p{1-r}$.

    Let us now ask what the expected time until being absorbed is.
    By the law of total expectation:
    Now, $\expecof{T_{\set{1,3}}}[X_1=2]=\expecof{T_{\set{1,3}}}[X_0=2]+1$ since it takes one more step, and so
    $$ = \bigl(1+\expecof{T_{\set{1,3}}}[X_0=2]\bigr)\cdot\probof[2]{X_1=2} + \probof[2]{X_1=1} + \probof[2]{X_1=3} $$
    So let $x=\expecof{T_{\set{1,3}}}[X_0=2]$, we get
    $$ x = (1+x)r + p + q = (1+x)r + (1-r) \implies x = \frac1{1-r} $$

\eexam

\bexam

    Suppose we have the following Markov chain:
    $$ P = \pmatrix{1 & 0 & 0 & 0 & 0 & 0\cr 0 & 1 & 0 & 0 & 0 & 0\cr a_1 & a_2 & 0 & a_4 & 0 & 0\cr 0 & 0 & b_3 & 0 & b_5 & b_6\cr 0 & 0 & 0 & 0 & 1 & 0\cr 0 & 0 & 0 & 0 & 0 & 1} $$
    What is the probability of being absorbed into one of the absorbing states ($1,2,5,6$) if it starts on one of the non-absorbing states ($3,4$)?

    Let us define $\ell_{m,k}=\probof[m]{T_k<\infty}$.
    Now, let us notice that
    $$ \ell_{m,k} = \probof{T_k<\infty}[X_0=m] = \sum_{j=1}^6\probof{T_k<\infty}[X_1=j]\cdot\probof{X_1=j}[X_0=m] = \sum_{j=1}^6 P_{mj}\ell_{jk} $$
    So if we define $L_{ij}=\ell_{ij}$ then we get that $L=PL$ and we can solve for $L$.

    What is the expected time until being absorbed?
    We can consolidate $A=\set{1,2,5,6}$ to a state we will call $1$, then the new transition matrix is
    $$ P' = \pmatrix{1 & 0 & 0\cr a_1+a_2 & 0 & a_4\cr b_5+b_6 & b_3 & 0} $$
    Now let us define $r_j=\expecof{T_1}[X_0=j]$, then we get
    $$ r_j = \sum_{i=1}^3\expecof{T_1}[X_1=i]P_{ji} = P_{j1} + \sum_{i=2}^3(r_i+1)P_{ji} = P_{j1} + P_{j2} + P_{j3} + r_2P_{j2} + r_3P_{j3} $$
    Which is a linear system of equations which can be solved.

\eexam

\subsection{Stationary Distributions and the Convergence of Markov Chains}

\bdefn

    Suppose $\abs S=N$, then a {\emphcolor stationary distribution} of $P$ is a row vector $\pi$ which represents a distribution (meaning $\pi_i\geq0$ and $\sum_{i=1}^N\pi_i=1$) such that $\pi=\pi P$.

\edefn

A stationary distribution is an eigenvector (or the transpose of one) of $P^\top$ whose eigenvalue is $1$.
If $\pi$ is a stationary distribution, then $\pi P=\pi\implies\pi P^n=\pi$ for every $n\geq0$.
This means that if $X_0\sim\pi$ then $X_n\sim\pi$ for every $n$ (since $\probof{X_n=k}[X_0\sim\pi]=(\pi P^n)_k=\pi_k$).

For example if $G=(V,E)$ is an undirected graph where $\abs V=N$ and the transitions from each state are all uniform (meaning $\probof{X_n=v}[X_{n-1}=u]=\frac1{\deg(u)}$ if $v\oto u$), then let
$$ \tilde\pi = (\deg(v_1),\dots,\deg(v_N)) $$
Then (using the notation $\delta\phi$ which is $1$ if $\phi$ is true and $0$ otherwise) we have that $P_{xy}=\frac1{\deg(x)}\delta(x\oto y)$, so
$$ (\tilde\pi P)_y = \sum_{x\in V}\tilde\pi_x P_{xy} = \sum_{x\in V}\deg(x)\frac1{\deg(x)}\delta(x\oto y) = \sum_{x\in V}\delta(x\oto y) = \deg(y) = \tilde\pi_y $$
So $\tilde\pi$ is a non-negative row vector, but it must be normalized to become a distribution, so we define
$$ \pi_v = \frac{\deg(v)}{\sum_{u\in V}\deg(u)} = \frac{\deg(v)}{2\abs E} $$
If the degree of each vertex is constant, suppose $\deg(v)=d$ for all $v\in V$, then $\pi_v=\frac{d}{dN}=\frac1N$ so $\pi$ is a uniform distribution.

\bthrm[title=Existence and Uniqueness Theorem, name=eautheorem]

    Let $P$ be the transition matrix of irreducible finite-state Markov chain, then there exists a unique stationary distribution $\pi$ for $P$.

\ethrm

We know that $P{\bf1}={\bf1}$ and so $1$ is an eigenvalue for $P$, and since $P$ and $P^\top$ are similar, they share eigenvalues.
Thus $P^\top$ has an eigenvalue of $1$ and therefore must have a stationary distribution.
To show that this eigenvector is unique, we will show that the column eigenspace of $P$ has a dimension of one, and since the eigenspaces of a matrix and its transpose are equal (think Jordan normal forms),
this is sufficient.
So we will show that if $h\in{\bb R}^N$ is an eigenvector of $P$ with an eigenvalue of $1$, it is of the form $h=(c,\dots,c)^\top$.
Because $S$ is finite, there exists a state $a\in S$ such that $h_a=M$ is maximal.
Now suppose there exists a $z\in S$ such that $h_z<M$ and $P_{az}>0$ then
$$ h_a = (Ph)_a = \sum_{y\in S}P_{ay}h_y = P_{az}h_z + \sum_{y\neq z}P_{ay}h_y < M\parens{\sum_{y\in S}P_{ay}} = M = h_a $$
since $P_{az}>0$ and $h_z<M$, and this is a contradiction.
So for every state where $P_{az}>0$, $h_z=M$.
If we continue this proof (since $P^nh=h$), we get that if $a\to z$ then $h_z=M$.
Since the Markov chain is irreducible, it is closed and therefore $h_z=M$ for every $z\in S$.
\qed

Notice that the proof of existence here assumes nothing about $S$ other than it being finite.
But in the case that the chain is irreducible, we can also provide a constructive proof of the existence of a stationary distribution.
But first, a lemma:

\blemm

    For every two states $x,y\in S$ in a finite irreducible state space $\expecof[x]{T_y}<\infty$.

\elemm

Since $S$ is irreducible and finite, there exists an $\epsilon>0$ and a $r\in{\bb N}$ such that for every $a,b\in S$, there exists a $j\leq r$ such that $P^j(a,b)>\epsilon$.
This is since $S$ is connected and so between every two states there exists a path of length $\leq r$ (taking the maximum length of all paths, or just $N$) and so $P^j(a,b)>0$.
Take $\epsilon$ to be less than the minimum of all such $P^j(a,b)$, which we can do since $S$ is finite.

Thus
$$ \probof{(\exists m\in[0,\dots,r])X_m=b}[X_n=a]>\epsilon $$
Now we know that $T_b>kr$ if and only if $X_0,\dots,X_r\neq b$ and then we don't hit $b$ for another $(k-1)r$ rounds, meaning $T_b>(k-1)r$.
By homogeneity this means
\multlines{
    \probof{T_b>kr}[X_0=a] \leq \max_{a'}\probof{T_b>(k-1)r}[X_0=a']\probof{(\forall m\in[0,r])X_m\neq b}[X_0=a]\cr
    &\leq \max_{a'}\probof{T_b>(k-1)r}[X_0=a']\cdot(1-\epsilon)
}
and so by induction, this is $\leq(1-\epsilon)^k$.
Thus
$$ \expecof{T_b}[X_0=a] = \sum_{n=0}^\infty\probof{T_b>n}[X_0=a] \leq r\sum_{k=0}^\infty\probof{T_b>kr}[X_0=a] \leq r\sum_{k=0}^\infty(1-\epsilon)^k < \infty $$
The first inequality is due to the series being decreasing, and so we can take a summand and copy it $r$ times, then take the $r$th next.
\qed

Now we can construct a stationary distribution.
Let us define
$$ \tilde\pi_y = \expecof[z_0]{\centermath{the number of times $y$ is visited,\cr including at time $0$,\cr before returning to $z_0$}} = \sum_{n=0}^\infty\probof{X_n=y,T_{z_0}>n}[X_0=z_0] $$
The last equality is since this probability is equal to the number of visits being $\geq n$.
This is well-defined as
$$ \tilde\pi_y \leq \sum_{n=0}^\infty\probof{T_{z_0}>n}[X_0=z_0] = \expecof[z_0]{T_{z_0}} $$
and this is finite by the above lemma, so $\tilde\pi_y<\infty$.
Now we will compute $(\tilde\pi P)_y$:
$$ \eqalign{
    (\tilde\pi P)_y &= \sum_{x\in S}\tilde\pi_x P_{xy}\cr
    &= \sum_{x\in S}\sum_{n=0}^\infty\probof[z_0]{X_n=x,T_{z_0}>n}P_{xy}\cr
    &= \sum_{n=0}^\infty\sum_{x\in S}\probof[z_0]{X_n=x,T_{z_0}\geq n+1}\probof{X_{n+1}=y}[X_n=x]\cr
    &= \sum_{n=0}^\infty\sum_{x\in S}\probof[z_0]{X_{n+1}=y,X_n=x,T_{z_0}\geq n+1}\cr
    &= \sum_{n=0}^\infty\probof[z_0]{X_{n+1}=y,T_{z_0}\geq n+1}\cr
    &= \sum_{k=1}^\infty\probof[z_0]{X_k=y,T_{z_0}\geq k}\cr
    &= \sum_{k=0}^\infty\probof[z_0]{X_k=y,T_{z_0}\geq k} + \sum_{k=0}^\infty\probof[z_0]{X_k=y,T_{z_0}=k} - \probof[z_0]{X_0=y,T_{z_0}=0}\cr
    &= \tilde\pi_y + \sum_{k=0}^\infty\probof[z_0]{X_k=y,T_{z_0}=k} - \delta(y=z_0)\cr
} $$ 
Notice that $X_k=y,T_{z_0}=k$ if and only if $T_{z_0}=k$ and $y=z_0$, and so the sum is equal to $\delta(y=z_0)$.
So we get that $\tilde\pi P=\tilde\pi$ as required.
So we just need to normalize it by
$$ \sum_{x\in S}\tilde\pi_S = \expecof[z_0]{T_{z_0}} $$
And thus the stationary distribution is
$$ \pi_x = \frac{\tilde\pi_x}{\expecof[z_0]{T_{z_0}}} $$
\qed

\bcoro

    If $P$ is irreducible then $\pi_a=\frac1{\expecof[a]{T_a}}$.

\ecoro

Since $\pi$ is unique we can choose any $z_0$ and get the same result.
So we can choose $z_0=a$ and so
$$ \pi_a = \frac{\expecof{\centermath{The number of times we visit $a$\cr before returning to $a$\cr including $t=0$}}[X_0=a]}{\expecof[a]{T_a}} $$
The numerator here is obviously $1$, and so $\pi_a=\frac1{\expecof[a]{T_a}}$.
\qed

For example, we showed that for a connected graph where the degree of each vertex is $d$ (a connected $d$-regular graph), $\pi_v=\frac1N$ where $N=\abs V$.
Thus since $P$ is irreducible, we get that
$$ \frac1N = \pi_v = \frac1{\expecof[a]{T_a}} \implies \expecof[a]{T_a} = N $$
This is independent of the structure of the graph.
But importantly, $T_a$ is dependent on the structure of the graph!

As another example, if $P$ is symmetric then ${\bf1}^\top P=(P{\bf1})^\top={\bf1}^\top$ and so $\frac1N{\bf1}$ is a stationary distribution of $P$.
And thus $\expecof[a]{T_a}=N$ where $N=\abs S$.

\bthrm

    If $a\in S$ is a transient state and $S$ is finite, then for every stationary distribution $\pi$, $\pi_a=0$.

\ethrm

There are two cases we will consider: that $a$ is connected to only transient states, and that there exists a recurrent state $b$ such that $a\to b$.
In the second case we have that $b\notto a$ since $a$ is transient and $b$ is recurrent.
Let $a_0=a\to a_1\to\cdots\to a_n\to b$ be the path from $a$ to $b$, and we can assume that all $a_i$ are transient (as otherwise we could set $b=a_i$ for the minimum $i$ where $a_i$ is recurrent).
Let $C$ be the connected component of $b$ and $\pi$ be a stationary distribution on all of $S$.
Then
$$ \sum_{z\in C}\pi_z = \sum_{z\in C}(\pi P)_z = \sum_{z\in C}\parens{\sum_{y\in C}\pi_y P(y,z) + \sum_{y\notin C}\pi_y P(y,z)} = \sum_{y\in C}\pi_y\sum_{z\in C}P(y,z) +
\sum_{z\in C}\sum_{y\notin C}\pi_y P(y,z) $$
Since $C$ is closed and $y\in C$, we have that $\sum_{z\in C}P(y,z)=1$ and thus we get that the left sum is $\sum_{y\in C}\pi_y$, and since the entire expression is equal to $\sum_{z\in C}\pi_z$, we must
have that the right sum is zero.
So for every $z\in C$ and $y\notin C$, $\pi_yP(y,z)=0$.

This must be true in particular for $y=a_n$ and $z=b$, and since $P(a_n,b)>0$ this means $\pi_{a_n}=0$.
And we claim inductively that $\pi_{a_k}=0$, since
$$ \pi_{a_k} = \sum_{y\in S}\pi_yP(y,a_k) $$
and so if $\pi_{a_k}=0$ then $\pi_yP(y,a_k)=0$ for all $y\in S$.
Since $P(a_{k-1},a_k)>0$ this means $\pi_{a_{k-1}}=0$.
And so in particular we have that $\pi_a=\pi_{a_0}=0$ as required.
\qed

Now suppose $S$ is a finite state space, then it can be uniquely partitioned into
$$ S = T\dcup C_1\dcup\cdots\dcup C_n $$
where $T$ is the set of all transient states, and $C_i$ are irreducible components.
We showed that for every stationary distribution $\pi$, for every $a\in T$ we have $\pi_a=0$.
And we also showed that for every $1\leq i\leq n$ there exists a unique stationary distribution $\pi_i$ whose support is $C_i$ (meaning for every $a\notin C_i$, $\pi_i(a)=0$).
Thus a general stationary distribution is a normalized vector (meaning the sum of its coefficients is one) in ${\rm span}\set{\pi_1,\dots,\pi_n}$.
This is since the transition matrix $P$ can be viewed as a block matrix over the partition of $S$.

For the next lemma, let us state a combinatorical fact: if $A\subseteq{\bb N}$ is closed under addition and has a greatest common divisor of $1$, then ${\bb N}\setminus A$ is finite.
This is trivial if $1\in A$.

\blemm

    Suppose $P$ is the transition matrix of an irreducible, aperiodic, finite-state, homogeneus Markov chain.
    Then there exists an $r_0>0$ such that for all $r\geq r_0$ and $a,b\in S$, $P^r(a,b)>0$.

\elemm

Let us define as before $\tau(a)=\set{n\geq1}[P^n(a,a)>0]$.
Since $P$ is aperiodic, $d(a)=\gcd\tau(a)=1$, and $\tau(a)$ is closed under addition since $P^{n+m}(a,a)\geq P^n(a,a)P^m(a,a)$.
This means that ${\bb N}\setminus\tau(a)$ is finite.
This means that $\bigcup_{a\in S}\bigl({\bb N}\setminus\tau(a)\bigr)={\bb N}\setminus\bigcap_{a\in S}\tau(a)$ is finite as well as the finite union of finite sets.
Let $t_0$ be an upper bound for ${\bb N}\setminus\bigcap_{a\in S}\tau(a)$, so for every $t\geq t_0$ we have that $t\in\bigcap_{a\in S}\tau(a)$ meaning $P^t(a,a)>0$ for all $a\in S$.

Since $P$ is irreducible, for every $a,b\in S$ there exists an $n=n(a,b)$ such that $P^n(a,b)>0$.
Now $n$ is bound by $\abs S$ and therefore we can define $n_0=\max_{a,b\in S}n(a,b)$ and so for every $r\geq t_0+n_0$ we have that $r-n_0\geq t_0$ and so $P^{r-n_0}(a,a)>0$.
Thus
$$ P^r(a,b) \geq P^{r-n_0}(a,a)P^{n_0}(a,b) > 0 $$
so $r_0=t_0+n_0$ satisfies the condition.
\qed

\blemm

    Again suppose $P$ is irreducible and aperiodic, and let $\pi$ be its unique stationary distribution.
    Then there exists an $0<\alpha<1$ and a constant $c>0$ such that for every $k\in{\bb N}$ and every distribution vector $v$,
    $$ \norm{vP^k-\pi}_1 \leq c\alpha^k $$
    where $\norm{\,\cdot\,}_1$ is the $1$-norm on ${\bb R}^n$: $\norm{u}_1=\sum_{k=1}^n\abs{u_i}$.

\elemm

By the previous lemma, there exists an $r>0$ such that $P^r>0$ (meaning every coefficient of $P^r$ is positive).
Since $P$ is finite, there exists a $0<\delta<1$ such that for every $a,b\in S\colon P^r(a,b)\geq\delta\pi_b$.
Let $\Pi$ be the matrix whose rows are all $\pi$.
Then let us define the matrix $Q$ by
$$ P^r = \delta\Pi + (1-\delta)Q $$
and since $P^r\geq\delta\Pi$ (pointwise), we have $Q\geq0$ (pointwise).
Now notice that $\Pi$ is stochastic since $(\Pi{\bf1})_i=\pi{\bf1}=1$, and so $Q$ is also stochastic:
$$ {\bf1} = P^r{\bf1} = \delta{\bf1} + (1-\delta)Q{\bf1} \implies (1-\delta){\bf1} = (1-\delta)Q{\bf1} $$
and since $\delta<1$, $1-\delta\neq0$.
Let us define $\theta\coloneqq1-\delta$ and we will prove by induction that for all $k\geq1$,
$$ P^{rk} = (1-\theta^k)\Pi + \theta^kQ^k $$
for $k=1$ this is trivial.
For the induction step,
$$ P^{r(k+1)} = P^{rk}P^r = \bigl((1-\theta^k)\Pi + \theta^kQ^k\bigr)P^r = (1-\theta^k)\Pi P^r + \theta^kQ^kP^r $$
Since $\Pi P=\Pi$, we have that $\Pi P^r=\Pi$ and so this is equal to
$$ = (1-\theta^k)\Pi + \theta^k\bigl((1-\theta)Q^k\Pi + \theta Q^{k+1}\bigr) = (1-\theta^k)\Pi + \theta^k(1-\theta)Q^k\Pi + \theta^{k+1}Q^{k+1} $$
Now since $Q^k$ is stochastic and $\Pi$'s columns are constant, $Q^k\Pi=\Pi$.
And so this is equal to
$$ = (1-\theta^k+\theta^k-\theta^{k+1})\Pi + \theta^{k+1}Q^{k+1} = (1-\theta^{k+1})\Pi + \theta^{k+1}Q^{k+1} $$
as required.

And so now we have for all $j\geq0$, $P^{rk+j} = (1-\theta^k)\Pi + \theta^kQ^kP^j$ and so
$$ P^{rk+j} - \Pi = \theta^k\bigl(Q^kP^j - \Pi\bigr) $$
Since $Q^kP^j$ and $\Pi$ are all stochastic matrices and thus their coefficients all are bound by $1$, the coefficients of $Q^kP^j-\Pi$ all have an absolute value bound by $1$ as well.
Now since $(v\Pi)_i$ is equal to $v$ times the $i$th column of $\Pi$ which is $\pi_i{\bf1}$, we have $(v\Pi)_i=\pi_iv{\bf1}=\pi_i$ (since $v$ is a distribution, $v{\bf1}=1$).
And so $\pi=v\Pi$, so
$$ \norm{vP^{rk+j}-\pi}_1 = \norm{vP^{rk+j}-v\Pi}_1 = \norm{v(P^{rk+j}-\Pi)}_1 = \theta^k\norm{v(Q^kP^j-\Pi)}_1 $$
since $Q^kP^j-\Pi$'s coefficients are all bound by $1$, the norm is bound by a constant (which is the norm of $v$ times the matrix of all ones, since $v$ is positive).
So we have that $\norm{vP^{rk+j}-\pi}_1\leq c\theta^k$ and finding the appropriate values, we can bound this by some $c'\alpha^{rk+j}$.
\qed

\bthrm

    Let $P$ be the transition matrix of an irreducible aperiodic Markov chain, and let $\pi$ be its unique stationary distribution.
    Then for every initial distribution $v$, $vP^n\xvarrightarrow{n\to\infty}\pi$ pointwise (meaning $(vP^n)_i\xvarrightarrow{n\to\infty}\pi_i$).
    Since $(vP^n)_i=\probof[v]{X_n=i}$, equivalently $\probof[v]{X_n=i}\xvarrightarrow{n\to\infty}\pi_i$ or $X_n\xvarrightarrow{\,d\,}\pi$.

\ethrm

So we must simply show that $\abs{(vP^n)_i-\pi_i}\xvarrightarrow{n\to\infty}0$.
This is an immediate consequence of the previous lemma, which gave us that $\norm{vP^n-\pi}_1\leq c\alpha^n$ and so in particular $\norm{vP^n-\pi}_1\xvarrightarrow{n\to\infty}0$.
Since $\norm{vP^n-\pi}_1=\sum_{i=1}^N\abs{(vP^n)_i-\pi_i}$, certainly $\abs{(vP^n)_i-\pi_i}\xvarrightarrow{n\to\infty}0$, as required.
(In general convergence in the $p$-norms of ${\bb R}^N$ is equivalent to pointwise convergence.)
\qed

\bcoro

    If $P$ is a stochastic matrix then all of its eigenvalues are bound by $1$ (in absolute value).

\ecoro

Let $\gamma$ be an eigenvalue of $P$, then there exists a vector $v$ such that $Pv=\lambda v$.
Let $j$ be the state in $S$ such that $\abs{v_j}=\max_{i\in S}\abs{v_i}$ and so
$$ \abs\lambda\abs{v_j} = \abs{(Pv)_j} = \abs{\sum_{i\in S}P_{ji}v_i} \leq \sum_{i\in S}P_{ji}\abs{v_i} \leq \abs{v_j}\sum_{i\in S}P_{ji} = \abs{v_j} $$
Thus $\abs\lambda\leq1$.
\qed

\subsection{Mixing Times}

\bdefn

    Let $\mu$ and $\nu$ be two be two probability measures over the same $\sigma$-algebra ${\cal F}$, then we define their {\emphcolor total variation} to be
    $$ \norm{\mu-\nu}_{\rm TV} = \sup_{A\in{\cal F}}\abs{\mu(A)-\nu(A)} $$
    This is also denoted $d_{\rm TV}(\mu,\nu)$, and this is in fact a metric over the space of probability measures on ${\cal F}$.

\edefn

If $\mu$ and $\nu$ are discrete probability distributions on $\Omega$, then let $B=\set{x}[\mu(x)\geq\nu(x)]$, and let $A\subseteq\Omega$ be any event.
Then for any $x\in A\cap B^c$, $\mu(x)-\nu(x)<0$ and so $\mu(A\cap B^c)-\nu(A\cap B^c)\leq0$.
Thus
$$ \mu(A) - \nu(A) = \mu(A\cap B) - \nu(A\cap B) + \mu(A\cap B^c) - \nu(A\cap B^c) \leq \mu(A\cap B) - \nu(A\cap B) $$
and for every $x\in B\cap A^c$, $\mu(x)-\nu(x)\geq0$ and so $\mu(B\cap A^c)-\nu(B\cap A^c)\geq0$ so
$$ \leq \mu(B) - \nu(B) $$
And similarly we have that $\nu(A)-\mu(A)\leq\nu(B^c)-\mu(B^c)=\mu(B)-\nu(B)$.
Thus we have that for every event $A$, $\abs{\mu(A)-\nu(A)}\leq\mu(B)-\nu(B)$ and so
\multlines{
    \norm{\mu-\nu}_{\rm TV} = \mu(B) - \nu(B) = \frac12\bigl(\mu(B) - \nu(B) + \nu(B^c) - \mu(B^c)\bigr) = \frac12\parens{\sum_{x\in B}\bigl(\mu(x)-\nu(x)\bigr) + \sum_{x\notin B}\bigl(\nu(x)-\mu(x)\bigr)}
    \cr
    &= \frac12\sum_{x\in\Omega}\abs{\mu(x)-\nu(x)}
}
the second equality is since $\mu(B)-\nu(B)=\nu(B^c)-\mu(B^c)$.
So we have proven

\bprop

    If $\mu$ and $\nu$ are two discrete probability measures over the same space, then their total variation distance is equal to half of their $L^1$ distance, ie.
    $$ \norm{\mu-\nu}_{\rm TV} = \frac12\sum_{x\in\Omega}\abs{\mu(x)-\nu(x)} $$

\eprop

This holds in particular for when $\mu$ and $\nu$ are distribution vectors.

\bdefn

    Let $P$ be the transition matrix of an irreducible Markov chain whose stationary distribution is $\pi$, the we define
    $$ d(k) = \max_{j\in S}d_{\rm TV}(e_jP^k,\pi) $$
    Since $e_jP^k$ and $\pi$ are both distributions, they can be viewed as probability measures, and so we can discuss their total variation.
    $e_jP^k$ is the distribution of $X_k$ if $X_0=j$, and so $d(k)$ gives us the maximum total variation of the distribution of $X_k$ and $\pi$ over all possible initial states.
    Let us also define the {\emphcolor mixing time} to be
    $$ t_{\rm mix}(\epsilon) = \min\set{k}[d(k)\leq\epsilon] $$
    $t_{\rm mix}(\epsilon)$ gives us the minimum $k$ where the total variation of the distribution $X_k$ and $\pi$ is less than $\epsilon$, independent of the initial state.
    Though generally if we talk about the ``mixing time'' of a Markov chain, we set $\epsilon=\frac14$.
    And finally we also define
    $$ \bar d(k) = \max_{i,j\in S}d_{\rm TV}(e_iP^k,e_jP^k) $$

\edefn

By the triangle inequality, $\bar d(k)\leq2d(k)$.
And in fact $d(k)\leq\bar d(k)$ so
$$ d(k) \leq \max_{i,j\in S}\norm{e_iP^k-e_jP^k}_{\rm TV} $$

\bdefn

    A {\emphcolor coupling} of two probability measures $\mu$ and $\nu$ over the same $\sigma$-algebra ${\cal F}$ is a pair of random variables $(X,Y)$ such that $X\sim\mu$ and $Y\sim\nu$.
    Formally, a coupling is a new probability space and random variables whose codomain is ${\cal F}$ such that for every $A\in{\cal F}$, $\probof{X\in A}=\mu(A)$ and $\probof{Y\in A}=\nu(A)$.

\edefn

\bprop

    If $\mu$ and $\nu$ are probability measures over the same $\sigma$-algebra, then
    $$ \norm{\mu-\nu}_{\rm TV} \leq \inf\set{\probof{X\neq Y}}[\hbox{$(X,Y)$ is a coupling of $\mu$ and $\nu$}] $$

\eprop

Let $(X,Y)$ be a coupling and $A\in{\cal F}$ then
$$ \mu(A) - \nu(A) = \probof{X\in A} - \probof{Y\in A} \leq \probof{X\in A,Y\notin A} \leq \probof{X\neq Y} $$
and taking the infimum over all couplings $(X,Y)$ preserves this inequality.
\qed

In fact, there is actually an equality here but the other direction is harder to prove.

\bthrm

    Suppose $\set{X_n}$ and $\set{Y_n}$ are two Markov chains with the same transition matrix $P$.
    Further suppose that if $X_s=Y_s$ then $X_t=Y_t$ for all $t\geq s$, then
    $$ \norm{e_xP^t-e_yP^t}_{\rm TV} \leq \probof{X_t\neq Y_t}[X_0=x,Y_0=y] $$

\ethrm

This is as $e_xP^t$ and $e_yP^t$ are the distributions of $X_t$ and $Y_t$ under the assumption that $X_0=x$ and $Y_0=y$.
And $(X_t,Y_t)$ is certainly a coupling of these distributions in $\probof{\,\cdot\,}[X_0=x,Y_0=y]$.
\qed

This means that if $\set{X_n}$ is a Markov chain, and $\set{Y_n}$ is some other Markov chain with the same transition matrix then $d(k)$ (for either $\set{X_n}$ or $\set{Y_n}$) can be bound by:
$$ d(k) \leq \max_{i,j\in S}\probof{X_k\neq Y_k}[X_0=i,Y_0=j] $$

\bexam

    What is the mixing time of the random walk on the circle $C_N$ (this is the graph of $N$ nodes, $\set{v_1,\dots,v_N}$ with the edges $\set{v_i,v_i}$ and $\set{v_i,v_{i+1}}$)?
    Let us define two Markov chains $X_n$ and $Y_n$ where at every step we choose a random chain with equal probability and that will be the chain which will make the next step.
    As soon as the two chains intercept, they step together.
    Let $T$ be the time that the two chains intercept, then by above and Markov's inequality
    $$ d(t) \leq \max_{x,y}\probof[x,y]{T>t} \leq \max_{x,y}\frac{\expecof[x,y]T}t $$
    The expected hitting time of $k\in\set{0,\dots,N}$ for a random walk on the circle is $k(N-k)$ (this will be shown later), which takes a maximum at $k=\frac N2$, and so
    $\max\expecof[x,y]T\leq\frac{N^2}4$.
    And since we measure mixing times with $\epsilon=\frac14$ we get that if $\frac{N^2}{4t}\leq\frac14$, meaning $t\geq N^2$ (and in particular if $t=N^2$), then $d(t)\leq\frac14$.
    So $t_{\rm mix}\leq N^2$.

\eexam

\subsection{Famous Markov Chains}

In this subsection we will discuss various useful Markov chains.

\smallskip
{\bf 1 Gambler's Ruin}

The first one we will discuss is called the {\it Gambler's Ruin}: suppose a gambler goes to a casino with the goal of winning $n$ dollars.
If the gambler reaches his goal of $n$ dollars or fails and loses all his money (reaches $0$ dollars), he leaves the casino.
Suppose he bets a single dollar each time, and has a fair chance of winning.
We can ask two questions: how much time will it take for the gambler to leave the casino, and what is the probability that the gambler goes broke (reaches $0$ before $n$)?

So we can define a Markov chain $X_n$ where $X_n$ is the amount of money the gambler has after $n$ bets.
The transition matrix here is
$$ P(i\to i+1) = P(i\to i-1) = \frac12 \hbox{ for $0<i<n$},\qquad P(0\to0) = P(n\to n) = 1 $$
Let us define $\tau=\minof{T_0,T_n}$ which is the time the gambler will leave the casino.
We make two claims:
$$ \probof[k]{X_\tau=n} = \frac kn,\qquad \expecof[k]\tau = 4k(n-k) $$
so if the gambler starts with $k$ dollars, the probability he gets his goal of $n$ dollars is $\frac kn$, and the expected time it takes him to leave the casino is $4k(n-k)$.
Note that $X_\tau=n$ is equivalent to $T_n<T_0$.

To prove this let us define $p_k=\probof[k]{X_\tau=n}$.
Then $p_0=0$ and $p_n=1$.
And using first step analysis,
$$ p_k = \probof{T_n<T_0}[X_0=k] = \frac12\probof{T_n<T_0}[X_1=k+1] + \frac12\probof{T_n<T_0}[X_1=k-1] = \frac12p_{k+1} + \frac12p_{k-1} $$
Now given the initial conditions, there must be a unique solution to this.
And since $p_k=\frac kn$ works as a solution, it must be the unique solution.

Let us denote $\mu_k=\expecof[k]\tau$, and then $\mu_0=\mu_n=0$.
And we also get
\lmultlines{
    \mu_k = \expecof{\tau}[X_0=k] = \frac12\expecof\tau{X_1=k+1} + \frac12\expecof\tau[X_1=k-1]\cr
    &= \frac12\bigl(1+\expecof\tau[X_0=k+1]\bigr) + \frac12\bigl(1+\expecof\tau[X_0=k-1]\bigr)\cr
    &= 1 + \frac12\mu_{k-1} + \frac12\mu_{k+1}
}
Again, this must have a unique solution due to the initial conditions, and $4k(n-k)$ satisfies this.

\smallskip
{\bf 2 Coupon Collector}

There are $n$ types of coupons, and we would like to collect them all.
When we are given a coupon, the probability it is a specific type distributes uniformly.
So let us define $X_k$ to be the number of types of coupons we have after collecting $k$ coupons, and so
$$ P(i\to i) = \frac in,\qquad P(i\to i+1) = 1-\frac in $$
$n$ is the only absorbing state and so all other states are transient and we will eventually be absorbed by $n$ with probability $1$.
What is the expected time that we hit $n$ for the first time?
Now, $T_{k+1}-T_k$ is the number of times that we get one of the $k$ types of coupons we already have, and so $T_{k+1}-T_k\sim{\rm Geo}\parens{1-\frac kn}$.
Thus
$$ \expecof[0]{T_n} = \sum_{k=0}^{n-1}\expecof[0]{T_{k+1}-T_k} = \sum_{k=0}^{n-1}\frac1{1-\frac kn} = n\sum_{k=1}^n\frac1k \sim n\log(n) $$

What is the probability it took ``much more time'' to get to $n$?
We claim
$$ \probof{T_n>\ceil{n\log n+cn}}\leq e^{-c} $$
Let $A_i$ be the probability that after $\ceil{n\log n+cn}$ time, we have not collected the $i$th coupon type.
Then
$$ \probof{T_n>\ceil{n\log n+cn}} = \probof{\bigcup_{i=1}^n A_i} \leq \sum_{i=1}^n\probof{A_i} = \sum_{i=1}^n\parens{1-\frac1n}^{\ceil{n\log n+cn}} $$
since $1-x\leq e^{-x}$ this can be further bound by
$$ \leq n\parens{e^{-\frac1n}}^{n\log n+cn} = ne^{-\log n-c} = e^{-c} $$
as required.

\smallskip
{\bf 3 P\'olya Urn}

In an urn there are two balls: one white and one black.
At every step we choose an arbitrary ball and add a new one of the same color.
Let us define $(B_k,W_k)$ to be the number of black and white balls, respectively.
The state space is then $S={\bb N}^2$.
The transistion probabilities are
$$ P\bigl((i,j)\to(i+1,j)\bigr) = \frac i{i+j},\qquad P\bigl((i,j)\to(i,j+1)\bigr) = \frac j{i+j} $$

Is $B_k$ a Markov chain?
Well if we know $B_k$ then we know that $W_k=(k+2)-B_k$ since the total number of balls after $k$ steps is $k+2$.
And so we can then determine the probability for transitioning, so it is indeed a Markov chain.
But it is not homogeneus: in order to determine the transition probability we must use $k$: $\probof{B_{k+1}=i+1}[B_k=i]=\frac i{k+2}$, which is dependent on $k$.

Nevertheless we claim that $B_k\sim{\rm Unif}\set{1,2,\dots,k+1}$.
We prove this by induction: for $k=1$ this is trivial as the probabilities that $B_k=1$ and $B_k=2$ are the same.
And if $B_{k-1}\sim{\rm Unif}\set{1,2,\dots,k}$ then
\multlines{
    \probof{B_k=j} = \probof{B_k=j}[B_{k-1}=j-1]\cdot\probof{B_{k-1}=j-1} + \probof{B_k=j}[B_{k-1}=j]\cdot\probof{B_{k-1}=j}\cr
    &= \frac{j-1}{k+1}\cdot\frac1k + \parens{1-\frac{k-1}{k+1}}\cdot\frac1k = \frac1k\cdot\parens{1-\frac1{k+1}} = \frac1{k+1}
}
as required.

\smallskip
{\bf 4 Random Walks on ${\bb Z}$}

This is a homogeneus chain on ${\bb Z}$ whose transitions are
$$ P(k\to k+1)=p,\quad P(k\to k)=r,\quad P(k\to k-1)=q\qquad\qquad (p+q+r=1) $$
We will first focus on the case that $r=0$ and $p=q=\frac12$, which is a {\it fair} walk.
In this case we claim that for all $t,j,k>0$,
$$ \probof[k]{T_0<t,X_t=j} = \probof[k]{X_t=-j},\qquad \probof[k]{T_0<t,X_t>0} = \probof[k]{X_t<0} $$
This is as if $T_0=s$ then the walk starting from time $s$ is equivalent to the walk starting from $X_0=0$.
Thus
$$ \probof[k]{T_0=s,X_t=j} = \probof[k]{T_0=s}\cdot\probof[0]{X_{t-s}=j} $$
by symmetry this is equal to
$$ = \probof[k]{T_0=s}\cdot\probof[0]{X_{t-s}=-j} = \probof[k]{T_0=s,X_t=-j} $$
And so we get that
$$ \probof[k]{T_0<t,X_t=j} = \sum_{s<t}\probof[k]{T_0=s,X_t=j} = \sum_{s<t}\probof[k]{T_0=s,X_t=-j} = \probof[k]{T_0<t,X_t=-j} $$
if we start at $k$ and at time $t$, $X_t=-j<0$ then necessarily $T_0<t$ so this is equal to $\probof[k]{X_t=-j}$ as required.
And now
$$ \probof[k]{T_0<t,X_t>0} = \sum_{j>0}\probof[k]{T_0<t,X_t=s} = \sum_{j>0}\probof[k]{X_t=-s} = \probof[k]{X_t<0} $$
as required.

We further claim that for every $k>0$,
$$ \probof[k]{T_0>t} = \probof[0]{-k<X_t\leq k} $$
This is as
$$ \probof[k]{X_t>0} = \probof[k]{X_t>0,T_0\leq t}+\probof[k]{T_0>t} $$
now we showed that $\probof[k]{X_t>0,T_0\leq t}=\probof[k]{X_t<0}$ and by symmetry about $k$ this is equal to $\probof[k]{X_t>2k}$.
Thus we get that
$$ \probof[l]{T_0>t} = \probof[k]{X_t>0} - \probof[k]{X_t>2k} = \probof[k]{0<X_t\leq 2k} = \probof[0]{-k<X_t\leq k} $$
as required.

Notice that if we start at $X_0=0$ then in order to get to $X_t=k$, we must take $r$ steps to the right and $\ell$ steps to the left where $r+\ell=t$ and $r-\ell=k$.
This means that $r=\frac{t+k}2$ and $\ell=\frac{t-k}2$.
Thus
$$ \probof[0]{X_t=k} = \cases{\displaystyle\binom{t}{\frac{t-k}2}2^{-t} & $t\equiv k\pmod2$\cr\noalign{\kern3pt} \hfil0 & else} $$
And so by Stirling we get
$$ \probof[0]{X_t=k} \leq \frac1{\sqrt\pi}\cdot\frac1{\sqrt t} = \frac c{\sqrt t} $$
And this means that
$$ \probof[k]{T_0>t} = \probof[0]{-k<X_t\leq k} = \sum_{j=-k+1}^k\probof[0]{X_t=j} \leq \frac{2ck}{\sqrt t} $$

\bprop

    For a fair walk on ${\bb Z}$, every state is transient but the expected return time to each state is infinite.

\eprop

We will prove that every state is transient in two ways.
For the first way, let us define $A_k=\set{T_{\pm2^k}<T_0}$ the event that we get to $2^k$ or $-2^k$ before $0$.
And so $\probof[0]{A_{k+1}}[A_k]=\probof[2^k]{T_0<T_{2^{k+1}}}=\frac12$ since the distance between $0$ and $2^k$ is the same as the distance between $2^k$ and $2^{k+1}$.
Since $\probof[0]{A_1}=\frac12$ and $\probof[0]{A_k}=\probof[0]{A_k}[A_{k-1}]\cdot\probof[0]{A_{k-1}}$, by induction $\probof[0]{A_k}=2^{-k}$.
And
$$ \probof[0]{T_0<\infty} = \probof[0]{\bigcap_{k=1}^\infty A_k} = \lim_{n\to\infty}\probof{A_n} = \lim_{n\to\infty}2^{-n} = 0 $$
where the second equality is since $\set{A_k}$ is a decreasing sequence and so this is due to the continuity of probability.
So $0$ is recurrent and since all states are connected, so is every other state.

For the second proof, by Stirling $\probof[0]{X_{2n}=0}=2^{-2n}\binom{2n}n\geq\frac c{\sqrt n}$ and so
$$ \sum_{n=1}^\infty\probof[0]{X_n=0} = \sum_{n=1}^\infty\probof[0]{X_{2n}=0} \geq \sum_{n=1}^\infty\frac c{\sqrt n} = \infty $$
and since $N_0(0)=\sum_{n=1}^\infty\chi\set{X_n=0}$, we get that
$$ \expecof{N_0(0)} = \sum_{n=1}^\infty\probof{X_n=0} = \infty $$
which means that $0$ is recurrent (since $N_0(0)\sim{\rm Geo}(1-f_0)$ if $f_0<0$ then its expected value would be finite, so $f_0=1$ meaning $0$ is recurrent).

To compute the expected return time, let us denote $\alpha=\expecof[1]{T_0}=\expecof[n]{T_{n-1}}$.
And by first step analysis,
$$ \alpha = \expecof[1]{T_0} = \frac12\expecof{T_0}[X_1=0] + \frac12\expecof{T_0}[X_1=2] = \frac12 + \frac12\bigl(1+\expecof[2]{T_0}\bigr) $$
Now, $\expecof[2]{T_0}=\expecof[2]{T_1}+\expecof[1]{T_0}$ since this is the expected time to go from $2$ to $1$ to $0$ (the only path from $2$ to $0$), and this is equal to $2\alpha$.
Thus we get that $1+\alpha=\alpha$.
But no finite number satisfies this, so $\alpha=\infty$.
And so we have show that
$$ \probof[0]{T_0<\infty} = 1,\qquad \expecof[0]{T_0} = \infty \qed $$

\subsection{Asymptotic Behavior}

\bdefn

    Let $\set{A_n}_{n=1}^\infty\subseteq{\cal F}$ be a sequence of events, then let us define
    $$ \displaylines{
        \set{A_n\hbox{ i.o.}} = \set{\omega\in\Omega}[(\forall k)(\exists m\geq k)\omega\in A_m] = \bigcap_{k=1}^\infty\bigcup_{m=k}^\infty A_m = \limsup A_n\cr
        \set{A_n\hbox{ a.e.}} = \set{\omega\in\Omega}[(\exists k)(\forall m\geq k)\omega\in A_m] = \bigcup_{k=1}^\infty\bigcap_{m=k}^\infty A_m = \liminf A_n
    } $$
    So $\set{A_n\hbox{ i.o.}}$ is the set of all elements which are in infinitely many $A_n$s, and $\set{A_n\hbox{ a.e.}}$ is the set of all elements which are in all but finitely many $A_n$s.

\edefn

Notice that in general
$$ \set{A_n\hbox{ i.o.}}^c = \set{A_n^c\hbox{ a.e.}},\qquad \set{A_n\hbox{ a.e.}} \subseteq \set{A_n\hbox{ i.o.}} $$

So for example, let $(\set{0,1}^{\bb N},{\cal F},{\bb P})$ be the probability space of the flipping of a fair coin.
Then let us define $A_n=\set{\omega}[\omega_n=1]$, the event that the $n$th flip resulted in $1$.
Then $\set{A_n\hbox{ i.o.}}$ is the set of all $\omega$ with infinitely many $1$s, and $\set{A_n\hbox{ a.e.}}$ is the set of all $\omega$s with finitely many $0$s.
Notice that all $A_n$ are independent since the coin flips are independent and so
$$ \probof{A_n\hbox{ a.e.}} = \probof{\bigcup_k\bigcap_{m\geq k}A_m} \leq \sum_k\probof{\bigcap_{m\geq k}A_m} = \sum_k\lim_{n\to\infty}\probof{\bigcap_{m=k}^{k+n}A_m} =
\sum_k\lim_n\prod_{m=k}^{k+n}\probof{A_m} = \sum_k\lim_n\frac1{2^n} = \sum_k0 = 0 $$

\blemm[title=Borel-Cantelli Lemma, name=bclemm]

    Let $\set{A_n}_{n=1}^\infty$ be a sequence of events, then
    \benum
        \item If $\sum\probof{A_n}<\infty$ then $\probof{A_n\hbox{ i.o.}}=0$.
        \item If $\sum\probof{A_n}=\infty$ and $\set{A_n}$ is independent then $\probof{A_n\hbox{ i.o.}}=1$.
    \eenum

\elemm

For the first, due to the continuity of measures
$$ \probof{A_n\hbox{ i.o.}} = \lim_{n\to\infty}\probof{\bigcup_{k=n}^\infty A_k} \leq \lim_{n\to\infty}\sum_{k=n}^\infty\probof{A_k} = 0 $$
the final equality is since the series $\sum_{k=1}^\infty\probof{A_k}$ converges and so its tail must converge to zero.
For the second,
$$ \probof{A_n\hbox{ i.o.}} = 1 - \probof{\bigcup_{m=1}^\infty\bigcap_{k=m}^\infty A_k^c} \geq 1 - \sum_{m=1}^\infty\probof{\bigcap_{k=m}^\infty A_k^c} $$
We will show that for every $m$, $\probof{\bigcap_{k=m}^\infty A_k^c}=0$ and this will be sufficient.
$$ \probof{\bigcap_{k=m}^\infty A_k^c} = \lim_{n\to\infty}\probof{\bigcap_{k=m}^{m+n}A_k^c} = \lim_{n\to\infty}\prod_{k=m}^{m+n}\probof{A_k^c} = \lim_{n\to\infty}\prod_{k=m}^{m+n}(1-\probof{A_k}) $$
since $1-x\leq e^{-x}$,
$$ \leq \lim_{n\to\infty}\exp\parens{-\sum_{k=m}^{m+n}\probof{A_k}} $$
Since the sum goes to $-\infty$, this goes to zero, as required.
\qed

\bexam

    We say that a number is {\it normal} if in its base $10$ representation, every finite string occurs infinitely many times.
    What is the probability that a number chosen uniformly in $[0,1]$ is normal?
    Suppose we choose $U=0.X_1X_2X_3\dots\in[0,1]$ where $X_i$ is uniformly chosen, $X_i\sim{\rm Unif}\set{0,\dots,9}$.
    Let us set a finite string $S=S_1\cdots S_N$ and define
    $$ A_i = \set{X_{iN+1}=S_1,X_{iN+2}=S_2,\dots,X_{(i+1)N}=S_N} $$
    $A_i$ is the event that the string $S$ occurs in $U$ beginning at index $iN+1$.
    $\set{A_i}$ are all independent since $A_i$ looks at a disjoint set of $X_k$s than $A_j$ does.
    And $\probof{A_i}=\frac1{10^N}$ for every $i$, so $\sum_{i=1}^\infty\probof{A_i}=\infty$ and so by the \refmath{bclemm} we have that $\probof{A_n\hbox{ i.o}}=1$.
    Meaning that the probability $S$ occurs infinitely many times in $U$ is $1$.

    Notice that this does not mean the probability of $U$ being normal is $1$, rather that the probability that $U$ has an arbitrary finite string occurring infinitely many times is $1$.
    But this does not necessarily mean that the probability of every finite string occurring infinitely many times is $1$.
    Fortunately it does, since if we denote the events by $A_i^S$, then we want to compute the probability of $\bigcap_S(A_n^S\hbox{ i.o.})$.
    Now, the countable intersection of probability-$1$ events also has probability $1$: if $\probof{B_n}=1$ then
    $$ \probof{\bigcap_{n=1}^\infty B_n} = 1 - \probof{\bigcup_{n=1}^\infty B_n^c} \geq 1 - \sum_{n=1}^\infty\probof{B_n^c} = 1 $$
    And so $\probof{\bigcap_S(A_n^S\hbox{ i.o.})}=1$ as required.

\eexam

\bexam

    Suppose we have a random process $\set{X_n}$ where each step is independent and distributes the same.
    Let us define $S_0=0$ and $S_n=\sum_{i=1}^n X_i$ then we claim
    $$ \probof{S_n=S_m\hbox{ i.o.}} \in \set{0,1} $$
    Suppose there exists an $N$ such that $p=\probof{X_1+\cdots+X_N=0}>0$ then let us define $A_i=\set{\sum_{j=iN}^{(i+1)N}X_j=0}$ then since $X_i$ are all independent and have the same distribution,
    $\probof{A_i}=p>0$ for all $i$.
    Then since $A_i$ are also all independent, by the \refmath{bclemm}, $\probof{A_i\hbox{ i.o.}}=\probof{S_{iN-1}=S_{(i+1)N}\hbox{ i.o.}}=1$ and this implies $\probof{S_n=S_m\hbox{ i.o}}=1$.
    Alternatively, for every $N$, $\probof{X_1+\cdots+X_N}=0$ and this means that we can never have that $S_n=S_m$ for $n>m$, as then $X_{n+1}+\cdots+X_m=0$.
    Thus $\probof{(\exists n>m)S_n=S_m}=0$, as required.

\eexam

\bdefn

    Let $\set{A_j}$ be a sequence of events.
    Then a {\emphcolor tail event} is an event in the $\sigma$-algebra $\sigma\bigl(\set{A_j}_{j=k}^\infty\bigr)$ for every $k>0$.
    Ie. tail events are elements of the $\sigma$-algebra $\bigcap_{k=1}^\infty\sigma(\set{A_j}_{j=k}^\infty)$.
    Recall that $\sigma({\cal F})$ is the $\sigma$-algebra generated by the family of sets ${\cal F}$.

\edefn

\bthrm[title=Kolmogorov's Zero-One Law, name=kolmo01]

    If $\set{A_j}$ is a sequence of independent events, then every tail event is trivial.

\ethrm

We lack tools to fully justify each step, but an outline of the proof is as follows: it can be shown that if the generators of a $\sigma$-algebra are independent then so is the $\sigma$-algebra.
Thus for every $k$, $\sigma(A_1,\dots,A_k)$ and $\sigma(A_{k+1},\dots)$ are independent.
So let $B$ be a tail event, thus $B\in\sigma(A_{k+1},\dots)$ and so $B$ is independent of every $\sigma(A_1,\dots,A_k)$.
And this means that $B$ is independent of $\sigma(A_1,\dots)$, and in particular $B$ is independent of itself.
And so $\probof B=\probof{B\cap B}=\probof B^2$ so $\probof B\in\set{0,1}$.
\qed

For random variables there exists a variation of the Zero-One law:

\bthrm

    If $\set{X_j}$ are independent random variables, and if $Y$ is a random variable (measurable) with respect to $\sigma(\set{X_j})$ then there exists a constant $c$ such that $\probof{Y=c}=1$.

\ethrm

\bthrm[title=Hewitt-Savage, name=hewittsavage]

    Suppose $\set{X_j}_{j=1}^\infty$ is a sequence of independent and equal-distribution random variables.
    Let $A\in\sigma(\set{X_j}_{j=1}^\infty)$ be an event such that for every for every finite permutation of indexes $\pi$, $\pi A=A$ (since elements of $A$ are of the form
    $\omega=(\omega_1,\omega_2,\dots)$, and so $\pi A=\set{\pi\omega}[\omega\in A]$ where $\pi$ acts on the vector $\omega$).
    Then $\probof A\in\set{0,1}$.

\ethrm

There must exist a sequence of events $A_n\in\sigma(X_1,\dots,X_n)$ such that $\probof{A_n\symdiff A}\to0$.
Then let us define
$$ \pi(j) = \cases{j + n & $1\leq j\leq n$\cr j-n & $n+1\leq j\leq2n$\cr j & $j>2n$} $$
this is a permutation of only a finite number of indexes, and notice that $\pi^2={\rm id}$.
Now we have that
$$ \probof{\set{\omega}[\omega\in A_n\symdiff A]} = \probof{\set{\omega}[\pi\omega\in A_n\symdiff A]} $$
And since $\set{\omega}[\pi\omega\in A] = \pi^{-1}A=\pi A=A$ and $A_n$ is of the form $\set{\omega}[(\omega_1,\dots,\omega_n)\in B_n]$ so $\set{\omega}[\pi\omega\in A_n] =
\set{\omega}[(\omega_{n+1},\dots,\omega_{2n})\in B_n]=A'_n$, we get that $\probof{A\symdiff A_n}=\probof{A\symdiff A_n'}$.
In general one has $\abs{\probof B-\probof C}\leq\probof{B\symdiff C}$, and so $\probof{A_n}\to\probof A$ and $\probof{A_n'}\to\probof A$.
$$ \probof{A_n\symdiff A_n'} \leq \probof{A_n\symdiff A} + \probof{A_n'\symdiff A} \to 0 $$
(since $A\symdiff B\subseteq(A\symdiff C)\cup(B\symdiff C)$.)
And so $\probof{A_n}-\probof{A_n\cap A_n'}\leq\probof{A_n\symdiff A_n'}\to0$, meaning that $\probof{A_n\cap A_n'}\to\probof A$.
But at the same time, since $A_n$ and $A_n'$ are independent (as they refer to different $X_i$s), we get that $\probof{A_n\cap A_n'}=\probof{A_n}\cdot\probof{A_n'}\to\probof A^2$.
Thus $\probof A=\probof A^2$ meaning $\probof A\in\set{0,1}$.
\qed

\vfill\break

\section{Brownian Motion}

A general random process is a sequence of random variables $\set{S_n}_{n=1}^\infty$ where $S_n=X_1+\cdots+X_n$ where $\set{X_i}_{i=1}^\infty$ are independent and equal-distribution.
By the law of large numbers, if $\expecof{X_n}=0$ then $\frac{S_n}n\xvarrightarrow{a.s.}0$, and if further $\expecof{X_n^2}=1$ (meaning $\Varof{X_n}=1$) then by the central limit theorem
$\frac{S_n}{\sqrt n}\xvarrightarrow{\,d\,}{\cal N}(0,1)$.
In general, we get the following by Hewitt-Savage (this was in homework):

\bthrm

    Let $S_n$ be a general random process, then one of the following occurs with probability $1$:
    $$ (1)\ (\forall n)S_n=0,\quad (2)\ S_n\to\infty\quad (3)\ S_n\to-\infty\quad (4)\ \limsup S_n=\infty,\ \liminf S_n=-\infty $$

\ethrm


\bdefn

    A collection of random variables $\set{B(t)}_{t\geq0}$ (meaning that for every $0\leq t\in{\bb R}$, $B(t)$ is a random variable) is called {\emphcolor Brownian motion} which starts at $x\in{\bb R}$ if
    the following conditions are met:
    \benum
        \item $B(0)\buildrel as\over=x$,
        \item Differences are independent: for every $0\leq t_1\leq\cdots\leq t_n$, $B(t_2)-B(t_1),\dots,B(t_n)-B(t_{n-1})$ are independent,
        \item Differences are normal: for every $t,h\geq0$, $B(t+h)-B(t)\sim{\cal N}(0,h)$,
        \item Continuity: with probability $1$, $t\varmapsto B(t)$ is continuous.
    \eenum

\edefn

Since each $B(t)$ is a random variable, meaning a function $B(t)\colon\Omega\longto{\bb R}$, we can view Brownian motion as a function $B\colon[0,\infty)\times\Omega\longto{\bb R}$ where
$B(t,\omega)=B(t)(\omega)$.
The final condition can then be stated with more formality:
$$ \probof{\set{\omega\in\Omega}[t\varmapsto B(t,\omega)\hbox{ is continuous}]} = 1 $$
Brownian motion describes a continuous random process, unlike Markov chains whose steps are all discrete.

Now suppose $B(t)$ is Brownian motion which starts at $0$, then for every $h>0$ let us focus on $B(0),B(h),B(2h),\dots$ and so
$$ B(nh) = \sum_{k=1}^n\bigl(B(kh) - B((k-1)h)\bigr) $$
which is the sum of independent normal distributions ${\cal N}(0,h)$, and so $\set{B(nh)}_{n=0}^\infty$ is a general random process whose steps distribute with ${\cal N}(0,h)$.
This has the following properties:
\benum
    \item $\limsup_{t\to\infty} B(t)=\infty$ and $\liminf_{t\to\infty}B(t)=-\infty$ with probability $1$.
        This is since for every $h>0$, $B(nh)$ is a general random process and so either for every $n$ it is equal to $0$, or $B(nh)\to\pm\infty$ or $\limsup B(nh)=\infty$ and $\liminf B(nh)=-\infty$.
        The first is not true since then $B(kh)-B((k-1)h)=0$ with probability $1$, which is not true.
        The second and third can be ignored by symmetry.
    \item $B(t)$ is transient: it visits every open interval an infinite number of times.
        This is due to the above point, as since $B(t)$ is continuous if there is an open interval $(a,b)$ which it visits a finite number of times, then after the final time it must always be above $b$
        or below $a$, and so $\liminf B(t)\geq b$ or $\limsup B(t)\leq a$ in contradiction.
    \item $\limsup\frac{\abs{B(t)}}{\sqrt t}\geq1$ almost surely.
    \item $B(t)\sim{\cal N}(0,t)$ since $B(t)-B(0)\sim{\cal N}(0,t)$ and $B(0)=0$.
    \item $\Covof{B(t),B(s)}=\minof{t,s}$.
        Suppose $s\leq t$ then $B(t)-B(s)\sim{\cal N}(0,t-s)$ and $B(t)\sim{\cal N}(0,t)$ and $B(s)\sim{\cal N}(0,s)$.
        This means that $\expecof{B(t)}=\expecof{B(s)}=0$ so $t=\Varof{B(t)}=\expecof{B(t)^2}$ and similarly $s=\expecof{B(s)^2}$ and $t-s=\expecof{(B(t)-B(s))^2}$.
        Thus
        \multlines{
            \Covof{B(t),B(s)} = \expecof{B(t)B(s)} = \expecof{-\frac{(B(t)-B(s))^2-B(t)^2-B(s)^2}2}\cr
            &= -\frac{\expecof{(B(t)-B(s))^2}-\expecof{B(t)^2}-\expecof{B(s)^2}}2 = -\frac{(t-s)-t-s}2 = s
        }
        as required.
\eenum

Now suppose $B(t)$ is Brownian motion, then we generally study it by studying the marginal distributions (the distribution functions) of every finite sampling of $B(t)$, ie. the distribution of
$\bigl(B(t_1),\dots,B(t_n)\bigr)$ for every $0\leq t_1<\cdots<t_n$.
This is essentially what is dictated in the first three conditions of Brownian motion, but the fourth condition on continuity cannot be proven by the study of these distributions.
This is since if $U\sim{\rm Unif}([0,1])$ then defining
$$ \tilde B(t) = \cases{B(t) & $t\neq U$\cr 0 & $t=U$} $$
gives us a collection of random variables $\tilde B(t)$ which have the same marginal distributions as $B(t)$ but is almost surely not continuous.

Now recall the following properties of normal distributions:
$$ \displaylines{
    Z\sim{\cal N}(\mu,\sigma^2)\implies \expecof Z=\mu,\ \Varof Z=\sigma^2\cr
    Z_1\sim{\cal N}(\mu_1,\sigma_1^2),\ Z_2\sim{\cal N}(\mu_2,\sigma_2^2)\hbox{ are independent} \implies Z_1+Z_2\sim{\cal N}(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)
} $$
Now for another, if $Z\sim{\cal N}(0,1)$ then then for every $0<t$, we have the following series of inequalities:
$$ \frac1{\sqrt{2\pi}}\parens{\frac1t-\frac1{t^3}}e^{-t^2/2} \leq \probof{Z>t} \leq \frac1{\sqrt{2\pi}}\frac1te^{-t^2/2} $$
This is since
$$ \eqalign{
    \probof{Z>t} &= \frac1{\sqrt{2\pi}}\int_t^\infty e^{-u^2/2}\,du\cr
    &= \frac1{\sqrt{2\pi}}\int_0^\infty e^{-\parens{t+\frac vt}^2/2}\cdot\frac1t\,dv\qquad(\hbox{substituting $u=t+\frac vt$})\cr
    &= \frac1{\sqrt{2\pi}}\frac{e^{-t^2/2}}t\int_0^\infty e^{-v-\frac{v^2}{2t^2}}\,dv
} $$
Since $1-x\leq e^{-x}\leq1$ for $x>0$ so we have $1-\frac{v^2}{2t^2}\leq e^{-\frac{v^2}{2t^2}}\leq1$ and so
$$ \eqalign{
    \int_0^\infty e^{-v-\frac{v^2}{2t^2}}\,dv &\leq \int_0^\infty e^{-v}\,dv = 1\cr
    \int_0^\infty e^{-v-\frac{v^2}{2t^2}}\,dv &\geq \int_0^\infty e^{-v}-\frac{v^2}{2t^2}e^{-v}\,dv = 1 - \frac1{2t^2}\int_0^\infty v^2e^{-v}\,dv = 1-\frac1{t^2}
} $$
which finishes the proof.

Now, our samplings are of the form $\bigl(B(t_1),\dots,B(t_n)\bigr)\in{\bb R}^n$ so we have to now understand vectors of normal distributions: multi-normal vectors (also known as Gaussian vectors).

\bdefn

    If $z_1,\dots,z_n$ are all independent and have the distribution ${\cal N}(0,1)$ then the vector $Z=(z_1,\dots,z_n)$ has the {\emphcolor standard normal distribution in ${\bb R}^n$}.
    This is denoted $Z\sim{\cal N}_n(0,I)$.
    And a vector of random variables $X=(x_1,\dots,x_n)$ is called {\emphcolor Gaussian} (or multi-normal) if there exists a matrix $A\in M_{n\times m}({\bb R})$ and a vector $\mu\in{\bb R}^n$ such that
    $$ \pmatrix{x_1\cr\vdots\cr x_n} \buildrel d\over= A\pmatrix{z_1\cr\vdots\cr z_m} + \mu $$
    where $Z\sim{\cal N}_m(0,I)$.
    $A$ is called the {\emphcolor transition matrix} and $\mu$ the {\emphcolor expected value vector}.
    The {\emphcolor covariance matrix} of $X$ is defined to be $\Sigma_{ij}=\Covof{x_i,x_j}$.

\edefn

This means that
$$ x_i = \sum_{j=1}^m A_{ij}z_j + \mu_i $$
and since $z_j\sim{\cal N}(0,1)$ are independent, this means $x_i\sim{\cal N}\parens{\mu_i,\sum_{j=1}^m A_{ij}^2}$.
Thus our definition of Gaussian vectors are equivalent to just having a vector of normal random variables.

Notice then that $((X-\mu)\cdot(X-\mu)^\top)_{ij}=(x_i-\mu_i)(x_j-\mu_j)$ and $\expecof{x_i}=\mu_i$ (since $x_i$ is some linear combination of $z_j$s and $\mu_i$, and $\expecof{z_j}=0$), so
$$ \Sigma_{ij}=\expecof{(x_i-\mu_i)(x_j-\mu_j)}=\expecof{(X-\mu)(X-\mu)^\top}_{ij} = \expecof{AZZ^\top A^\top} = A\expecof{ZZ^\top}A^\top = AA^\top $$
the final equality is since $\expecof{ZZ^\top}_{ij}=\Covof{z_i,z_j}=\delta_{ij}$ so $\expecof{ZZ^\top}=I$.
Meaning that
$$ \Sigma = AA^\top $$
Now notice that $AA^\top$ is invertible if and only if $A^\top$ has full column rank, meaning $A$ has full row rank.
If it does not have full column rank then there exists an $x$ such that $A^\top x=0$ and so $\Sigma x=0$ for $x\neq0$ so $\Sigma$ is not invertible.
And if $\Sigma$ is not invertible then there exists an $x\neq0$ such that $AA^\top x=0$ and so $A^\top x$ is in $A$'s nullspace.
But the nullspace of $A$ and $A^\top$'s range are orthogonal complements and so $A^\top x=0$ meaning $A^\top$ does not have full column rank.

Thus if $\Sigma$ is invertible, then $A$ has full row rank, and so we can assume that it is invertible.

\bprop

    If $\Sigma$ is invertible then $X$ has a density
    $$ f_\Sigma(x) = \frac1{\sqrt{2\pi}^n}\frac1{\sqrt{\det\Sigma}}e^{-(x-\mu)^\top\Sigma^{-1}(x-\mu)/2} $$

\eprop

Since $\Sigma=AA^\top$ and $Z\sim{\cal N}_n(0,I)$ are independent (let the densities of its coefficients be $f_i$), then
$$ f_I(z) = \prod_{i=1}^n f_i(z_i) = \prod_{i=1}^n \frac1{\sqrt{2\pi}}e^{-z_i^2/2} = \frac1{\sqrt{2\pi}^n}e^{-\sum_{i=1}^nz_i^2/2} = \frac1{\sqrt{2\pi}^n}e^{-z^\top z} $$
Now if $x=Az+\mu$ then $z=A^{-1}(x-\mu)$ and so $dz=\det(A^{-1})\,dx$ by the Jacobian.
And $\det(A^{-1})=\frac1{\det A}=\frac1{\sqrt{\det\Sigma}}$, so for every event $E$,
$$ \probof{Z\in E} = \int_E f_I(z)\,dz = \int_{E'=AE+\mu}f_I(A^{-1}(x-\mu))\frac1{\det A}\,dx = \int_{E'}\frac1{\sqrt{2\pi}^n}\frac1{\sqrt{\det\Sigma}}e^{-(x-\mu)^\top(A^{-1})^\top A^{-1}(x-\mu)/2}\,dx $$
Since $(A^{-1})^\top A^{-1}=\Sigma^{-1}$ we get that
$$ \probof{X\in E'} = \probof{Z\in E} = \int_{E'}\frac1{\sqrt{2\pi}^n}\frac1{\sqrt{\det\Sigma}}e^{-(x-\mu)^\top\Sigma^{-1}(x-\mu)/2}\,dx $$
as required.
\qed

Notice that a Gaussian vector may have multiple transition matrices (for example if $Z\sim{\cal N}_n(0,I)$ and $P$ is orthogonal then $PZ\buildrel d\over=Z$). 
So we denote the distribution of $X$ by its covariance matrix and expected variable vector, which are unique to $X$:
$$ X\sim{\cal N}(\mu,\Sigma) $$

Here are some properties of Gaussian vectors:
\benum
    \item If $X\sim{\cal N}(\mu,\Sigma)$ then $\expecof{X_i}=\mu_i$ and $\Covof{X_i,X_j}=\Sigma_{ij}$ (shown/by definition),
    \item If $X\sim{\cal N}(\mu_1,\Sigma_1)$ and $Y\sim{\cal N}(\mu_2,\Sigma_2)$ then $X+Y\sim{\cal N}(\mu_1+\mu_2,\Sigma_1+\Sigma_2)$.
        This results directly from the linearity of expected values and covariance.
    \item If $X$ is Gaussian and $B$ is a matrix, then $BX$ is Gaussian (since $BX=BAZ+B\mu$),
    \item Conditioning a Gaussian vector on the value of some of its coordinates, or their values on a linear combination of coordinates, is still Gaussian,
    \item If $X\sim{\cal N}(\mu,\Sigma)$ then there exists an upper (or lower) triangle matrix $U$ which serves as its transition matrix.
\eenum

\subsection{Wiener Process}

We will construct Brownian motion as the limit of continuous random functions on $[0,1]$.
We will then continue this construction onto the intervals $\set{[n,n+1]}_{n\in{\bb Z}}$, but for now we focus on $[0,1]$.
Let us define
$$ D = \bigcup_{n=0}^\infty D_n,\qquad D_n = \set{\frac k{2^n}}[0\leq k\leq 2^n] $$
$D$ is a dense countable subset of $[0,1]$.
Then let $\set{Z_t}_{t\in D}$ be a set of independent random variables which distribute ${\cal N}(0,1)$.
Then let us define $B(0)=0$ and $B(1)=Z_1$ and for every $d\in D_n\setminus D_{n-1}$,
$$ B(d) = \frac{B(d-2^{-n}) + B(d+2^{-n})}2 + \frac{Z_d}{\sqrt{2^{n+1}}} $$
This is since $d=\frac k{2^n}$ for some odd $k$, and so $d+2^{-n}=\frac{k+1}{2^n}$ which is of the form $\frac{k'}{2^{n-1}}$ since $k+1$ is even, so $d\pm2^{-n}\in D_{n-1}$ so this is inductive definition
is well-defined.
Then $B(d)$ is continuous on $D$ and can therefore be uniquely extended (since $D$ is dense) to a continuous function on all of $[0,1]$.
We will show inductively that
\benum
    \item For every $r<s<t$ in $D_n$, $B(t)-B(s)$ and $B(s)-B(r)$ are independent, and $B(t)-B(s)\sim{\cal N}(0,t-s)$.
    \item The set $\set{B(d)}[d\in D_n]$ is independent of $\set{Z_t}[t\in D\setminus D_n]$ (this is obvious from the construction).
\eenum
For $n=0$ this is true trivially.
Let $d\in D_n\setminus D_{n-1}$, then by the inductive assumption
$$ X = \frac12\parens{B\parens{d+\frac1{2^n}}-B\parens{d-\frac1{2^n}}}\sim{\cal N}\parens{0,\frac1{2^{n+1}}} $$
and this is independent of $\set{Z_t}[t\in D\setminus D_{n-1}]$ and in particular $Z_d$.
Similarly
$$ Y = \frac1{\sqrt{2^{n+1}}}Z_d \sim {\cal N}\parens{0,\frac1{2^{n+1}}} $$
And so $X+Y$ and $X-Y$ are independent and distribute ${\cal N}(0,2^{-n})$.
But
$$ X + Y = B(d) - B\parens{d-\frac1{2^n}},\qquad X - Y = B\parens{d+\frac1{2^n}} - B(d) $$
And so $\set{B(d)-B(d-2^{-n})}_{0\neq d\in D_n}$ are independent (since for Gaussian vectors, pairwise independence implies independence).

For every $d\in D_n$ let us define $G_n(d)=B(d)$, and $G_n$ is linear between points in $D_n$ (so it is continuous).

\blemm

    Let $\sqrt{2\log 2}<c$ then
    $$ \probof{(\exists N)(\forall n\geq N)(\forall d\in D_n)\,\abs{Z_d}<c\sqrt n} = 1 $$

\elemm

Let us define
$$ A_n = \set{(\forall d\in D_n)\, \abs{Z_d}<c\sqrt n}\implies A_n^c = \set{(\exists d\in D_n)\, \abs{Z_d}\geq c\sqrt n} $$
Then
$$ \sum_{n=1}^\infty \probof{A_n^c} = \sum_{n=1}^\infty \probof{(\exists d\in D_n)\, \abs{Z_d}\geq c\sqrt n} \leq \sum_{n=1}^\infty\sum_{d\in D_n}\probof{\abs{Z_d}\geq c\sqrt n} $$
Since $Z_n\sim{\cal N}(0,1)$, $\probof{\abs{Z_d}\geq c\sqrt n}\leq\frac2{\sqrt{2\pi}}\frac1{c\sqrt n}e^{-c^2n/2}$ (we showed this before).
So
$$ \leq \sum_{n=1}^\infty(2^n+1)e^{-c^2n/2} $$
Since $c>\sqrt{2\log2}$, $e^{-cn^2/2}\leq 2^{-2n}$, so this series converges.
By the \refmath{bclemm} this means that $\probof{A_n^c\hbox{ i.o.}}=0$ so $\probof{A_n\hbox{ a.e.}}=1$, which is precisely the probability we're trying to compute.
\qed

And so
$$ \sup_{t\in[0,1]}\abs{G_n(t)-G_{n-1}(t)} \leq \sup_{d\in D_n}\frac{\abs{Z_d}}{\sqrt{2^{n+1}}} \buildrel as\over< c\sqrt{\frac n{2^{n+1}}} $$
the first inequality is since the difference is bound by when $t\in D_n\setminus D_{n-1}$ in which case the difference is $\frac{\abs{Z_d}}{\sqrt{2^{n+1}}}$ (since
$G_{n-1}(t)=\frac{B(d-2^{-n})+B(d+2^{-n})}2$).
The final inequality is due to the above lemma.

Finally we define
$$ G_\infty(t) = \lim_{n\to\infty}G_n(t) = \sum_{n=1}^\infty\bigl(G_n(t) - G_{n-1}(t)\bigr) $$
The rightmost series does converge almost surely since it is bound by $\sum c\sqrt{\frac n{2^{n+1}}}$.
This means that by the Weierstrass M-test, $\sum G_n-G_{n-1}$ converges uniformly to $G_\infty$, and so $G_\infty$ is a continuous function.
We claim that $G_\infty$ is indeed our Brownian motion (in $[0,1]$), so we now denote it by $B(t)=G_\infty(t)$.

Let $t_1<\cdots<t_n$ in $[0,1]$, then let $t_{1,k}<\cdots<t_{n,k}$ in $D$ such that $t_i=\lim_{k\to\infty}t_{i,k}$.
By the continuity of $B$, we have $B(t_{i+1})-B(t_i)=\lim_{k\to\infty}B(t_{i+1,k})-B(t_{i,k})$.

\bprop

    If $\set{X_n}_{n=1}^\infty$ is a sequence of Gaussian vectors such that $\lim_{n\to\infty}X_n\buildrel as\over=X$ then if the limits $\mu=\lim\expecof{X_n}$ and $\Sigma=\lim\Covof{X_n}$ exist, then
    $X\sim{\cal N}(\mu,\Sigma)$.

\eprop

Then since $B(t_{i+1,k})-B(t_{i,k})\sim{\cal N}(0,t_{i+1,k}-t_{i,k})$ we get by this above proposition, $B(t_{i+1})-B(t_i)\sim{\cal N}(0,t_{i+1}-t_i)$.

Now in order to continue $B$ to $[n,n+1]$ for all $n\in{\bb N}$, we continue this construction (though $B(n)$ must not be redefined).

\bdefn

    We provide an equivalent definition of Brownian motion: $\set{B(t)}_{t\geq0}$ is Brownian motion starting at $x\in{\bb R}$ if
    \benum
        \item $B(0)\buildrel as\over=x$,
        \item It is a Gaussian process: for every $0\leq t_1<\cdots<t_n$, $(B(t_1),\dots,B(t_n))$ is a Gaussian vector,
        \item For every $t,s$, $\expecof{B(t)}=0$ and $\expecof{B(t)B(s)}=\minof{t,s}$.
        \item $t\varmapsto B(t)$ is continuous.
    \eenum

\edefn

We have already shown that the first definition implies this one.
Now suppose $B(t)$ satisfies this definition, then let $s\leq t\leq u$ then since $(B(s),B(t),B(u))$ is a Gaussian vector, so is $(B(u)-B(t),B(s))$ (as it is equal to the product of the original Gaussian
vector and a matrix).
Then by the minimum property
$$ \Covof{B(u)-B(t),B(s)} = \expecof{(B(u)-B(t))B(s)} = \expecof{B(u)B(s)} - \expecof{B(t)B(s)} = s - s = 0 $$
In a Gaussian vector, if two coordinates are uncoorelated then they are independent, and so $B(t)-B(s)$ and $B(u)$ are independent, so we have shown that differences are independent.

And let $t,h>0$ then $(B(t),B(t+h))$ is Gaussian and therefore $B(t+h)-B(t)$ must have a normal distribution as well.
Now
$$ \expecof{B(t+h) - B(t)} = \expecof{B(t+h)} - \expecof{B(t)} = 0 $$
and
$$ \Varof{B(t+h) - B(t)} = \expecof{B(t+h)^2} - 2\expecof{B(t+h)B(t)} + \expecof{B(t)^2} = t+h - 2t + t = h $$
so $B(t+h)-B(t)\sim{\cal N}(0,h)$ as required.
So we have shown the equivalence of these two definitions.

\bye

