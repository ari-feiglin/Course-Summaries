\input pdfToolbox

\setlayout{horizontal margin=1.5cm, vertical margin=1.5cm}
\parindent=0cm
\parskip=3pt plus 2pt minus 2pt

\input pdfmsym
\input preamble

\pdfmsymsetscalefactor{10}

\footline={}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pageborder{rgb{1 .5 .5}}{rgb{1 0 0}}{5}

\color rgb{1 0 0}

{\def\boxshadowcolor{rgb{.8 0 0}}
\bppbox{rgb{1 .7 .7}}{rgb{1 0 0}}{rgb{.8 0 0}}

    \centerline{\setfontandscale{bf}{20pt}Introduction to Stochastic Processes}
    \smallskip
    \centerline{\setfont{it}Dr. Naomi Feldheim, \setfont{tt}naomi.feldheim@biu.ac.il}
    \centerline{\setfont{it}Summary by Ari Feiglin}

\eppbox

\bigskip

\bppbox{rgb{1 .7 .7}}{rgb{1 0 0}}{rgb{.8 0 0}}
\section*{Contents}

\tableofcontents
\eppbox

}

\vfill\break

\color{black}

\pageborder{rgb{1 .5 .5}}{rgb{1 0 0}}{5}
\null
\vfill\break

\pageno=1
\newif\ifpageodd
\pageoddtrue
\headline={%
    \hbox to \hsize{\color{black}%
        \ifpageodd\hfil{\it\currsubsection\quad\bf\folio}\global\pageoddfalse%
        \else{\bf\folio\quad\it\currsubsection}\hfil\global\pageoddtrue\fi%
    }%
}

\section{Introduction}

This course will focus on tools which can be used to study random processes.
A random process is a sequence of random variables which represent measurements of the process.
Examples of random processes are random walks (these are commonly described as the path a drunk man would take while trying to get home), card shuffles (which can be viewed as choosing a card and placing
it randomly in the deck), and branching (for example the population of bunnies in a specific area: the random variable being the number of bunnies in each generation).

\subsection{Markov Chains}

\bdefn

    A {\emphcolor discrete-time Markov process}\addtoindex{markov process}[discrete-time] is a sequence of random variables $\set{X_n}_{n\geq0}$.
    This sequence is called a {\emphcolor Markov chain}\addtoindex{markov chain} on a set of states $S$ if:
    \benum
        \item For every $n$, $X_n\in S$ almost surely (meaning $\probof{X_n\in S}=1$),
        \item For every $n\geq0$ and for every $s_0,\dots,s_{n+1}\in S$,
        $$\probof{X_{n+1}=s_{n+1}}[X_0=s_0,\dots,X_n=s_n]=\probof{X_{n+1}=s_{n+1}}[X_n=s_n]$$
        ie. the probability of the next measurement being some arbitrary value is dependent only on the previous measurement.
        This is only necessary if $\probof{X_0=s_0,\dots,X_n=s_n}>0$.
    \eenum

\edefn

In this course $S$ will always be countable.
We can also write the second condition using distributive equivalence:
$$ X_{n+1}\vert X_0,\dots,X_n \deq X_{n+1}\vert X_n $$

Notice how the Markov property can be strengthened in various ways, for example if $n>m$ then

\medskip
{\tabskip=0pt
\openup1\jot\halign to\hsize{$\displaystyle#$\hfil\tabskip=0pt plus 1fil&$\displaystyle#$\hfil\tabskip=0pt\cr
    \omit\span\omit$\displaystyle\probof{X_n=s_n}[X_{n-1}=s_{n-1},\dots,X_m=s_m]$\cr
    &= \sum_{s_m,\dots,s_0}\probof{X_n=s_n}[X_{n-1}=s_{n-1},\dots,X_0=s_0]\cdot\probof{X_{m-1}=s_{m-1},\dots,X_0=s_0}[X_{n-1}=s_{n-1},\dots,X_m=s_m]\cr
    &= \probof{X_n=s_n}[X_{n-1}=s_{n-1}]\cdot\sum\probof{X_{m-1}=s_{m-1},\dots,X_0=s_0}[X_{n-1}=s_{n-1},\dots,X_m=s_m]\cr
    &= \probof{X_n=s_n}[X_{n-1}=s_{n-1}]\cr
}}
\medskip

This can be viewed as the base case for
$$ \probof{X_{n+k}=s_{n+k}}[X_n=s_n,\dots,X_m=s_m] = \probof{X_{n+k}=s_{n+k}}[X_n=s_n,\dots,X_{m'}=s_{m'}] $$
where $m'<m$.
This is since for $k=1$, both of these are equal to $\probof{X_{n+1}=s_{n+1}}[X_n=s_n]$.
The induction step follows by

\medskip
{\tabskip=0pt
\openup1\jot\halign to\hsize{$\displaystyle#$\hfil\tabskip=0pt plus 1fil&$\displaystyle#$\hfil\tabskip=0pt\cr
    \omit\span\omit$\displaystyle\probof{X_{n+k+1}=s_{n+k+1}}[X_n=s_n,\dots,X_m=s_m]$\cr
    &= \sum_{s_{n+1}}\probof{X_{n+k+1}=s_{n+k+1}}[X_{n+1}=s_{n+1},\dots,X_m=s_m]\cdot\probof{X_{n+1}=s_{n+1}}[X_n=s_n,\dots,X_m=s_m]\cr
    &= \sum_{s_{n+1}}\probof{X_{n+k+1}=s_{n+k+1}}[X_{n+1}=s_{n+1},\dots,X_{m'}=s_{m'}]\cdot\probof{X_{n+1}=s_{n+1}}[X_n=s_n,\dots,X_{m'}=s_{m'}]\cr
    &= \probof{X_{n+k+1}=s_{n+k+1}}[X_n=s_n,\dots,X_{m'}=s_{m'}]\cr
}}
\medskip

By taking $m'=0$ and $m=n$ we get $\probof{X_{n+k}=s_{n+k}}[X_n=s_n] = \probof{X_{n+k}=s_{n+k}}[X_n=s_n,\dots,X_0=s_0]$, or in other words for all $m<n$,
$$ \probof{X_n=s_n}[X_m=s_m,\dots,X_0=s_0] = \probof{X_n=s_n}[X_m=s_m] $$

This can be even further strengthened: let $\varnothing\neq B\subseteq\set{0,\dots,n-1}$ and $m=\max B$ then
$$ \probof{X_n=s_n}[\forall i\in B\colon X_i=s_i] = \probof{X_n=s_n}[X_m=s_m] $$
To prove this let $C=\set{0,\dots,m}\setminus B$ then
$$ \eqalign{
    \probof{X_n=s_n}[\forall i\in B\colon X_i=s_i] &= \sum_{(s_i)_{i\in C}\in S^C}\probof{X_n=s_n}[X_m=s_m,\dots,X_0=s_0]\cdot\probof{\forall i\in C\colon X_i=s_i}[\forall i\in B\colon X_i=s_i]\cr
    &= \probof{X_n=s_n}[X_m=s_m]\cdot\sum\probof{\forall i\in C\colon X_i=s_i}[\forall i\in B\colon X_i=s_i]\cr
    &= \probof{X_n=s_n}[X_m=s_m]\cr
} $$

A consequence of this is that if $\set{X_n}_{n\geq0}$ is a Markov chain and $\set{a_n}_{n\geq0}$ is strictly monotonic then $Y_n=X_{a_n}$ is also a Markov chain.
After all if we let $B=\set{a_{n-1},\dots,a_0}$ then $\max B=a_{n-1}$ and so
\multlines{
    \probof{Y_n=s_{a_n}}[Y_{n-1}=s_{a_{n-1}},\dots,Y_0=s_{a_0}] = \probof{X_{a_n}=s_{a_n}}[\forall i\in B\colon X_i=s_i] = \probof{X_{a_n}=s_{a_n}}[X_{a_{n-1}}=s_{a_{n-1}}]\cr
    &= \probof{Y_n=s_{a_n}}[Y_{n-1}=s_{a_{n-1}}]
}
as required.

\bdefn

    For a Markov chain $\set{X_n}_{n\geq0}$ on a finite set of states $S$, we define the {\emphcolor adjacency matrix} at the $n$th measurement by
    $$ P^{(n)}_{ij} = \probof{X_n=j}[X_{n-1}=i] $$
    for $i,j\in S$.
    This is also sometimes written as $P_n(i\to j)$ (the probability measuring $i$ on the $n-1$th measurement gives $j$ on the next).
    If $P^{(n)}$ is the same for all $n$, then we say that the chain is {\emphcolor homogeneus in time}, and we generally write $P$ in place of $P^{(n)}$.

\edefn

For example, suppose a frog is hopping between $N$ leaves.
The frog can hopping from every leaf to every other leaf, and it always chooses a leaf in an independent and uniform manner.
This defines a Markov chain where the states are the leaves, and $X_n$ is the leaf the frog is on after $n$ hops.
This Markov chain is even homogeneus since the frog makes its choices in a manner which does not take the current number of hops into account.
The adjacency matrix is defined by
$$ P_{ij} = \cases{\frac1{N-1} & $i\neq j$\cr 0 & $i=j$} $$
This is the simple random process on the complete graph of $N$ vertices, $K_N$.

Suppose $N=4$, and supppose that at the beginning the frog is on either the first or second leaf with equal probability.
What is the probability that after one hop the frog is on the fourth leaf?
The following notation will be used: $X\sim(a_0,\dots,a_n)$ will be used to mean $\probof{X=s_i}=a_i$, where $s_i$ is some understood ordering of the set of states $S$.
Then
$$ \probof{X_1=j}[X_0\sim\parens{\frac12,\frac12,0,0}] = \probof{X_1=j}[X_0=1]\cdot\frac12 + \probof{X_1=j}[X_0=2]\cdot\frac12 $$
as the rest of the terms are zero.
For $j=4$ we get that this is equal to $\frac13$.
Notice that we can generalize this and get
$$ \probof{X_{n+1}=j}[X_n\sim\vec v] = \sum_{i\in S}\probof{X_{n+1}=j}[X_n=i]\cdot\probof{X_n=i} = \sum_{i\in S}P^{(n+1)}_{ij}\vec v_i = (\vec v\cdot P^{(n+1)})_j $$
So we have proven the following:

\bprop

    If $X_n\sim\vec v$ then $X_{n+1}\vert X_n\sim\vec v\cdot P^{(n+1)}$, and so $X_n\vert X_0\sim\vec v\cdot P^{(n)}\cdots P^{(1)}$.
    In particular if the Markov chain is homogeneus, $X_n\vert X_0\sim\vec v\cdot P^n$.

\eprop

This simplifies dealing with Markov chains, especially homogeneus ones.

\bexam

    Suppose $\set{Y_n}_{n=1}^\infty$ is a sequence of random variables which have the distribution $Y_n\sim\Berof{\frac1n}$ (recall that $X\sim\Berof p$ means that $X$ is $1$ with probability $p$ and zero
    otherwise).
    And we define $X_n=\chi\set{(\exists m\leq n)\,Y_m=1}$, the indicator of the set of all values such that there is an index before $n$ where $Y_m=1$ ($\chi_S$ is the {\it indicator function} of the set
    $S$, defined by $\chi_S(x)=1$ for $x\in S$ and zero otherwise).
    We will prove $X_n$ is a Markov chain.
    Notice that
    $$ X_n = \chi\set{(\exists m\leq n)\,Y_m=1} = \chi\set{(\exists m\leq n-1)\,Y_m=1}\lor\chi\set{Y_n=1} = X_{n-1}\lor\chi\set{Y_n=1} $$
    $\lor$ is bitwise or, or equivalently the maximum.
    And therefore we get that $X_n=\bigvee_{i=1}^n\chi\set{Y_i=1}$.
    This means that if $X_{n-1}=1$ then $X_n=1$, and if $X_{n-1}=0$ then $X_n=1$ if and only if $Y_n=1$.
    And so $X_n$'s value depends only on $X_{n-1}$'s and not any previous $X_i$.
    So $\set{X_n}_{n=1}^\infty$ is indeed a Markov chain.

    Notice that
    $$ \displaylines{
        \probof{X_n=0}[X_{n-1}=0] = \probof{Y_n=0} = \frac{n-1}n,\quad \probof{X_n=1}[X_{n-1}=0] = \probof{Y_n=1} = \frac1n,\quad\cr
        \probof{X_n=0}[X_{n-1}=1] = 0,\quad \probof{X_n=1}[X_{n-1}=1] = 1
    } $$
    And so we get that
    $$ P^{(n)} = \pmatrix{\frac{n-1}n & \frac1n \cr 0 & 1} $$

\eexam

\bdefn

    A real $n\times n$ matrix $P$ such that $P_{ij}\geq0$ for every $i,j$, and for every row $i$ we have $\sum_{j=1}^n P_{ij}=1$ then $P$ is called an {\emphcolor stochastic matrix}.

\edefn

Notice that we can draw a diagram for every stochastic matrix and it will be the transition matrix of a Markov chain.
Meaning every stochastic matrix is the transition matrix of some Markov chain, and every transition matrix is stochastic.
Notice that the second condition for a matrix to be stochastic can be written as $P{\bf1}={\bf1}$ where ${\bf1}=(1,\dots,1)^\top$.

\bdefn

    Let $\set{X_n}_{n\geq0}$ be a Markov chain over a state space $S$, and let $A\subseteq S$.
    Then we define the {\emphcolor hitting time} to $A$ to be the random variable
    $$ T_A = \minof{t\geq1}[X_t\in A] $$
    Note that if $X_t$ is never in $A$ then $T_A$ can be $\infty$, and so $T_A$ is a function from the probability space to the extended reals: $\Omega\longto{\bb R}\cup\set\infty$.
    This means that $T_A^{-1}\set\infty$ must also be measurable (an event).

\edefn

In the case that $A$ is a singleton $A=\set a$ then we write $T_a$ in place of $T_A$.
Notice that $T_A$ measures starting from $t=1$, while it is possible that the initial condition is in $A$, ie. $X_0\in A$.
So in the case that $X_0\in A$, $T_A$ measures the {\it return time} to $A$, in particular if $X_0\sim\delta_a$ where $\delta_a=(0,\dots,1,\dots,0)$ ($1$ is at the index corresponding to the state $a$).
We also use the following notation
$$ \probof[V]E = \probof E[X_0\sim V],\qquad \probof[\delta_a]E = \probof[a]E = \probof{E}[X_0=a] $$

If $P$ is the transition matrix of a homogeneus Markov chain, then $P^n(a\to b)$ means $P^n_{ba}=\probof{X_n=b}[X_0=a]$.

\blemm

    If $\set{X_n}$ is a homogeneus Markov chain, then
    $$ P^n(a\to b)=\sum_{m=1}^n\probof[a]{T_b=m}P^{n-m}(b\to b) $$

\elemm

\multlines{%
    P^n(a\to b) = \probof[a]{X_n=b} = \probof{\bigdcup_{m=1}^n\set{T_b=m},X_n=b}[X_0=b] = \sum_{m=1}\probof{T_b=m,X_n=b}[X_0=b]\cr
    &= \sum_{m=1}^n\probof{X_n=b}[T_b=m,X_0=a]\cdot\probof{T_b=m}[X_0=a]\cr
}
Now, $\probof{X_n=b}[T_b=m,X_0=a]=\probof{X_n=b}[X_m=b,X_{m-1}\neq b,\dots,X_1\neq b,X_0=a]=\probof{X_n=b}[X_m=b]$ by the Markov property.
Since $\set{X_n}$ is homogeneus this is just equal to $P^{n-m}(b\to b)$.
Thus this formula is equal to
$$ \sum_{m=1}^b\probof{X_n=b}[X_m=b]\cdot\probof[a]{T_b=m} = \sum_{m=1}^b P^{n-m}(b\to b)\cdot\probof[a]{T_b=m} \qed $$

Let us introduce some more notation:
$$ f_{a\to b} = \probof{T_b<\infty}[X_0=a],\qquad f_{a\to a}=f_a=\probof{T_a<\infty}[X_0=a] $$
thus $f_{a\to b}$ is the probability that if we start at $a$, we eventually reach $b$.

\blemm

    $f_{a\to c}\geq f_{a\to b}\cdot f_{b\to c}$

\elemm

Notice that $\set{T_c<\infty}=\set{(\exists t>0)X_t=c}\supseteq\bigdcup_{k>0}\set{T_b=k,(\exists t>k)X_t=c}$.
Thus we get
\lmultlines{
    f_{a\to c} = \probof{T_c<\infty}[X_0=a] \geq \sum_{k=1}^\infty\probof{T_b=k,(\exists t>k)X_t=c}[X_0=a]\cr
    &= \sum_{k=1}^\infty\probof{T_b=k}[X_0=a]\cdot\probof{(\exists t>k)X_t=c}[T_b=k,X_0=a]\cr
    &= \sum_{k=1}^\infty\probof{T_b=k}[X_0=a]\cdot\probof{(\exists t>k)X_t=c}[X_k=b,X_{k-1}\neq b,\dots,X_1\neq b,X_0=a]\cr
    &\llap{(Markov property) }= \sum_{k=1}^\infty\probof{T_b=k}[X_0=a]\cdot\probof{(\exists t>k)X_t=c}[X_k=b]\cr
    &\llap{(homogeneity) }= \sum_{k=1}^\infty\probof{T_b=k}[X_0=a]\cdot\probof{(\exists t>0)X_t=c}[X_0=b]\cr
    &= \sum_{k=1}^\infty\probof{T_b=k}[X_0=a]\cdot f_{b\to c} = f_{a\to b}\cdot f_{b\to c}\hfill\mathchar"404\cr
}

In particular this means
$$ f_a\geq f_{a\to b}\cdot f_{b\to a} $$

For every $a\in S$ we define the random variable $N(a)=\sum_{n=1}^\infty\chi\set{X_n=a}$, which is the number of times the state $a$ is visited from time $1$ and onward.
When $X_0\sim V$ we write $N_V(a)$.
Notice then that $f_{a\to b}=\probof{N(b)\geq1}[X_0=a]$ and so $f_a=\probof{N(a)\geq1}[X_0=a]$.

\bprop

    $\probof{N(a)\geq k}[X_0=a]=f_a^k$

\eprop

We prove this by induction, for $k=1$ this is simply what we just said.
Now
\lmultlines{
    \probof{N(a)\geq k+1}[X_0=a] = \sum_{m=1}^\infty\probof{T_a=m,\abs{\set{j>m}[X_j=a]}\geq k}[X_0=a]\cr
    &\llap{(Markov property) }= \sum_{m=1}^\infty\probof{T_a=m}[X_0=a]\cdot\probof{\abs{\set{j>m}[X_j=a]}\geq k}[X_m=a]\cr
    &\llap{(homogeneity) }=\sum_{m=1}^\infty\probof{T_a=m}[X_0=a]\cdot\probof[a]{N(a)\geq k}\cr
    &\llap{(induction) }=f_a^k\sum_{m=1}^\infty\probof{T_a=m}[X_0=a] = f_a^{k+1}\hfill\mathchar"404\cr
}

Notice then that
$$ \probof{N(a)=k}[X_0=a] = \probof[a]{N(a)\geq k} - \probof[a]{N(a)\geq k+1} = f_a^k - f_a^{k+1} = f_a^k(1-f_a) $$
Thus $N_a(a)\sim\Geoof{1-f_a}-1$ (the $+1$ is since $X\sim\Geoof p$ means $\probof{X=k}=p(1-p)^{k-1}$).
Thus
$$ \expecof{N_a(a)} = \frac1{1-f_a} - 1 = \frac{f_a}{1-f_a} $$

\bdefn

    A state $b\in S$ is {\emphcolor recurrent} if $f_b=1$, equivalently if $\probof[b]{T_b<\infty}$ (the probability of returning to $b$ is $1$).
    A non-recurrent state is called {\emphcolor transient}.
    $b$ is {\emphcolor absorbing} if $P(b\to b)=1$.

\edefn

Notice that if $b$ is recurrent then if $f_b=1$, $N_b(b)\sim\Geoof{0}-1$, meaning $\probof[b]{N(b)=\infty}=1$.
And if $b$ is transient then $N_b(b)$ is a finite geometric variable and so $\probof[b]{N(b)<\infty}=1$.
And so
$$ \eqalign{
    \hbox{$b$ is recurrent} &\iff \probof{N(b)=\infty}[X_0=b] = 1,\cr
    \hbox{$b$ is transient} &\iff \probof{N(b)<\infty}[X_0=b] = 1 \iff \probof{N(b)<\infty}[X_0\sim v] = 1
} $$

\bdefn

    Let $a,b\in S$ be states.
    Then $b$ is {\emphcolor reachable} from $a$ if $f_{a\to b}\neq0$ or $a=b$, this is denoted $a\to b$.
    $a$ and $b$ are {\emphcolor connected} if both $a\to b$ and $b\to a$, this is denoted $a\oto b$.

\edefn

This means that $a\to b$ if and only if there exists some $n\geq0$ such that $P^n(a\to b)>0$.
Furthermore, connectivity is an equivalence relation: it is obviously reflexive and symmetric and if $a\to b$ and $b\to c$, since $f_{a\to c}\geq f_{a\to b}\cdot f_{b\to c}>0$, we get that reachability
and therefore connectivity is transitive.
Thus $S$ can be partitioned into {\it connectivity classes}.

\blemm

    If $a\to b$ and $a\neq b$ then $\probof{T_b<T_a}[X_0=a]>0$.

\elemm

Since $a\to b$, there exists a sequence of states $a=s_0,\dots,s_m=b$ such that $P_{s_is_{i+1}}>0$ for all $i$.
We can assume that for every $i>0$, $a\neq s_i$.
So we have a sequence whose probability is positive and where the hitting time of $b$ is before that of $a$, so the probability that $T_b<T_a$ must be positive.
\qed

\bdefn

    $A\subseteq S$ is {\emphcolor closed} if for every $a\in A$ and every $b\notin A$, $b$ is not reachable from $a$.
    $A$ is also called {\emphcolor irreducible} if it is closed and connected.

\edefn

\bthrm

    If $a$ is recurrent and $a\to b$, then also $b\to a$ and $b$ is recurrent.

\ethrm

We know
$$ f_{a\to b} = \probof[a]{T_a>T_b} + \probof[a]{T_a<T_b}\cdot\probof{T_b<\infty}[T_a<T_b] $$
by the above lemma $p=\probof[a]{T_b<T_a}>0$ and so by homogeneity
$$ = p + (1-p)\cdot\probof{T_b<\infty}[X_0=a] = p + (1-p)f_{a\to b} $$
Thus we get that $p\cdot f_{a\to b}=p$ and since $p\neq0$, $f_{a\to b}=1$.
Now
$$ f_{a\to b}(1-f_{b\to a}) = \probof{\hbox{$X_n$ hits $b$ and never returns to $a$}}[X_0=a] \leq \probof[a]{N(a)<\infty} = 0 $$
Thus $f_{b\to a}=1$.
Now $f_b\geq f_{b\to a}\cdot f_{a\to b}=1$ so $b$ is also recurrent.
\qed

So if $a\oto b$, then $a$ is recurrent if and only if $b$ is.
If $b$ is reachable from $a$ but $a$ is not reachable from $b$, then $a$ is transient.
And if $a$ is recurrent and $a\to b$ then $\probof[b]{N(a)=\infty}=1$.

\bthrm

    A finite closed set of states $A\subseteq S$ contains a recurrent state.

\ethrm

Suppose $A$ has only transient states.
This means that $\probof[v]{N(a)<\infty}=1$ for every $a\in A$, and so we get that $\probof[v]{(\forall a\in A)N(a)<\infty}=1$ (as the intersection of a countable number of events with probability one).
And this means $\probof[v]{\sum_{a\in A}N(a)<\infty}=1$ since $A$ is finite.
But since $A$ is closed, we can never leave $A$ and so if $v$'s support is in $A$ then $\sum_{a\in A}N_v(a)=\infty$.
\qed

In particular, since $S$ is closed, if $S$ is finite it contains a recurrent state.

\bthrm

    If $S$ is a finite state space, then it can be uniquely partitioned into
    $$ S = T\dcup C_1\dcup\cdots\dcup C_k $$
    where $T$ is the set of all transient states, and $C_i$ are all disjoint irreducible (closed and connected) sets.

\ethrm

So $T$ is the set of all transient states, and for every recurrent state $a\in S\setminus T$ let $C_a=\set{b}[a\to b]$.
By a previous theorem, for every $b\in C_a$, $b\to a$ so and if $b\to b'$ then $a\to b'$ meaning $b'\in C_a$, so $C_a$ is closed.
And if $b,b'\in C_a$ then $a\to b$ and $a\to b'\implies b'\to a$ and therefore $b'\to b$, so $C_a$ is connected and therefore irreducible.
By taking representatives of each $C_a$, let $C_i=C_{a_i}$, we get the partition.

This partition is unique: since if $C_1\dcup\cdots\dcup C_k=C'_1\dcup\cdots\dcup C'_m$ let $a\in C_1$ then $a\in C'_i$ for some $i$, without loss of generality assume $a\in C'_1$.
Then for every $b\in C_1$, since $C_1$ is connected $a\to b$ and so $b\in C'_1$ since $C'_1$ is closed, thus $C_1=C'_1$.
Continuing inductively we get $k=m$ and $C_i=C'_i$ as required.
\qed

\bexam

    Suppose Elise is in a room $0$, and can either stay in the room with probability $1-p_1-p_2$, go to room $1$ with probability $p_1$ or go to room $2$ with probability $p_2$.
    If she goes to a new room, she stays there forever.
    Knowing that ends up in room $2$, what is the expected amount of time she spends waiting in room $0$?

    So we want to find the expected value of $N_0(0)$ knowing that $T_2<\infty$.
    So we will compute
    $$ \probof{N_0(0)=k}[T_2<\infty] = \frac{\probof{N_0(0)=k,T_2<\infty}}{\probof{T_2<\infty}} = \frac{\probof{X_1=\cdots=X_k=0,X_{k+1}=2}}{\probof{T_2<\infty}} $$
    Now, utilizing conditional probability and the Markov property (this is all done under the assumption $X_0=0$),
    $$ \probof{X_1=\cdots=X_k=0,X_{k+1}=2} = \probof{X_{k+1}=2}[X_k=0]\cdot\probof{X_k=0}[X_{k-1}=0]\cdots\probof{X_1=0} = p_2\cdot(1-p_1-p_2)^k $$
    And $\probof{T_2<\infty}=\frac{p_2}{p_1+p_2}$ since to get to room $2$ we must visit room $0$ an arbitrary number of times, and then go to room $2$, so
    $$ \probof{T_2<\infty} = \sum_{n=0}^\infty p_2\cdot(1-p_1-p_2)^n = \frac{p_2}{p_1+p_2} $$
    Thus
    $$ \probof{N_0(0)=k}[T_2<\infty] = (p_1+p_2)\cdot(1-p_1-p_2)^k $$
    Which means that
    $$ (N_0(0)\,|\,T_2<\infty) \sim \Geoof{p_1+p_2} - 1 \implies \expecof{N_0(0)}[T_2<\infty] = \frac{1-p_1-p_2}{p_1+p_2} $$

    Notice two things: firstly, by symmetry this means that $(N_0(0)\,|\,T_1<\infty)\sim\Geoof{p_1+p_2}-1$ which is the same distribution.
    And secondly, this is the same distribution as $N_0(0)$, so the expected time Elise waits at room $0$ does not change if we know which room she ends up in.

\eexam

\bdefn

    Let $a\in S\setminus T$ be a recurrent state, then we define its {\emphcolor period} to be
    $$ d(a) = \gcd\set{n\geq 1}[P^n(a\to a)>0] $$
    An irreducible Markov chain is called {\emphcolor periodic} if every state is recurrent and has the same period greater than $1$, which is the {\emphcolor period} of the Markov chain.

\edefn

Notice that if $P(a\to a)>0$ then $d(a)=1$, and so a periodic chain can be made aperiodic by adding a self-edge whose probability is nonzero.

\bprop

    If $P$ is the transition matrix of some periodic chain with a period of $d$, then $P^d$ is reducible.

\eprop

\bprop

    If the Markov chain is irreducible then every state has the same period.

\eprop

Let $P$ be the transition matrix of the chain.
Since $x\oto y$, there exist natural $r,\ell$ such that $P^r(x,y),P^\ell(y,x)>0$.
So let $m=r+\ell$ and so
$$ P^m(x,x) \geq P^r(x,y)\cdot P^\ell(y,x) > 0,\qquad P^m(y,y) \geq P^\ell(y,x)\cdot P^r(x,y) > 0 $$
So let $\tau(a)=\set{n\geq1}[P^n(a,a)>0]$, and by above we have shown that $m\in\tau(x)\cap\tau(y)$.
Now for every $n\in\tau(x)$ we have that $P^{\ell+n+r}(y,y)\geq P^\ell(y,x)P^n(x,x)P^r(x,y)>0$ and so $n+m\in\tau(y)$.
Thus $m+\tau(x)\subseteq\tau(y)$.
By definition we have $d(y)=\gcd(\tau(y))$ and since $m\in\tau(y)$ we have $d(y)\divides m$ and since $m+\tau(x)\subseteq\tau(y)$ we must have that $d(y)\divides\tau(x)$.
Thus $d(y)\divides d(x)$, and since $x,y$ are arbitrary we get $d(x)\divides d(y)$ and so $d(x)=d(y)$ as required.
\qed

This means that every irreducible Markov chain has a period, and if the period is $>1$, it is periodic.
So in order for an irreducible Markov chain to be periodic, it is sufficient for there to exist a state $a$ with $d(a)>1$.

A common Markov chain is a random walk on ${\bb Z}$, where
$$ P(i,i+1) = p,\quad P(i,i-1) = 1-p,\quad P(i,j)=0\hbox{ for $j\notin\set{i\pm 1}$} $$
Another way of representing $X_n$ is by $X_n=\sum_{k=1}^nB_k$ where $B_k=1$ with probability $p$ and $B_k=-1$ with probability $1-p$.
$\set{B_k}$ is independent.
If $p=\frac12$, the walk is called {\it fair}.

\bthrm

    If $p\neq\frac12$, every state in ${\bb Z}$ is transient.

\ethrm

Since all the states are connected, it is sufficient to show that $0$ is transient.
So we set $X_0=0$ and notice that $\frac{B_k+1}2\sim\Berof p$ and thus $\frac{X_n+n}2\sim\Binof{n,p}$ thus
$$ \probof{X_{2n}=0} = \probof{\frac{X_{2n}+2n}2=n} = {2n\choose n}p^n(1-p)^n $$
and $\probof{X_{2n+1}=0}=\probof{\frac{X_{2n+1}+2n+1}2=n+\frac12}=0$ since binomial distributions take on only integer values.
By Stirling's approximation: $k!\in\Theta(k^{k+1/2}e^{-k})$, we get that there exists some $c>0$ such that
$$ \probof{X_{2n}=0} = \frac{(2n)!}{n!n!}p^n(1-p)^n \leq cp^n(1-p)^n\frac{(2n)^{2n+1/2}e^{-2n}}{n^{2n+1}e^{-2n}} = cp^n(1-p)^n\frac{2^{2n+1/2}}{\sqrt n} = c'\frac{\bigl(4p(1-p)\bigr)^n}{\sqrt n} $$
This can be bound by a $q^n$ where $q\in[0,1)$, since $4p(1-p)<1$ for $p\neq\frac12$.
Thus we get that $\sum_{k=1}^\infty\probof{X_k=0}[X_0=0]$ and so by Borel-Cantelli we then get that $\probof{X_k=0\ \hbox{i.o.}}[X_0=0]=0$, meaning that the probability $X_k=0$ an infinite number of times
is zero.
Thus $\probof{N(0)=\infty}[X_0=0]=0$, and so this means $0$ is transient as required.
\qed

If we have a Markov chain, and $A\subseteq S$, we can ask questions about hitting times in $A$ by removing all the states in $A$ and adding a new state $\hat A$.
This can only be done if for every $a,a'\in A$ and $b\notin A$, $P(a\to b)=P(a'\to b)$, and we define that the probability $P(\hat A\to b)=P(a\to b)$.
And $P(b\to\hat A)=\sum_{a\in A}P(b\to a)$.
In particular this can be done if $A$ is closed.

\bexam

    Suppose we have the following Markov chain:
    $$ P = \pmatrix{1 & 0 & 0\cr p & q & r\cr 0 & 0 & 1} $$
    where $p,q,r\geq0$ and $p+q+r=1$.
    If we know that $X_0=2$, what is the probability that the chain will be absorbed into $1$ or $3$?

    Let us define
    $$ \ell_j = \probof{T_1<\infty}[X_0=j] $$
    since $1$ and $3$ are absorbing states, $\ell_1=1$ and $\ell_3=0$.
    Now, we want to compute $\ell_2$:
    \multlines{
        \ell_2 = \probof{T_1<\infty}[X_0=2] = \sum_{j=1}^3\probof{T_1<\infty}[X_1=j,X_0=2]\cdot\probof{X_1=j}[X_0=2]\cr\noalign{\kern-.4cm}
        &= \sum_{j=1}^3\probof{T_1<\infty}[X_1=j]\cdot P_{2j}
    }
    where the last step is due to homogeneity.
    This is equal to $\sum_{j=1}^3\ell_jP_{2j}=\ell_1p+\ell_2q+\ell_3r=p+\ell_2r$.
    Thus we get that $\ell_2=p+\ell_2r$ and so $\ell_2=\frac p{1-r}$.
    Thus the probability that starting from $X_0=2$ we are absorbed into $1$ (meaning $T_1=\infty$) is $1-\ell_2=\frac q{1-r}$.
    Since $2$ is transient, we are either absorbed into $1$ or $3$, so the probability of being absorbed into $3$ is $\frac p{1-r}$.

    Let us now ask what the expected time until being absorbed is.
    By the law of total expectation:
    Now, $\expecof{T_{\set{1,3}}}[X_1=2]=\expecof{T_{\set{1,3}}}[X_0=2]+1$ since it takes one more step, and so
    $$ = \bigl(1+\expecof{T_{\set{1,3}}}[X_0=2]\bigr)\cdot\probof[2]{X_1=2} + \probof[2]{X_1=1} + \probof[2]{X_1=3} $$
    So let $x=\expecof{T_{\set{1,3}}}[X_0=2]$, we get
    $$ x = (1+x)r + p + q = (1+x)r + (1-r) \implies x = \frac1{1-r} $$

\eexam

\bexam

    Suppose we have the following Markov chain:
    $$ P = \pmatrix{1 & 0 & 0 & 0 & 0 & 0\cr 0 & 1 & 0 & 0 & 0 & 0\cr a_1 & a_2 & 0 & a_4 & 0 & 0\cr 0 & 0 & b_3 & 0 & b_5 & b_6\cr 0 & 0 & 0 & 0 & 1 & 0\cr 0 & 0 & 0 & 0 & 0 & 1} $$
    What is the probability of being absorbed into one of the absorbing states ($1,2,5,6$) if it starts on one of the non-absorbing states ($3,4$)?

    Let us define $\ell_{m,k}=\probof[m]{T_k<\infty}$.
    Now, let us notice that
    $$ \ell_{m,k} = \probof{T_k<\infty}[X_0=m] = \sum_{j=1}^6\probof{T_k<\infty}[X_1=j]\cdot\probof{X_1=j}[X_0=m] = \sum_{j=1}^6 P_{mj}\ell_{jk} $$
    So if we define $L_{ij}=\ell_{ij}$ then we get that $L=PL$ and we can solve for $L$.

    What is the expected time until being absorbed?
    We can consolidate $A=\set{1,2,5,6}$ to a state we will call $1$, then the new transition matrix is
    $$ P' = \pmatrix{1 & 0 & 0\cr a_1+a_2 & 0 & a_4\cr b_5+b_6 & b_3 & 0} $$
    Now let us define $r_j=\expecof{T_1}[X_0=j]$, then we get
    $$ r_j = \sum_{i=1}^3\expecof{T_1}[X_1=i]P_{ji} = P_{j1} + \sum_{i=2}^3(r_i+1)P_{ji} = P_{j1} + P_{j2} + P_{j3} + r_2P_{j2} + r_3P_{j3} $$
    Which is a linear system of equations which can be solved.

\eexam

\bdefn

    Suppose $\abs S=N$, then a {\emphcolor stationary distribution} of $P$ is a row vector $\pi$ which represents a distribution (meaning $\pi_i\geq0$ and $\sum_{i=1}^N\pi_i=1$) such that $\pi=\pi P$.

\edefn

A stationary distribution is an eigenvector (or the transpose of one) of $P^\top$ whose eigenvalue is $1$.
If $\pi$ is a stationary distribution, then $\pi P=\pi\implies\pi P^n=\pi$ for every $n\geq0$.
This means that if $X_0\sim\pi$ then $X_n\sim\pi$ for every $n$ (since $\probof{X_n=k}[X_0\sim\pi]=(\pi P^n)_k=\pi_k$).

For example if $G=(V,E)$ is an undirected graph where $\abs V=N$ and the transitions from each state are all uniform (meaning $\probof{X_n=v}[X_{n-1}=u]=\frac1{\deg(u)}$ if $v\oto u$), then let
$$ \tilde\pi = (\deg(v_1),\dots,\deg(v_N)) $$
Then (using the notation $\delta\phi$ which is $1$ if $\phi$ is true and $0$ otherwise) we have that $P_{xy}=\frac1{\deg(x)}\delta(x\oto y)$, so
$$ (\tilde\pi P)_y = \sum_{x\in V}\tilde\pi_x P_{xy} = \sum_{x\in V}\deg(x)\frac1{\deg(x)}\delta(x\oto y) = \sum_{x\in V}\delta(x\oto y) = \deg(y) = \tilde\pi_y $$
So $\tilde\pi$ is a non-negative row vector, but it must be normalized to become a distribution, so we define
$$ \pi_v = \frac{\deg(v)}{\sum_{u\in V}\deg(u)} = \frac{\deg(v)}{2\abs E} $$
If the degree of each vertex is constant, suppose $\deg(v)=d$ for all $v\in V$, then $\pi_v=\frac{d}{dN}=\frac1N$ so $\pi$ is a uniform distribution.

\bthrm[title=Existence and Uniqueness Theorem for Stationary Distributions, name=eautheorem]

    Let $P$ be the transition matrix of irreducible finite-state Markov chain, then there exists a unique stationary distribution $\pi$ for $P$.

\ethrm

We know that $P{\bf1}={\bf1}$ and so $1$ is an eigenvalue for $P$, and since $P$ and $P^\top$ are similar, they share eigenvalues.
Thus $P^\top$ has an eigenvalue of $1$ and therefore must have a stationary distribution.
To show that this eigenvector is unique, we will show that the column eigenspace of $P$ has a dimension of one, and since the eigenspaces of a matrix and its transpose are equal (think Jordan normal forms),
this is sufficient.
So we will show that if $h\in{\bb R}^N$ is an eigenvector of $P$ with an eigenvalue of $1$, it is of the form $h=(c,\dots,c)^\top$.
Because $S$ is finite, there exists a state $a\in S$ such that $h_a=M$ is maximal.
Now suppose there exists a $z\in S$ such that $h_z<M$ and $P_{az}>0$ then
$$ h_a = (Ph)_a = \sum_{y\in S}P_{ay}h_y = P_{az}h_z + \sum_{y\neq z}P_{ay}h_y < M\parens{\sum_{y\in S}P_{ay}} = M = h_a $$
since $P_{az}>0$ and $h_z<M$, and this is a contradiction.
So for every state where $P_{az}>0$, $h_z=M$.
If we continue this proof (since $P^nh=h$), we get that if $a\to z$ then $h_z=M$.
Since the Markov chain is irreducible, it is closed and therefore $h_z=M$ for every $z\in S$.
\qed

We can also provide a constructive proof of the existence of a stationary distribution.
But first, a lemma:

\blemm

    For every two states $x,y\in S$ in a finite irreducible state space $\expecof[x]{T_y}<\infty$.

\elemm

Since $S$ is irreducible and finite, there exists an $\epsilon>0$ and a $r\in{\bb N}$ such that for every $a,b\in S$, there exists a $j\leq r$ such that $P^j(a,b)>\epsilon$.
This is since $S$ is connected and so between every two states there exists a path of length $\leq r$ (taking the maximum length of all paths, or just $N$) and so $P^j(a,b)>0$.
Take $\epsilon$ to be less than the minimum of all such $P^j(a,b)$, which we can do since $S$ is finite.

Thus
$$ \probof{(\exists m\in[0,\dots,r])X_m=b}[X_n=a]>\epsilon $$
Now we know that $T_b>kr$ if and only if $X_0,\dots,X_r\neq b$ and then we don't hit $b$ for another $(k-1)r$ rounds, meaning $T_b>(k-1)r$.
By homogeneity this means
\multlines{
    \probof{T_b>kr}[X_0=a] \leq \max_{a'}\probof{T_b>(k-1)r}[X_0=a']\probof{(\forall m\in[0,r])X_m\neq b}[X_0=a]\cr
    &\leq \max_{a'}\probof{T_b>(k-1)r}[X_0=a']\cdot(1-\epsilon)
}
and so by induction, this is $\leq(1-\epsilon)^k$.
Thus
$$ \expecof{T_b}[X_0=a] = \sum_{n=0}^\infty\probof{T_b>n}[X_0=a] \leq r\sum_{k=0}^\infty\probof{T_b>kr}[X_0=a] \leq r\sum_{k=0}^\infty(1-\epsilon)^k < \infty $$
The first inequality is due to the series being decreasing, and so we can take a summand and copy it $r$ times, then take the $r$th next.
\qed

Now we can construct a stationary distribution.
Let us define
$$ \tilde\pi_y = \expecof[z_0]{\centermath{the number of times $y$ is visited,\cr including at time $0$,\cr before returning to $z_0$}} = \sum_{n=0}^\infty\probof{X_n=y,T_{z_0}>n}[X_0=z_0] $$
The last equality is since this probability is equal to the number of visits being $\geq n$.
This is well-defined as
$$ \tilde\pi_y \leq \sum_{n=0}^\infty\probof{T_{z_0}>n}[X_0=z_0] = \expecof[z_0]{T_{z_0}} $$
and this is finite by the above lemma, so $\tilde\pi_y<\infty$.
Now we will compute $(\tilde\pi P)_y$:
$$ \eqalign{
    (\tilde\pi P)_y &= \sum_{x\in S}\tilde\pi_x P_{xy}\cr
    &= \sum_{x\in S}\sum_{n=0}^\infty\probof[z_0]{X_n=x,T_{z_0}>n}P_{xy}\cr
    &= \sum_{n=0}^\infty\sum_{x\in S}\probof[z_0]{X_n=x,T_{z_0}\geq n+1}\probof{X_{n+1}=y}[X_n=x]\cr
    &= \sum_{n=0}^\infty\sum_{x\in S}\probof[z_0]{X_{n+1}=y,X_n=x,T_{z_0}\geq n+1}\cr
    &= \sum_{n=0}^\infty\probof[z_0]{X_{n+1}=y,T_{z_0}\geq n+1}\cr
    &= \sum_{k=1}^\infty\probof[z_0]{X_k=y,T_{z_0}\geq k}\cr
    &= \sum_{k=0}^\infty\probof[z_0]{X_k=y,T_{z_0}\geq k} + \sum_{k=0}^\infty\probof[z_0]{X_k=y,T_{z_0}=k} - \probof[z_0]{X_0=y,T_{z_0}=0}\cr
    &= \tilde\pi_y + \sum_{k=0}^\infty\probof[z_0]{X_k=y,T_{z_0}=k} - \delta(y=z_0)\cr
} $$ 
Notice that $X_k=y,T_{z_0}=k$ if and only if $T_{z_0}=k$ and $y=z_0$, and so the sum is equal to $\delta(y=z_0)$.
So we get that $\tilde\pi P=\tilde\pi$ as required.
So we just need to normalize it by
$$ \sum_{x\in S}\tilde\pi_S = \expecof[z_0]{T_{z_0}} $$
And thus the stationary distribution is
$$ \pi_x = \frac{\tilde\pi_x}{\expecof[z_0]{T_{z_0}}} $$
\qed

\bcoro

    If $P$ is irreducible then $\pi_a=\frac1{\expecof[a]{T_a}}$.

\ecoro

Since $\pi$ is unique we can choose any $z_0$ and get the same result.
So we can choose $z_0=a$ and so
$$ \pi_a = \frac{\expecof{\centermath{The number of times we visit $a$\cr before returning to $a$\cr including $t=0$}}[X_0=a]}{\expecof[a]{T_a}} $$
The numerator here is obviously $1$, and so $\pi_a=\frac1{\expecof[a]{T_a}}$.
\qed

For example, we showed that for a connected graph where the degree of each vertex is $d$ (a connected $d$-regular graph), $\pi_v=\frac1N$ where $N=\abs V$.
Thus since $P$ is irreducible, we get that
$$ \frac1N = \pi_v = \frac1{\expecof[a]{T_a}} \implies \expecof[a]{T_a} = N $$
This is independent of the structure of the graph.
But importantly, $T_a$ is dependent on the structure of the graph!

As another example, if $P$ is symmetric then ${\bf1}^\top P=(P{\bf1})^\top={\bf1}^\top$ and so $\frac1N{\bf1}$ is a stationary distribution of $P$.
And thus $\expecof[a]{T_a}=N$ where $N=\abs S$.

\bye

