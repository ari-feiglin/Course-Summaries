\documentclass[10pt]{article}

\usepackage{amsmath, amssymb, mathtools}
\usepackage{tikz}
\usepackage[margin=1.5cm]{geometry}

\input pdfmsym
\input prettyprint
\input preamble

\pdfmsymsetscalefactor{10}
\initpps

\def\pmat#1{\begin{pmatrix} #1 \end{pmatrix}}

\let\divides=\mid
\newfunc{metric}\rho({})
\newfunc{metricc}\sigma({})

\font\bigbf = cmbx12 scaled 2000

\def\openset{{\cal O}}
\let\lineseg=\overleftrightvecc
\let\ds=\displaystyle

\def\pdv#1#2{\frac{\partial #1}{\partial #2}}

\def\differ#1#2{\left.d#1\strut\right|_{#2}}

\def\@ppmathcount{\thesection.\thepp@mathcount}

\begin{document}

\c@section=17

\barcolorbox{220, 255, 220}{0, 130, 0}{80, 200, 80}{
    \leftskip=0pt plus 1fill \rightskip=\leftskip
    {\bigbf Infinitesimal Calculus 3}

    \medskip
    \textit{Lecture \thesection, Wednsday December 28, 2022}

    \textit{Ari Feiglin}
}

\bigskip

We now generalize Taylor polynomials to more than $1$ or $2$ dimensions.
Suppose $f\colon\bR^n\longvarrightarrow\bR$ which is differentiable $m+1$ times ($f\in C^{m+1}$).
We can define an initial point $x^0=(x^0_1,\dots,x^0_n)$ and $h$ be our difference vector $h=(h_1,\dots,h_n)$ then:
\[ f(x^0+h) = \sum_{k=0}^m\frac1{k!}\biggl(\left(h_1\frac\partial{\partial x_1}+\cdots+h_n\frac\partial{\partial x_n}\right)^kf\biggr)(x^0)+
\frac1{(n+1)!}\cdot\biggr(\left(h_1\frac\partial{\partial x_1}+\cdots+h_n\frac\partial{\partial x_n}\right)^{n+1}f\biggr)(x^0+\theta h) \]
for some $0\leq\theta\leq1$ where:
\[ \left(h_1\frac\partial{\partial x_1}+\cdots+h_n\frac\partial{\partial x_n}\right)^kf =
\suum_{i_1+\cdots+i_n=k}\binom k{i_1,\dots,i_n}\cdot\frac{\partial^k f}{\partial x_1^{i_1}\dots\partial x_n^{i_n}}h_1^{i_1}\cdots h_n^{i_n} \]

If we perform an $m=0$ order Taylor expansion:
\[ f(x^0+h) = f(x^0) + \sum_{k=0}^n h_k f_{x_k}(x^0+\theta h) \implies f(x^0+h) - f(x^0) = \sum_{k=0}^n h_k f_{x_k}(x^0+\theta h) \]
And this is equal to $\nabla f(x^0+\theta h)\cdot h$, and so we get that:
\[ f(y^0) - f(x^0) = \nabla f(x^0+\theta(y^0-x_0))\cdot(y^0-x^0) \]
So if we let $c=\nabla f(x^0+\theta(y^0-x^0))$ then we get the following theorem, which is a generalization of Lagrange:

\begin{thrm*}[lagrangeTheorem,Lagrange's\ Theorem]

    If $f\colon\bR^n\longvarrightarrow\bR$ is differentiable then for every $x,y\in\bR^n$:
    \[ f(y)-f(x)=c\cdot(y-x) \]
    where $c$ is in $\lineseg{xy}$.

\end{thrm*}

\begin{coro*}

    Suppose $D\subseteq\bR^n$ is open and connected (and therefore path-connected), and $f\colon D\longvarrightarrow\bR$ is differentiable.
    If $\nabla f$ is identically $0$ in $D$, then $f$ is constant.

\end{coro*}

\begin{proof}

    Let $x,y\in D$, since $D$ is path connected, there is a path between $x$ and $y$.
    Since $D$ is open, it is polygonal connected, so we can assume that $x$ and $y$ are connected by a line (otherwise, we show that $f(x)=f(x_1)$ the next point in the polygonal chain connecting $x$ and $y$
    and so on).
    We know that $f(x)-f(y)=\nabla f(c)\cdot(x-y)$, and $c$ must lie on the line between $x$ and $y$, so $c\in D$ and therefore $f(x)-f(y)=0$ as required.

    \hfill$\blacksquare$

\end{proof}

\newpage
\begin{thrm*}

    Suppose $f\colon\bR^n\longvarrightarrow\bR$ defined in a neighborhood of $x^0=(x^0_1,\dots,x^0_n)$ which accepts a maximum or minimum there.
    If for every $1\leq k\leq n$, $f_{x_k}(x^0)$ exists, then they are all $0$.

\end{thrm*}

\begin{proof}

    We define $g_k(x)=f(x^0_1,\dots,x^0_{k-1},x,x^0_k,\dots,x^0_n)$, then $g_k$ has a maximum or minimum at $x^0_k$.
    Furthermore, we know that $g_k'(x^0_k)=f_{x_k}(x^0)$ and since $g_k$ has a local maximum at this point, $g'(x_k^0)=0$ and therefore $f_{x_k}(x^0)=0$ as well.

    \hfill$\blacksquare$

\end{proof}

Notice then that at a local maximum or minimum, $\nabla f(x)=0$.

\begin{defn*}

    A \ppemph{critical point} of $f$ is a point $x$ such that $\nabla f(x)$.

\end{defn*}

So local minima and maxima are critical points, but not all critical points are maxima or minima.
The degree $1$ Taylor expansion of $f\colon\bR^2\longvarrightarrow\bR$ at a critical point is:
\[ f(x_0+h, y_0+\ell) = f(x, y) + \frac12\bigl(f_{xx}h^2+2f_{xy}h\ell+f_{yy}\ell^2\bigr)(x^0+\theta\vec h) \]
If we let $A=f_{xx}(x^0+\theta\vec h)$, $B=f_{xy}(x^0+\theta\vec h)$, and $C=f_{yy}(x^0+\theta\vec h)$ we have that:
\[ f(x^0+\vec h) - f(x^0) = \frac12\bigl(Ah^2+2Bhk+Ck^2\bigr) \]
Notice then that if we focus on the polynomial:
\[ Ah^2 + 2Bhk + Ck^2 = k^2\parens{A\parens{\frac hk}^2+2B\parens{\frac hk}+C} \]
if we let $x=\frac hk$ then this is equal to $Ax^2+2Bx+C$ multiplied by some positive constant.
Notice then that if the discriminant is negative the polynomial doesn't change its sign, that is if $B^2-AC<0$.

\begin{thrm*}

    Suppose $f(x,y)$ is in $C^2$ and $(x_0,y_0)$ is a critical point.
    If at $(x_0,y_0)$
    \benum
        \item $f_{xx}f_{yy}-f_{xy}>0$, then the discriminant is negative, and $f_{xx}>0$ then the point is a minimum.
        \item $f_{xx}f_{yy}-f_{xy}>0$ and $f_{xx}<0$ then the point is a maximum.
        \item $f_{xx}f_{yy}-f_{xy}<0$ then the point is neither a maximum nor a minimum.
        \item $f_{xx}f_{yy}-f_{xy}=0$ then everything is possible.
    \eenum

\end{thrm*}

\begin{proof}

    \benum
        \item Since second derivatives are continuous, the discriminant is negative in a neighborhood of $(x_0,y_0)$ so for every point in the neighborhood, we have that
        \[ f(x,y) - f(x_0,y_0) = \frac12\cdot p(x) \]
        for some polynomial $p(x)$, whose sign doesn't change in this neighborhood, so $f(x_0,y_0)$ is either below or above every point in this neighborhood, and from what we know about single dimension
        second derivatives, since $f_{xx}(x_0,y_0)>0$ the point is a maximum.
        \item The proof is identical to what it is above.
        \item Since the discriminant is positive, the difference in $f$ is positive and negative in any neighborhood of $(x_0,y_0)$ and it's therefore not a maximum nor minimum.
    \eenum

\end{proof}

\begin{defn*}

    The \ppemph{Hessian} of a function $f\colon\bR^n\longvarrightarrow\bR$ in $C^2$ is a matrix $H(\vec x)\in\bR^{n\times n}$ defined by $[H]_{ij}=f_{x_ix_j}(\vec x)$.

\end{defn*}

Note that if $\vec k=(k_1,\dots,k_n)$ then:
\[ \vec k^T H(\vec x)\vec k = \bigl(f_{x_1}h_1+\cdots+f_{x_n}h_n\bigr)^2(\vec x) \]

\end{document}

