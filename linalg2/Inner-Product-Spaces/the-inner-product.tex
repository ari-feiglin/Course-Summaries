Now we move onto the next section and arguably most important section of this course.
Up until now our vector spaces have been given little interesting structure, we haven't been able to give them much in the ways of geometry.
If you recall from high school, a very useful concept within $\bR^2$ and $\bR^3$ is the concept of vectors having \emph{magnitude}, and being \emph{perpendicular}.
Recall that one tool we used to define (or compute) both of these concepts is the \emph{dot product}.
In this section we will be generalizing this to general vector spaces.

Unfortunately, in order to discuss this generalization we must restrict our discussion only to vector spaces over the real or complex field.
\textbf{So for the purpose of this section, all vector spaces are implicitly real or complex}.

Recall that if $\vec v=(a_1,a_2,a_3)$ and $\vec u=(b_1,b_2,b_3)$ are real vectors, then we defined their inner product to be
\[ \vec v\cdot\vec u = a_1b_1 + a_2b_2 + a_3b_3 \]
This has the following properties (which you can verify yourself, or wait until we do):
\benum
    \item $(\alpha\vec v+\beta\vec u)\cdot\vec w=\alpha(\vec v\cdot\vec w)+\beta(\vec v\cdot\vec w)$
    \item $\vec v\cdot\vec u=\vec u\cdot\vec v$
    \item $\vec v\cdot\vec v$ is the square of the magnitude of $\vec v$, and is therefore non-negative and zero only when $\vec v=0$.
\eenum

It's not hard to see that from these properties we see that
\[ \vec v\cdot(\alpha\vec u+\beta\vec w)=\alpha(\vec v\cdot\vec u) + \beta(\vec v\cdot\vec w) \]
and
\[ 0\cdot\vec v=0 \]

But notice that such a function cannot exist in complex vector spaces, as we'd get that for every vector $\vec v$,
\[ (i\vec v)\cdot(i\vec v) = i^2(\vec v\cdot\vec v) = -\vec v\cdot\vec v \]
By the third property, $(i\vec v)\cdot(i\vec v)\geq0$ and so $\vec v\cdot\vec v\leq0$ which would mean that $\vec v\cdot\vec v=0$, meaning $\vec v=0$.
But not every vector is the zero vector.

So we need to come up with a different list of properties that our generalization should have if we are to generalize the dot product to complex vector spaces.
But at the same time, the above properties should hold for real vector spaces.

\begin{note}

    Since we are attempting to generalize dot products to general vector spaces, we cannot assume that we'll be able to define the generalization using an explicit formula like the dot product's.
    This is the importance of coming up with a list of properties that we want our generalization to have and then defining our generalization to be any object which satisfies these properties.
    This is similar to how we generalized our notions of $\bR^2$ and $\bR^3$ to general vector spaces.

\end{note}

\begin{defn*}

    A \ppemph{inner product space} is a vector space $V$ over the field $\bF$ (which is either $\bR$ or $\bC$) equipped with an \ppemph{inner product function} (for short, just an inner product), which is a
    function
    \[ \iprod{\,\cdot\,,\,\cdot\,}\colon V\times V\longto\bF \]
    which satisfies the following axioms: for every $\alpha$ and $\beta$ in $\bF$, and vectors $v,u,w\in V$:
    \benum
        \item $\iprod{\alpha v+\beta u,w} = \alpha\iprod{v,w} + \beta\iprod{u,w}$ (this means that inner products are linear in their first argument.)
        \item $\iprod{v,u}=\overline{\iprod{u,v}}$ (recall that $\overline z$ is the \emph{complex conjugate} of the complex number $z$.
        This axiom is called \emph{conjugate symmetry}.)
        \item If $v\neq0$ then $\iprod{v,v}>0$ (this implies that even when $\bF=\bC$, $\iprod{v,v}$ is real.
        This axiom is called \emph{positive-definiteness}).
    \eenum

\end{defn*}

The inner product is precisely our generalization of the dot product.
We will show soon that the dot product is a specific case of the inner product, and we will also show how all inner products (over finite spaces) relate to the dot product.

Notice that if we have an inner product of the form $\iprod{v,\alpha u+\beta w}$, in order to apply linearity in the first argument we apply conjugate symmetry to move the sum to the first argument:
\[ \iprod{v,\alpha u+\beta w} = \overline{\iprod{\alpha u+\beta w,v}} = \overline{\alpha\iprod{u,v} + \beta\iprod{w,v}} =
\overline\alpha\cdot\overline{\iprod{u,v}} + \overline\beta\cdot\overline{\iprod{w,v}} = \overline\alpha\iprod{v,u} + \overline\beta\iprod{v,w} \]
this property is called \emph{antilinearity} in the second argument.
Together in tandem with linearity in the first component, we say that inner products are \emph{sesquilinear}.

Since inner products are linear in their first argument, and $T0=0$ for all linear transforms $T$, we have
\[ \iprod{0,v} = 0 \]
for every vector $v$.
We can show this directly:
\[ \iprod{0,v} = \iprod{0+0,v} = \iprod{0,v} + \iprod{0,v} \]
And subtracting $\iprod{0,v}$ from both sides gives us
\[ \iprod{0,v} = 0 \]
as required.
Thus
\[ \iprod{v,0} = \overline{\iprod{0,v}} = \overline0 = 0 \]
And so we see that $\iprod{0,0}=0$, and so $\iprod{v,v}=0$ if and only if $v=0$ (by positive-definiteness).

Let us summarize these results in the following proposition:

\begin{prop*}

    Inner products must satisfy these additional properties:
    \benum
        \item $\iprod{v,\alpha u+\beta w}=\overline\alpha\iprod{v,u}+\overline\beta\iprod{v,w}$
        \item $\iprod{v,0}=\iprod{0,v}=0$ for all vectors $v$
        \item $\iprod{v,v}=0$ if and only if $v=0$
    \eenum

\end{prop*}

\begin{exam*}

    The generalized dot product over $\bF^n$ defined by $\iprod{v,u}=v^\top\overline u$ is an inner product.
    Explicitly,
    \[ \iprod{v,u} = \sum_{i=1}^n v_i\overline u_i \]
    We now verify the three axioms of inner products:
    \benum
        \item Linearity in the first argument:
            \[ \iprod{\alpha v+\beta w,u} = \sum_{i=1}^n (\alpha v_i+\beta w_i)\overline u_i = \alpha\sum_{i=1}^nv_i\overline u_i + \beta\sum_{i=1}^nw_i\overline u_i = \alpha\iprod{v,u} + \beta\iprod{w,u} \]
        \item Conjugate symmetry:
            \[ \overline{\iprod{u,v}} = \overline{\sum_{i=1}^n u_i\overline v_i} = \sum_{i=1}^n\overline u_iv_i = \iprod{v,u} \]
        \item Positive-definiteness: $\iprod{v,v}=\sum_{i=1}^nv_i\overline v_i=\sum_{i=1}^n\abs{v_i}$.
            If $v\neq0$ then suppose $v_i\neq0$, then $\iprod{v,v}\geq\abs{v_i}>0$ as required.
    \eenum

\end{exam*}

\begin{defn*}

    Two vectors $v,u$ in an inner product space are \ppemph{orthogonal} if their inner product is zero: $\iprod{v,u}=0$.
    This is denoted $v\perp u$.

\end{defn*}

Orthogonality generalizes our concept of perpendicular vectors on the plane, since two vectors are perpendicular if and only if their dot product is zero.

Orthogonality is a symmetric relation, since $\iprod{u,v}=\overline{\iprod{v,u}}$ and so the inner product of one is zero if and only if the other is.
Notice that immediately, every vector is orthogonal to the zero vector.
Furthermore if $v\perp u$ then $\alpha v\perp\beta u$ since $\iprod{\alpha v,\beta u}=\alpha\overline\beta\iprod{v,u}=0$.

\begin{defn*}

    A \ppemph{normed vector space} is a vector space $V$ equipped with a \ppemph{norm} function
    \[ \norm{\cdot}\colon V\longto\bR \]
    which satisfies the following axioms:
    \benum
        \item Positive-definiteness: $\norm v>0$ for all vectors $v\neq0$,
        \item Homogeneity: $\norm{\alpha v}=\abs\alpha\norm v$,
        \item The triangle inequality: $\norm{v+u}\leq\norm v+\norm u$.
    \eenum

\end{defn*}

Notice that $\norm 0=\norm{0\cdot0}=\abs0\norm0=0$, so $\norm v=0$ if and only if $v=0$.
In this course when discussing a norm, we will almost exclusively mean one generated from an inner product:
\[ \norm v\coloneqq\sqrt{\iprod{v,v}} \]
which already immediately satisfies positive-definiteness and homogeneity is obvious:
\[ \norm{\alpha v}=\sqrt{\iprod{\alpha v,\alpha v}}=\sqrt{\alpha\overline\alpha\iprod{v,v}}=\abs\alpha\sqrt{\iprod{v,v}} = \abs\alpha\norm v \]
But it will take some more work to prove the triangle inequality.

The norm generalizes the concept of the magnitude of a vector, as its axioms are exactly those you'd expect from such a ``magnitude function''.
Norms generated by inner products extend the result that $\abs v^2=v\cdot v$ for the dot product in $\bR$.

Notice that if $v$ and $u$ are orthogonal then $\iprod{v,u}=\iprod{u,v}=0$ so
\[ \norm{v+u}^2 = \iprod{v+u,v+u} = \iprod{v,v} + \iprod{u,v} + \iprod{v,u} + \iprod{u,u} = \iprod{v,v} + \iprod{u,u} = \norm v^2+\norm u^2 \]
which is a generalization of Pythagorean's theorem: since if $v$ and $u$ are orthogonal, the vectors $v,u,v+u$ form a right triangle with hypotenuse $v+u$.

\begin{thrm*}[cauchyschwarz,The\ Cauchy-Schwarz\ Inequality]

    Let $v,u\in V$ be vectors in an inner product space, then $\abs{\iprod{v,u}}\leq\norm v\cdot\norm u$ and there is equality if and only if $v$ and $u$ are linearly dependent.

\end{thrm*}

\begin{proof}

    First we prove the inequality, then we show when there is equality.
    If $u=0$ this is trivial, as $\iprod{v,u}=0$ and $\norm v\norm u=0$.
    Otherwise, let us define
    \[ z\coloneqq v - \frac{\iprod{v,u}}{\iprod{u,u}}u \]
    Let us give some intuition to this definition: $\iprod{v,u}$ is geometrically the length of the projection of $v$ onto $u$ multiplied by $\norm u$.
    And so $\frac{\iprod{v,u}}{\norm u}$ gives the length of this projection, which is parallel to $u$, thus the projection is equal to $\frac{\iprod{v,u}}{\iprod{u,u}}u$ since $\frac u{\norm u}$ is the
    unit vector in the direction of $u$ and $\norm u^2=\iprod{u,u}$.
    And so $z$ is then perpendicular to $u$, as we show:
    \[ \iprod{z,u} = \iprod{v,u} - \frac{\iprod{v,u}}{\iprod{u,u}}\iprod{u,u} = \iprod{v,u} - \iprod{v,u} = 0 \]
    Thus $z$ and $u$ are orthogonal, and so by the generalized Pythagorean theorem:
    \[ \norm v^2 = \norm{z+\frac{\iprod{v,u}}{\iprod{u,u}}u}^2 = \norm z^2 + \norm{\frac{\iprod{v,u}}{\iprod{u,u}}u}^2 = \norm z^2 + \frac{\abs{\iprod{v,u}}^2}{\iprod{u,u}^2}\norm u^2 
    = \norm z^2 + \frac{\abs{\iprod{v,u}}^2}{\norm u^2} \]
    Thus we get $\bigl(\norm v\norm u\bigr)^2=\norm z^2\norm u^2+\abs{\iprod{v,u}}^2$, which means that $\bigl(\norm v\norm u\bigr)^2\geq\abs{\iprod{v,u}}^2$ and since these are all nonnegative values, we
    get the desired inequality by taking the root of both sides.

    Notice that there is equality only when $\norm z=0$, meaning $v=\frac{\iprod{v,u}}{\iprod{u,u}}u$, so if there is equality then there is linear dependence.
    And if $v=\alpha u$ then $\abs{\iprod{v,u}}=\abs\alpha\abs{\iprod{u,u}}=\abs\alpha\norm u\norm u=\norm v\norm u$, so there is equality.
    \qed

\end{proof}

\begin{thrm*}

    The norm generated from an inner product is indeed a norm.

\end{thrm*}

\begin{proof}

    As discussed previously, all that requires verification is that the function satisfies the triangle inequality.
    \[ \norm{v+u} = \sqrt{\iprod{v,v} + \iprod{v,u} + \iprod{u,v} + \iprod{u,u}} = \sqrt{\norm v^2 + \iprod{v,u} + \iprod{u,v} + \norm u^2} \]
    since $\iprod{u,v}=\overline{\iprod{v,u}}$, $\iprod{v,u}+\iprod{u,v}=2\Re\iprod{v,u}\leq2\abs{\iprod{v,u}}$ which by \ppref{cauchyschwarz} is bound by $2\norm v\norm u$, thus
    \[ \norm{v+u} \leq \sqrt{\norm v^2 + 2\norm v\norm u + \norm u^2} = \sqrt{\bigl(\norm v+\norm u\bigr)^2} = \norm v + \norm u \]
    as required.
    \qed

\end{proof}

If $B=(v_1,\dots,v_n)$ is a basis, then let $v,u$ be two vectors such that $v=\sum_{i=1}^n\alpha_iv_i$ and $u=\sum_{i=1}^n\beta_iu_i$ then
\[ \iprod{v,u} = \iprod{\sum_{i=1}^n\alpha_iv_i,\;\sum_{j=1}^n\beta_jv_j} = \sum_{i=1}^n\sum_{j=1}^n\alpha_i\overline{\beta_j}\iprod{v_i,v_j} \]
This leads us to the following definition

\begin{defn*}

    Let $B=(v_1,\dots,v_n)$ be a set of vectors (not necessarily a basis), then define the \ppemph{Gram matrix} to be the matrix $G_B\in\bF^{n\times n}$ defined by $[G_B]_{ij}=\iprod{v_i,v_j}$.
    Meaning
    \[ G_B = \pmat{
        \iprod{v_1,v_1} & \iprod{v_1,v_2} & \cdots & \iprod{v_1,v_n}\cr
        \iprod{v_2,v_1} & \iprod{v_2,v_2} & \cdots & \iprod{v_2,v_n}\cr
        \vdots & \vdots & \ddots & \vdots\cr
        \iprod{v_n,v_1} & \iprod{v_n,v_2} & \cdots & \iprod{v_n,v_n}
    } \]

\end{defn*}

Then we get that if $B$ is a basis,
\[ \iprod{v,u} = [v]^\top_BG_B\overline{[w]_B} \]
(In general $v^\top Au=\sum_{i=1}^n\sum_{j=1}^nv_iA_{ij}u_j$, which is easy enough to verify.)

\begin{thrm*}

    $B$ is a linearly independent if and only if $G_B$ is invertible.

\end{thrm*}

\begin{proof}

    Suppose $B$ is a basis, then we will show that $G_B$'s columns are linearly independent.
    Let $\alpha_1,\dots,\alpha_n$ be scalars such that $\sum_{i=1}^n\alpha_iC_i(G_B)=0$, thus we must have that for every $1\leq j\leq n$ the $j$ coefficient of this linear combination is zero so,
    \[ \sum_{i=1}^n\alpha_i[G_B]_{ji} = \sum_{i=1}^n\alpha_i\iprod{v_j,v_i} = \iprod{v_j,\;\sum_{i=1}^n\overline\alpha_iv_i} = 0 \]
    Let us define $u=\sum_{i=1}^n\overline\alpha_iv_i$, then for every $1\leq j\leq n$, $\iprod{v_j,u}=0$.
    This means that
    \[ \iprod{u,u} = \iprod{\sum_{j=1}^n\overline\alpha_jv_j,u} = \sum_{j=1}^n\overline\alpha_j\iprod{v_j,u} = 0 \]
    Thus $u=0$, and since $v_i$ are linearly independent this means $\overline\alpha_i=0$ and so $\alpha_i=0$ for every $i$.
    So $G_B$ is invertible.

    Now suppose $G_B$ is invertible, then let $\alpha_1,\dots,\alpha_n$ be scalars such that $\sum_{i=1}^n\alpha_iv_i=0$.
    Then for $1\leq j\leq n$,
    \[ \bracks{\sum_{i=1}^n\overline\alpha_iC_i(G_B)}_j = \sum_{i=1}^n\overline\alpha_i[G_B]_{ji} = \sum_{i=1}^n\overline\alpha_i\iprod{v_j,v_i} = \iprod{v_j,\;\sum_{i=1}^n\alpha_iv_i} = \iprod{v_j,0} = 0 \]
    So $\sum_{i=1}^n\overline\alpha_iC_i(G_B)=0$, but since $G_B$ is invertible, this means that $\overline\alpha_i=0$ and so $\alpha_i=0$ for every $i$.
    So $B$ is linearly independent.
    \qed

\end{proof}

\begin{defn*}

    Call a set $S\subseteq V$ \ppemph{orthogonal} if every two vectors in $S$ are orthogonal.
    $S$ is \ppemph{orthonormal} if it is orthogonal and the norm of every vector in it is $1$.

\end{defn*}

\begin{coro*}

    Every orthogonal set not containing $0$ is linearly independent.

\end{coro*}

\begin{proof}

    Let $S$ be an orthogonal set not containing $0$.
    Then $G_S$ is a diagonal matrix without zeroes on its diagonal and is therefore invertible.
    By the above theorem this means that $S$ is linearly independent.
    \qed

\end{proof}

\begin{lemm*}

    Let $E=(e_1,\dots,e_n)$ be an orthonormal basis and $v\in V$ with $[v]_E=(\alpha_1,\dots,\alpha_n)$.
    Then $\alpha_i=\iprod{v,e_i}$ and $\norm v^2=\sum_{i=1}^n\abs{\alpha_i}^2$.

\end{lemm*}

\begin{proof}

    Notice that
    \[ \iprod{v,e_i} = \iprod{\sum_{j=1}^n\alpha_je_j,e_i} = \sum_{j=1}^n\alpha_j\iprod{e_j,e_i} = \alpha_i \]
    since $\iprod{e_j,e_i}=1$ when $i=j$ and zero otherwise.
    And
    \[ \norm v^2 = \iprod{v,v} = \iprod{\sum_{i=1}^n\alpha_ie_i,\sum_{j=1}^n\alpha_je_j} = \sum_{i,j=1}^n\alpha_i\overline\alpha_j\iprod{e_i,e_j} = \sum_{i=1}^n\alpha_i\overline\alpha_i =
    \sum_{i=1}^n\abs\alpha_i^2 \qed \]

\end{proof}

Let $v$ be a non-zero vector, then we can \textit{normalize} it to give the vector $\frac v{\norm v}$.
This vector has a norm of $1$: $\norm{\frac v{\norm v}}=\frac1{\norm v}\norm v=1$.

\begin{thrm*}[gramschmidt,The\ Gram-Schmidt\ Theorem]

    Every vector space has a orthonormal basis.

\end{thrm*}

\begin{proof}

    Let $V$ be a vector space with a basis $B=(v_1,\dots,v_n)$, we will convert this to an orthogonal basis (the vectors can then be normalized to give an orthonormal basis).
    Define $E=(w_1,\dots,w_n)$ recursively as follows:
    \[ w_1 = v_1,\qquad w_k = v_k - \sum_{i=1}^{k-1}\alpha_{ik}w_i \]
    Our goal now is to find the appropriate $\alpha_{ik}$s to make this set orthogonal.
    Let us suppose that $\set{w_1,\dots,w_{k-1}}$ is orthogonal, then we want to find $\alpha_{ik}$ such that $\set{w_1,\dots,w_k}$ is orthogonal.
    Let $k>\ell$, then
    \[ \iprod{w_k,w_\ell} = \iprod{v_k - \sum_{i=1}^{k-1}\alpha_{ik}w_i,\,w_\ell} = \iprod{v_k,w_\ell} - \sum_{i=1}^{k-1}\alpha_{ik}\iprod{w_i,w_\ell} \]
    By our assumption, $\iprod{w_i,w_\ell}=0$ for $i\neq\ell$ so
    \[ = \iprod{v_k,w_\ell} - \alpha_{\ell k}\iprod{w_\ell,w_\ell} \]
    We want this to be zero, so we must have that $\alpha_{\ell k}=\frac{\iprod{v_k,w_\ell}}{\iprod{w_\ell,w_\ell}}$.
    And so we have that
    \[ w_k = v_k - \sum_{i=1}^{k-1}\frac{\iprod{v_k,w_i}}{\iprod{w_i,w_i}}w_i \]
    This is only well-defined if $w_k\neq0$ for every $k$.

    But notice that $w_i\in\lspanof{v_1,\dots,v_i}$ and since $B$ is linearly independent this means that $w_k$ cannot be zero.
    Explicitly, $w_k=v_k+\alpha_1v_1+\cdots+\alpha_{k-1}v_{k-1}$ for some $\alpha_i$s and since $\set{v_1,\dots,v_k}$ is linearly independent and the coefficient of $v_k$ is nonzero, the sum is not zero.
    So $E$ is a set of orthogonal vectors which doesn't contain zero.
    Thus $E$ is a set of $n$ linearly independent vectors, thus a basis.
    \qed

\end{proof}

\begin{coro*}

    Every orthogonal set $B_0$ not containing zero can be extended to an orthogonal basis.

\end{coro*}

\begin{proof}

    Extend $B_0=(w_1,\dots,w_k)$ to a basis $B=(w_1,\dots,w_k,v_{k+1},\dots,v_n)$ and perform the Gram-Schmidt process on $(v_{k+1},\dots,v_n)$ using $w_1,\dots,w_k$ as the initial vectors.
    This extends $B_0$ to an orthogonal basis.
    \qed

\end{proof}

\begin{defn*}

    Let $S\subseteq V$ be a set of vectors, then define its \ppemph{orthogonal complement} to be the set of vectors orthogonal to all vectors in $S$:
    \[ S^\perp = \set{v\in V}[\forall w\in S\colon\iprod{v,w}=0] \]

\end{defn*}

Notice that $S^\perp$ is always a subspace: $0\in S^\perp$ trivially and if $v,u\in S^\perp$ then for $w\in S$: $\iprod{\alpha v+\beta u,w}=\alpha\iprod{v,w}+\beta\iprod{u,w}=0$.
Also trivially if $S_1\subseteq S_2$ then $S_2^\perp\subseteq S_1^\perp$.
Further notice that $S^\perp=\lspan(S)^\perp$, since if a vector is orthogonal to $S$ then it is orthogonal to any linear combination in $S$, since the inner product is linear.

\begin{thrm*}

    Let $W$ be a subspace of $V$, then $V=W\oplus W^\perp$.

\end{thrm*}

\begin{proof}

    Firstly it is obvious that $W\cap W^\perp=\set0$ since if $w\in W\cap W^\perp$ then it is orthogonal to itself and thus zero.
    Suppose $B_0=(w_1,\dots,w_k)$ is an orthogonal basis for $W$ which is extended to an orthogonal basis $B=(w_1,\dots,w_n)$ of $V$.
    Then we claim that $B_1=(w_{k+1},\dots,w_n)$ is a basis for $W^\perp$.
    Obviously all these vectors in $W^\perp$ since they are orthogonal to $B_0$ and thus its span, $W$.
    This means that $\dim W^\perp \geq n-k$, and so $\dim(W\oplus W^\perp)=\dim W+\dim W^\perp\geq n=\dim V$ so $V=W\oplus W^\perp$.
    \qed

\end{proof}

Notice the term
\[ \sum_{i=1}^{k-1}\frac{\iprod{v,w_i}}{\iprod{w_i,w_i}}w_i \]
in the proof of Gram-Schmidt.
Geometrically, $\frac{\iprod{v,w_i}}{\iprod{w_i,w_i}}w_i$ is the result of projecting $v$ onto $w_i$.
This is since $\iprod{v,w_i}$ is equal to the length of the projection of $v$ onto $w_i$ times the length of $w_i$.
Divide this by $\norm{w_i}$ to get the length of the projection, and multiplied by $w_i$ normalized ($\frac1{\norm{w_i}}w_i$) to get the projection itself.
This leads us to the following definition:

\begin{defn*}

    Let $V$ be a vector space and $W\leq V$ a subspace with an orthogonal basis $E=(w_1,\dots,w_n)$.
    Define the \ppemph{projection map} from $V$ to $W$ to be the function $\pi_W\colon V\longto W$ defined by
    \[ \pi_W(v) = \sum_{i=1}^n\frac{\iprod{v,w_i}}{\iprod{w_i,w_i}}w_i \]

\end{defn*}

Notice that despite being called \textit{the} projection map, it seems to be dependent on the choice of orthogonal basis $E$ for $W$.
We will show that this is indeed not the case: it is independent of the choice of $E$.

\begin{lemm*}

    Let $W$ be a subspace of $V$ and $\pi_W\colon V\longto W$ its projection map (with respect to $E$).
    Then
    \benum
        \item $\pi_W(v)\in W$,
        \item $\pi_W$ is a linear transformation,
        \item $\pi_W(v)=v$ if and only if $v\in W$,
        \item for all $w\in W$, $\iprod{v,w}=\iprod{v,\pi_W(v)}$,
        \item for all $w\in W$, $v-\pi_W(v)$ is orthogonal to $w$,
        \item $\pi_W(v)=0$ if and only if $v$ is orthogonal to all vectors in $W$.
    \eenum

\end{lemm*}

\begin{proof}

    \benum
        \item This is obvious as $\pi_W$ is in the span of $E$ by definition.
        \item This is obvious and left as an exercise to the reader.
        \item Since $\pi_W(v)\in W$ by $(1)$, if $v=\pi_W(v)$ then $v\in W$.
            And for the converse, suppose $v=\sum_{i=1}^n\alpha_iw_i$ then since $\pi_W$ is a linear transformation $\pi_W(v)=\sum_{i=1}^n\alpha_i\pi_W(w_i)$ and it is trivial to see that $\pi_W(w_i)=w_i$.
        \item If $w=\sum_{i=1}^n\alpha_iw_i$ then
            \[ \iprod{\pi_W(v),w} = \iprod{\sum_{i=1}^n\frac{\iprod{v,w_i}}{\iprod{w_i,w_i}}w_i,\,\sum_{j=1}^n\alpha_jw_j} =
            \sum_{i,j=1}^n\frac{\iprod{v,w_i}}{\iprod{w_i,w_i}}\overline\alpha_j\iprod{w_i,w_j} = \sum_{i=1}^n\overline\alpha_i\iprod{v,w_i} = \iprod{v,w} \]
        \item This is since $\iprod{v-\pi_W(v),w}=\iprod{v,w}-\iprod{\pi_W(v),w}=0$ by the above point.
        \item If $v$ is orthogonal to all vectors in $W$ then it is orthogonal to $w_i$ and thus $\iprod{v,w_i}=0$ so $\pi_W(v)=0$.
            Conversely, $\iprod{v,w}=\iprod{\pi_W(v),w}=0$ so $v$ is orthogonal to all $w\in W$.
            \qed
    \eenum

\end{proof}

\begin{thrm*}

    $\pi_W$ is independent on the choice of orthogonal basis for $W$.

\end{thrm*}

\begin{proof}

    Let $E$ and $E'$ be two orthogonal basis for $W$, let us denote $\pi_E$ to be the projection function with respect to $E$ and $\pi_{E'}$ with respect to $E'$.
    Then by the prior lemma, for every $v\in V$, $v-\pi_E(v)$ and $v-\pi_{E'}(v)$ are in $W^\perp$.
    Thus
    \[ v = \pi_E(v) + (v - \pi_E(v)) = \pi_{E'}(v) + (v - \pi_{E'}(v)) \]
    But these are both representations of $v$ as the sum of vectors in $W$ and $W^\perp$, and since $W\oplus W^\perp$ is a direct sum, this means that such representations are unique, so
    $\pi_E(v)=\pi_{E'}(v)$ as required.
    \qed

\end{proof}

\begin{defn*}

    A \ppemph{metric space} is a set $M$ equipped with a \ppemph{metric function} $\rho\colon M\times M\longto\bR$ such that for every $u,v\in M$:
    \benum
        \item $d$ is nonnegative: $\rho(u,v)\geq0$ and $\rho(u,v)=0$ if and only if $u=v$,
        \item $d$ is symmetric: $d(u,v)=d(v,u)$
        \item $d$ has the triangle inequality: for every $w\in M$, $d(u,v)\leq d(u,w)+d(w,v)$.
    \eenum

\end{defn*}

Notice that if $(V,\norm\cdot)$ is a normed vector space then $d(u,v)=\norm{u-v}$ is a metric function (this is trivially proven).

\begin{prop*}

    Let $W$ be a subspace and $v\in V$ a vector.
    Then the closest vector to $v$ in $W$ (the vector with the smallest metric) is $\pi_W(v)$, and no other vector is as close.

\end{prop*}

\begin{proof}

    Let $w\in W$, we need to show that $\norm{v-\pi_W(v)}\leq\norm{v-w}$.
    We know that $v-\pi_W(v)$ is in $W$'s orthogonal complement, so let $z=v-\pi_W(v)$ and so $\norm z=d\parens{\pi_W(v),v}$.
    And define $z'=v-w$ so $\norm{z'}=d(v,w)$.
    Now define $u=z'-z=\pi_W(v)-w$, which is in $W$.
    Since $z\in W^\perp$ and $z'=u+z$ we have that $\norm{z'}^2=\norm u^2+\norm z^2$ since $\iprod{u,z}=0$, thus $\norm{z'}^2\geq\norm z^2$.
    Thus $\norm z=d\parens{\pi_W(v),v}\leq d(w,v)=\norm{z'}$, and there is equality only when $u=0$, meaning $z'-z$ and $w=\pi_W(v)$.
    \qed

\end{proof}

