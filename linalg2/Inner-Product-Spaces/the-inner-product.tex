Now we move onto the next section and arguably most important section of this course.
Up until now our vector spaces have been given little interesting structure, we haven't been able to give them much in the ways of geometry.
If you recall from high school, a very useful concept within $\bR^2$ and $\bR^3$ is the concept of vectors having \emph{magnitude}, and being \emph{perpendicular}.
Recall that one tool we used to define (or compute) both of these concepts is the \emph{dot product}.
In this section we will be generalizing this to general vector spaces.

Unfortunately, in order to discuss this generalization we must restrict our discussion only to vector spaces over the real or complex field.
\textbf{So for the purpose of this section, all vector spaces are implicitly real or complex}.

Recall that if $\vec v=(a_1,a_2,a_3)$ and $\vec u=(b_1,b_2,b_3)$ are real vectors, then we defined their inner product to be
\[ \vec v\cdot\vec u = a_1b_1 + a_2b_2 + a_3b_3 \]
This has the following properties (which you can verify yourself, or wait until we do):
\benum
    \item $(\alpha\vec v+\beta\vec u)\cdot\vec w=\alpha(\vec v\cdot\vec w)+\beta(\vec v\cdot\vec w)$
    \item $\vec v\cdot\vec u=\vec u\cdot\vec v$
    \item $\vec v\cdot\vec v$ is the square of the magnitude of $\vec v$, and is therefore non-negative and zero only when $\vec v=0$.
\eenum

It's not hard to see that from these properties we see that
\[ \vec v\cdot(\alpha\vec u+\beta\vec w)=\alpha(\vec v\cdot\vec u) + \beta(\vec v\cdot\vec w) \]
and
\[ 0\cdot\vec v=0 \]

But notice that such a function cannot exist in complex vector spaces, as we'd get that for every vector $\vec v$,
\[ (i\vec v)\cdot(i\vec v) = i^2(\vec v\cdot\vec v) = -\vec v\cdot\vec v \]
By the third property, $(i\vec v)\cdot(i\vec v)\geq0$ and so $\vec v\cdot\vec v\leq0$ which would mean that $\vec v\cdot\vec v=0$, meaning $\vec v=0$.
But not every vector is the zero vector.

So we need to come up with a different list of properties that our generalization should have if we are to generalize the dot product to complex vector spaces.
But at the same time, the above properties should hold for real vector spaces.

\begin{note}

    Since we are attempting to generalize dot products to general vector spaces, we cannot assume that we'll be able to define the generalization using an explicit formula like the dot product's.
    This is the importance of coming up with a list of properties that we want our generalization to have and then defining our generalization to be any object which satisfies these properties.
    This is similar to how we generalized our notions of $\bR^2$ and $\bR^3$ to general vector spaces.

\end{note}

\begin{defn*}

    A \ppemph{inner product space} is a vector space $V$ over the field $\bF$ (which is either $\bR$ or $\bC$) equipped with an \ppemph{inner product function} (for short, just an inner product), which is a
    function
    \[ \iprod{\,\cdot\,,\,\cdot\,}\colon V\times V\longto\bF \]
    which satisfies the following axioms: for every $\alpha$ and $\beta$ in $\bF$, and vectors $v,u,w\in V$:
    \benum
        \item $\iprod{\alpha v+\beta u,w} = \alpha\iprod{v,w} + \beta\iprod{u,w}$ (this means that inner products are linear in their first argument.)
        \item $\iprod{v,u}=\overline{\iprod{u,v}}$ (recall that $\overline z$ is the \emph{complex conjugate} of the complex number $z$.
        This axiom is called \emph{conjugate symmetry}.)
        \item If $v\neq0$ then $\iprod{v,v}>0$ (this implies that even when $\bF=\bC$, $\iprod{v,v}$ is real.
        This axiom is called \emph{positive-definiteness}).
    \eenum

\end{defn*}

The inner product is precisely our generalization of the dot product.
We will show soon that the dot product is a specific case of the inner product, and we will also show how all inner products (over finite spaces) relate to the dot product.

Notice that if we have an inner product of the form $\iprod{v,\alpha u+\beta w}$, in order to apply linearity in the first argument we apply conjugate symmetry to move the sum to the first argument:
\[ \iprod{v,\alpha u+\beta w} = \overline{\iprod{\alpha u+\beta w,v}} = \overline{\alpha\iprod{u,v} + \beta\iprod{w,v}} =
\overline\alpha\cdot\overline{\iprod{u,v}} + \overline\beta\cdot\overline{\iprod{w,v}} = \overline\alpha\iprod{v,u} + \overline\beta\iprod{v,w} \]
this property is called \emph{antilinearity} in the second argument.
Together in tandem with linearity in the first component, we say that inner products are \emph{sesquilinear}.

Since inner products are linear in their first argument, and $T0=0$ for all linear transforms $T$, we have
\[ \iprod{0,v} = 0 \]
for every vector $v$.
We can show this directly:
\[ \iprod{0,v} = \iprod{0+0,v} = \iprod{0,v} + \iprod{0,v} \]
And subtracting $\iprod{0,v}$ from both sides gives us
\[ \iprod{0,v} = 0 \]
as required.
Thus
\[ \iprod{v,0} = \overline{\iprod{0,v}} = \overline0 = 0 \]
And so we see that $\iprod{0,0}=0$, and so $\iprod{v,v}=0$ if and only if $v=0$ (by positive-definiteness).

Let us summarize these results in the following proposition:

\begin{prop*}

    Inner products must satisfy these additional properties:
    \benum
        \item $\iprod{v,\alpha u+\beta w}=\overline\alpha\iprod{v,u}+\overline\beta\iprod{v,w}$
        \item $\iprod{v,0}=\iprod{0,v}=0$ for all vectors $v$
        \item $\iprod{v,v}=0$ if and only if $v=0$
    \eenum

\end{prop*}

\begin{exam*}

    The generalized dot product over $\bF^n$ defined by $\iprod{v,u}=v^\top\overline u$ is an inner product.
    Explicitly,
    \[ \iprod{v,u} = \sum_{i=1}^n v_i\overline u_i \]
    We now verify the three axioms of inner products:
    \benum
        \item Linearity in the first argument:
            \[ \iprod{\alpha v+\beta w,u} = \sum_{i=1}^n (\alpha v_i+\beta w_i)\overline u_i = \alpha\sum_{i=1}^nv_i\overline u_i + \beta\sum_{i=1}^nw_i\overline u_i = \alpha\iprod{v,u} + \beta\iprod{w,u} \]
        \item Conjugate symmetry:
            \[ \overline{\iprod{u,v}} = \overline{\sum_{i=1}^n u_i\overline v_i} = \sum_{i=1}^n\overline u_iv_i = \iprod{v,u} \]
        \item Positive-definiteness: $\iprod{v,v}=\sum_{i=1}^nv_i\overline v_i=\sum_{i=1}^n\abs{v_i}$.
            If $v\neq0$ then suppose $v_i\neq0$, then $\iprod{v,v}\geq\abs{v_i}>0$ as required.
    \eenum

\end{exam*}

\begin{defn*}

    Two vectors $v,u$ in an inner product space are \ppemph{orthogonal} if their inner product is zero: $\iprod{v,u}=0$.
    This is denoted $v\perp u$.

\end{defn*}

Orthogonality generalizes our concept of perpendicular vectors on the plane, since two vectors are perpendicular if and only if their dot product is zero.

Orthogonality is a symmetric relation, since $\iprod{u,v}=\overline{\iprod{v,u}}$ and so the inner product of one is zero if and only if the other is.
Notice that immediately, every vector is orthogonal to the zero vector.
Furthermore if $v\perp u$ then $\alpha v\perp\beta u$ since $\iprod{\alpha v,\beta u}=\alpha\overline\beta\iprod{v,u}=0$.

\begin{defn*}

    A \ppemph{normed vector space} is a vector space $V$ equipped with a \ppemph{norm} function
    \[ \norm{\cdot}\colon V\longto\bR \]
    which satisfies the following axioms:
    \benum
        \item Positive-definiteness: $\norm v>0$ for all vectors $v\neq0$,
        \item Homogeneity: $\norm{\alpha v}=\abs\alpha\norm v$,
        \item The triangle inequality: $\norm{v+u}\leq\norm v+\norm u$.
    \eenum

\end{defn*}

Notice that $\norm 0=\norm{0\cdot0}=\abs0\norm0=0$, so $\norm v=0$ if and only if $v=0$.
In this course when discussing a norm, we will almost exclusively mean one generated from an inner product:
\[ \norm v\coloneqq\sqrt{\iprod{v,v}} \]
which already immediately satisfies positive-definiteness and homogeneity is obvious:
\[ \norm{\alpha v}=\sqrt{\iprod{\alpha v,\alpha v}}=\sqrt{\alpha\overline\alpha\iprod{v,v}}=\abs\alpha\sqrt{\iprod{v,v}} = \abs\alpha\norm v \]
But it will take some more work to prove the triangle inequality.

The norm generalizes the concept of the magnitude of a vector, as its axioms are exactly those you'd expect from such a ``magnitude function''.
Norms generated by inner products extend the result that $\abs v^2=v\cdot v$ for the dot product in $\bR$.

Notice that if $v$ and $u$ are orthogonal then $\iprod{v,u}=\iprod{u,v}=0$ so
\[ \norm{v+u}^2 = \iprod{v+u,v+u} = \iprod{v,v} + \iprod{u,v} + \iprod{v,u} + \iprod{u,u} = \iprod{v,v} + \iprod{u,u} = \norm v^2+\norm u^2 \]
which is a generalization of Pythagorean's theorem: since if $v$ and $u$ are orthogonal, the vectors $v,u,v+u$ form a right triangle with hypotenuse $v+u$.

\begin{thrm*}[cauchyschwarz,The\ Cauchy-Schwarz\ Inequality]

    Let $v,u\in V$ be vectors in an inner product space, then $\abs{\iprod{v,u}}\leq\norm v\cdot\norm u$ and there is equality if and only if $v$ and $u$ are linearly dependent.

\end{thrm*}

\begin{proof}

    First we prove the inequality, then we show when there is equality.
    If $u=0$ this is trivial, as $\iprod{v,u}=0$ and $\norm v\norm u=0$.
    Otherwise, let us define
    \[ z\coloneqq v - \frac{\iprod{v,u}}{\iprod{u,u}}u \]
    Let us give some intuition to this definition: $\iprod{v,u}$ is geometrically the length of the projection of $v$ onto $u$ multiplied by $\norm u$.
    And so $\frac{\iprod{v,u}}{\norm u}$ gives the length of this projection, which is parallel to $u$, thus the projection is equal to $\frac{\iprod{v,u}}{\iprod{u,u}}u$ since $\frac u{\norm u}$ is the
    unit vector in the direction of $u$ and $\norm u^2=\iprod{u,u}$.
    And so $z$ is then perpendicular to $u$, as we show:
    \[ \iprod{z,u} = \iprod{v,u} - \frac{\iprod{v,u}}{\iprod{u,u}}\iprod{u,u} = \iprod{v,u} - \iprod{v,u} = 0 \]
    Thus $z$ and $u$ are orthogonal, and so by the generalized Pythagorean theorem:
    \[ \norm v^2 = \norm{z+\frac{\iprod{v,u}}{\iprod{u,u}}u}^2 = \norm z^2 + \norm{\frac{\iprod{v,u}}{\iprod{u,u}}u}^2 = \norm z^2 + \frac{\abs{\iprod{v,u}}^2}{\iprod{u,u}^2}\norm u^2 
    = \norm z^2 + \frac{\abs{\iprod{v,u}}^2}{\norm u^2} \]
    Thus we get $\bigl(\norm v\norm u\bigr)^2=\norm z^2\norm u^2+\abs{\iprod{v,u}}^2$, which means that $\bigl(\norm v\norm u\bigr)^2\geq\abs{\iprod{v,u}}^2$ and since these are all nonnegative values, we
    get the desired inequality by taking the root of both sides.

    Notice that there is equality only when $\norm z=0$, meaning $v=\frac{\iprod{v,u}}{\iprod{u,u}}u$, so if there is equality then there is linear dependence.
    And if $v=\alpha u$ then $\abs{\iprod{v,u}}=\abs\alpha\abs{\iprod{u,u}}=\abs\alpha\norm u\norm u=\norm v\norm u$, so there is equality.
    \qed

\end{proof}

\begin{thrm*}

    The norm generated from an inner product is indeed a norm.

\end{thrm*}

\begin{proof}

    As discussed previously, all that requires verification is that the function satisfies the triangle inequality.
    \[ \norm{v+u} = \sqrt{\iprod{v,v} + \iprod{v,u} + \iprod{u,v} + \iprod{u,u}} = \sqrt{\norm v^2 + \iprod{v,u} + \iprod{u,v} + \norm u^2} \]
    since $\iprod{u,v}=\overline{\iprod{v,u}}$, $\iprod{v,u}+\iprod{u,v}=2\Re\iprod{v,u}\leq2\abs{\iprod{v,u}}$ which by \ppref{cauchyschwarz} is bound by $2\norm v\norm u$, thus
    \[ \norm{v+u} \leq \sqrt{\norm v^2 + 2\norm v\norm u + \norm u^2} = \sqrt{\bigl(\norm v+\norm u\bigr)^2} = \norm v + \norm u \]
    as required.
    \qed

\end{proof}

