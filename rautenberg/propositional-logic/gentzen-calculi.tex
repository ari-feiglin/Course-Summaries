To begin this subsection, we will define a derivability relation $\vdash$ which axiomatizes the important properties of the consequence relation $\vDash$.
Our goal is to show that by using these axioms, $\vdash$ is equivalent to $\vDash$, and this will allow us to prove important facts about $\vDash$, namely its finitaryness.

\bdefn

    We define {\emphcolor Gentzen style sequent calculus}\addtoindex{Sequent calculus}[Gentzen style] of $\vdash$ as follows: $X\vdash\alpha$ is to be read as ``$\alpha$ is derivable from $X$'' where
    $\alpha$ is a formula and $X$ is a set of formulas.
    A pair $(X,\alpha)\in\powsetof{{\cal F}}\times{\cal F}$, or more suggestively written $X\vdash\alpha$, is called a {\emphcolor sequent}.
    Gentzen-style rules have the form
    $$ \gentzen{X_1\vdash\alpha_1&\cdots&X_n\vdash\alpha_n}{X\vdash\alpha} $$
    Which is to be understood as meaning that if for every $i$, $X_i\vdash\alpha_i$, then $X\vdash\alpha$.

    Gentzen calculus has the following basic rules:

    \tabskip=0pt plus 1fil
    {\openup1\jot\halign to\hsize{($#$)\hfil\tabskip=.2cm&$#$\hfil\tabskip=2cm&($#$)\hfil\tabskip=.2cm&$#$\hfil\tabskip=0pt plus 1fil\cr
        \hbox{IS} & \gentzen{}{\alpha\vdash\alpha} & \hbox{MR} & \gentzen{X\vdash\alpha}{X'\vDash\alpha}\quad (X\subseteq X')\cr
        \land1 & \gentzen{X\vdash\alpha,\beta}{X\vdash\alpha\land\beta} & \land2 & \gentzen{X\vdash\alpha\land\beta}{X\vdash\alpha,\beta}\cr
        \neg1 & \gentzen{X\vdash\alpha,\neg\alpha}{X\vdash\beta} & \neg2 & \gentzen{X,\alpha\vdash\beta & X,\neg\alpha\vdash\beta}{X\vdash\beta}\cr
    }}

    (IS means ``initial sequent'', MR means monotonacity rule.)

    Now we say that $\alpha$ is derivable from $X$, in short $X\vdash\alpha$, if $S_n=X\vdash\alpha$ and there exists a sequence of sequents $(S_0;\dots;S_n)$ where for every $S_i$, $S_i$ is either an
    initial sequent (IS) or derivable using the basic rules from previous sequents in the sequence.

\edefn

For example, we can derive $\alpha\land\beta$ from $\set{\alpha,\beta}$, meaning $\alpha,\beta\vdash\alpha\land\beta$.
This can be done by the sequence:

$$
\left(\vcenter{
    \halign{\hfil$#$\hfil&&\ ;\ \hfil$#$\hfil\cr
        \alpha\vdash\alpha & \alpha,\beta\vdash\alpha & \beta\vdash\beta & \alpha,\beta\vdash\beta & \alpha,\beta\vdash\alpha\land\beta\cr
        \hbox{IS} & \hbox{MR} & \hbox{IS} & \hbox{MR} & \land1\cr
    }
}\right)
$$

Let us prove some more useful rules

\def\gproof#1{%
    \vtop{%
        \count2=0\relax%
        \tabskip=0pt%
        \halign{\global\advance\count2 by 1\relax\the\count2\quad##&$##$\hfil\tabskip=1cm&##\hfil\cr
            #1\crcr
        }%
    }%
}
 
\penalty0
\tabskip=0pt plus 1fil
{\openup1\jot\halign to \hsize{\hfil#\hfil\tabskip=2cm&#\tabskip=0pt plus 1fil\cr
    $\gentzen{X,\neg\alpha\vdash\alpha}{X\vdash\alpha}$\cr
    ($\neg$-elimination)&
        \gproof{%
            &X,\alpha\vdash\alpha& (IS), (MR)\cr
            &X,\neg\alpha\vdash\alpha& supposition\cr
            &X\vdash\alpha& $(\neg2)$\cr
        }\cr
    $\gentzen{X,\neg\alpha\vdash\beta,\neg\beta}{X\vdash\alpha}$\cr
    (reductio ad absurdum)&
        \gproof{%
            &X,\neg\alpha\vdash\beta,\neg\beta & supposition\cr
            &X,\neg\alpha\vdash\alpha & $(\neg1)$\cr
            &X\vdash\alpha&$\neg$-elimination\cr
        }\cr
    $\gentzen{X\vdash\alpha\to\beta}{X,\alpha\vdash\beta}$\cr
    ($\to$-elimination)&
        \gproof{%
            &X,\alpha,\neg\beta\vdash\alpha,\neg\beta& (IS), (MR)\cr
            &X,\alpha,\neg\beta\vdash\alpha\land\neg\beta& $(\land1)$\cr
            &X\vdash\neg(\alpha\land\neg\beta)& supposition\cr
            &X,\alpha,\neg\beta\vdash\neg(\alpha\land\neg\beta)& (MR)\cr
            &X,\alpha,\neg\beta\vdash\beta & $(\neg1)$ on $2$ and $4$\cr
            &X,\alpha\vdash\beta& $\neg$-elimination\cr
        }\cr
    $\gentzen{X\vdash\alpha & X,\alpha\vdash\beta}{X\vdash\beta}$\cr
    (cut rule)&
        \gproof{%
            &X,\neg\alpha\vdash\alpha & supposition, (MR)\cr
            &X,\neg\alpha\vdash\neg\alpha & (IS), (MR)\cr
            &X,\neg\alpha\vdash\beta & $(\neg1)$\cr
            &X,\alpha\vdash\beta & supposition\cr
            &X\vdash\beta & $(\neg2)$ on $3$ and $4$\cr
        }\cr
    $\gentzen{X,\alpha\vdash\beta}{X\vdash\alpha\to\beta}$\cr
    ($\to$-introduction)&
        \gproof{%
            &X,\alpha\land\neg\beta,\alpha\vdash\beta & supposition, (MR)\cr
            &X,\alpha\land\neg\beta\vdash\alpha & (IS), (MR), $(\land2)$\cr
            &X,\alpha\land\neg\beta\vdash\beta & cut rule\cr
            &X,\alpha\land\neg\beta\vdash\neg\beta & (IS), (MR), $(\land2)$\cr
            &X,\alpha\land\neg\beta\vdash\alpha\to\beta & $(\neg1)$\cr
            &X,\neg(\alpha\land\neg\beta)\vdash\alpha\to\beta & (IS), (MR)\cr
            &X\vdash\alpha\to\beta & $(\neg2)$ on $5$ and $6$\cr
        }\cr
    $\gentzen{X\vdash\alpha,\alpha\to\beta}{X\vdash\beta}$\addtoindex{modus ponens}\cr
    (modus ponens)&
        \gproof{%
            &X\vdash\alpha\to\beta & supposition\cr
            &X,\alpha\to\beta & $\to$-elimination\cr
            &X\vdash\alpha & supposition\cr
            &X\vdash\beta & cut rule\cr
        }\cr
}}
\medskip

$\to$-elimination and $\to$-introduction give us the {\it syntactic deduction theorem}\addtoindex{deduction theorem}[syntactic]:
$$ X,\alpha\vdash\beta \iff X\vdash\alpha\to\beta $$

Let $R$ be a rule of the form
$$ R\colon \gentzen{X_1\vdash\alpha_1 & \cdots & X_n\vdash\alpha_n}{X\vdash\alpha} $$
Then we say that a property of sequents ${\cal E}$ is {\it closed under $R$} if ${\cal E}(X_1,\alpha_1),\dots,{\cal E}(X_n,\alpha_n)$ implies ${\cal E}(X,\alpha)$.

\bprop[title=Principle of Rule Induction, name=ruleinduction]

    Let ${\cal E}$ be a property of sequents which is closed under all the basic rules of $\vdash$.
    Then $X\vdash\alpha$ implies ${\cal E}(X,\alpha)$.

\eprop

We will prove this by induction on the length of the derivation of $S=X\vdash\alpha$, $n$.
If $n=1$ then $X\vdash\alpha$ must be an initial sequent and so by assumption ${\cal E}(X,\alpha)$.
For the induction step, suppose the derivation is $(S_0;\dots;S_n)$, so $S=S_n$.
Then by our inductive hypothesis ${\cal E}S_i$ for all $i<n$.
If $S$ is an initial sequent then ${\cal E}S$ holds by assumption.
Otherwise $S$ is obtained by applying a basic rule on some of the sequents $S_i$ for $i<n$.
And since ${\cal E}S_i$ and ${\cal E}$ is closed under basic rules, we have that ${\cal E}S$ as required.
\qed

\blemm

    If $X\vdash\alpha$ then $X\vDash\alpha$.
    More suggestively,
    $$ {\vdash}\subseteq{\vDash} $$

\elemm

Using the principle of rule induction, let ${\cal E}(X,\alpha)$ mean $X\vDash\alpha$ (formally this means ${\cal E}=\set{(X,\alpha)}[X\vDash\alpha]$).
Then we must show that ${\cal E}$ is closed under all the basic rules of $\vdash$.
This means that we must show that
\multlines{%
    \alpha\vDash\alpha,\quad X\vDash\alpha\implies X'\vDash\alpha\hbox{ for $X\subseteq X'$},\quad X\vDash\alpha,\beta \iff X\vDash\alpha\land\beta,&
    X\vDash\alpha,\neg\alpha\implies X\vDash\beta,\quad X,\alpha\vDash\beta\hbox{ and }X,\neg\alpha\vDash\beta\implies X\vDash\beta%
}
These are all readily verifiable (and some we have already shown).
So ${\cal E}$ is indeed closed under all the basic rules of $\vdash$, and so ${\cal E}(X,\alpha)$ (meaning $X\vDash\alpha$) implies $X\vdash\alpha$.
\qed

\bthrm[name=vdashfinite]

    If $X\vdash\alpha$ then there exists a finite subset $X_0\subseteq X$ such that $X_0\vdash\alpha$.

\ethrm

Let ${\cal E}(X,\alpha)$ be the property that there exists a finite subset $X_0\subseteq X$ such that $X_0\vdash\alpha$.
We will show that ${\cal E}$ is closed under the basic rules of $\vdash$.
Trivially, ${\cal E}(X,\alpha)$ holds for $X=\set\alpha$, meaning ${\cal E}$ holds for (IS).
And similarly if ${\cal E}(X,\alpha)$ and $X\subseteq X'$, since there exists a finite $X_0\subseteq X$ such that $X_0\vdash\alpha$, this same $X_0$ is a subset of $X'$ and so ${\cal E}(X',\alpha)$ so
${\cal E}$ is closed under (MR).

Now if ${\cal E}(X,\alpha)$ and ${\cal E}(X,\beta)$ then suppose $X_1\vdash\alpha$ and $X_2\vdash\beta$ where $X_1,X_2\subseteq X$ are finite.
Then $X_0=X_1\cup X_2$ is finite, $X_0\vdash\alpha,\beta$ and so $X_0\vdash\alpha\land\beta$, and since $X_0\subseteq X$ is finite, ${\cal E}(X,\alpha\land\beta)$ so ${\cal E}$ is closed under $(\land1)$.
Closure under the rest of the basic rules can be shown similarly.
\qed

\bdefn

    A set of formulas $X$ is {\emphcolor inconsistent} if $X\vdash\alpha$ for every formula $\alpha$.
    If $X$ is not inconsistent, it is termed {\emphcolor consistent}\addtoindex{consistent}.
    $X$ is {\emphcolor maximally consistent}\addtoindex{consistent}[maximally] if $X$ is consistent but for every proper superset $X\subset Y$, $Y$ is inconsistent.

\edefn

Notice that $X$ is inconsistent if and only if $X\vdash\bot$.
Obviously if $X$ is inconsistent, $X\vdash\bot$.
Conversely, if $X\vdash\bot$ then $X\vdash p_1\land\neg p_1$ and so by $(\land2)$, $X\vdash p_1,\neg p_2$ and thus by $(\neg1)$ for all formulas $\alpha$, $X\vdash\alpha$.

Furthermore, if $X$ is consistent it is maximally consistent if and only if for every formula $\alpha$, either $\alpha\in X$ or $\neg\alpha\in X$ exclusively.
If neither $\alpha$ nor $\neg\alpha$ are in $X$, then since $X$ is maximally consistent, $X,\alpha\vdash\bot$ and $X,\neg\alpha\vdash\bot$ and therefore by $(\neg2)$, $X\vdash\bot$ contradicting $X$'s
consistency.
And if $X$ contains $\alpha$ or $\neg\alpha$ for every formula $\alpha$, then it is maximal: adding another formula $\alpha$ would mean that $\alpha,\neg\alpha\in X$ and so by (IS), (MR), and $(\neg2)$, $X$
would be inconsistent.

This means that maximally consistent sets $X$ are {\it deductively closed}\addtoindex{deductively closed}:
$$ X\vdash\alpha \iff \alpha\in X $$
Obviously if $\alpha\in X$ then by (IS) and (MR), $X\vdash\alpha$.
Now suppose that $X\vdash\alpha$, then since $\alpha\in X$ or $\neg\alpha\in X$, we cannot have $\neg\alpha\in X$ since $X$ is consistent.
Therefore $\alpha\in X$.

\blemm

    The derivability relation has the following properties:
    $$ {\tt C}^+\colon\quad X\vdash\alpha\iff X,\neg\alpha\vdash\bot,\qquad {\tt C}^-\colon\quad X\vdash\neg\alpha\iff X,\alpha\vdash\bot $$
    Meaning $\alpha$ is derivable from $X$ if and only if $X\cup\set{\neg\alpha}$ is inconsistent.
    And similarly $\neg\alpha$ is derivable from $X$ if and only if $X\cup\set{\alpha}$ is inconsistent.

\elemm

We will prove ${\tt C}^+$.
Suppose $X\vdash\alpha$, then $X,\neg\alpha\vdash\alpha$ by (MR) and $X,\neg\alpha\vdash\neg\alpha$ by (IS) and (MR).
Thus by $(\neg1)$, $X,\neg\alpha\vdash\beta$ for all formulas $\beta$ by $(\neg1)$ and in particular, $X,\neg\alpha\vdash\bot$.
Now suppose $X,\neg\alpha\vdash\bot$ then by $(\land2)$ and $(\neg1)$, we have $X,\neg\alpha\vdash\alpha$ then by $\neg$-elimination, $X\vdash\alpha$.
${\tt C}^-$ is proven similarly.
\qed

\blemm[title=Lindenbaum's Theorem, name=lindenbaum]

    Every consistent set of formulas $X\subseteq{\cal F}$ can be extended to a maximally consistent set of formulas $X\subseteq X'\subseteq{\cal F}$.

\elemm

Let us define the set
$$ H = \set{Y\subseteq{\cal F}}[\hbox{$Y$ is consistent and $X\subseteq Y$}] $$
This is partially ordered with respect to $\subseteq$, and since $X\in H$, $H$ is not empty.
Let $C\subseteq H$ be a chain, meaning that for every $Z,Y\in C$, either $Z\subseteq Y$ or $Y\subseteq Z$.
Now we claim that $U=\bigcup C$ is an upper bound for $C$.
So we must show that $U\in H$.
Suppose not, then $U$ is not consistent meaning $U\vdash\bot$.
But then there must exist a finite $U_0=\set{\alpha_1,\dots,\alpha_n}\subseteq U$ such that $U_0\vdash\bot$.
Now suppose $\alpha_i\in Y_i\in C$ then since $C$ is linearly ordered, we can assume that every $Y_i$ is contained within $Y_n$.
But then by (MR), $Y_n\vdash\bot$ which conradicts $Y_n\in H$ being consistent.

So $U$ is consistent and so $U\in H$, and obviously for every $Y\in C$, $Y\subseteq U$.
So $U$ is an upper bound for $C$, meaning that every chain in $H$ has an upper bound in $H$, and so by Zorn's Lemma, $H$ has a maximal element.
This maximal element, call it $X'$, is precisely a maximally consistent set containing $X$: it is consistent and contains $X$ since it is in $H$, and it is maximal in $H$ so for every $X\subseteq Y$,
$Y\notin H$ so $Y$ is inconsistent.
\qed

\blemm

    A maximally consistent set of formulas $X$ has the following property:
    $$ X\vdash\neg\alpha \iff X\nvdash\alpha $$
    for all formulas $\alpha$.

\elemm

If $X\vdash\neg\alpha$ then $X\nvdash\alpha$ due to $X$'s consistency.
If $X\nvdash\alpha$ then $X\cup\set{\neg\alpha}$ is consistent in lieu of ${\tt C}^+$.
But since $X$ is maximal, $X\cup\set{\neg\alpha}=X$ meaning $\neg\alpha\in X$ and so by (IS) and (MR), $X\vdash\neg\alpha$.\
\qed

\blemm

    Maximally consistent sets are satisfiable.

\elemm

Suppose $X$ is maximally consistent, then let us define the valuation $w$ by $w\vDash p \iff X\vdash p$.
Then we claim that
$$ X\vdash\alpha \iff w\vDash\alpha $$
This is trival for prime formulas.
Now if $X\vdash\alpha\land\beta$:
$$ X\vdash\alpha\land\beta \iff X\vdash\alpha,\beta \iff w\vDash\alpha,\beta \iff w\vDash\alpha\land\beta $$
where the second equivalence is due to the induction hypothesis.
And if $X\vdash\neg\alpha$:
$$ X\vdash\neg\alpha \iff X\nvdash\alpha \iff w\nvDash\alpha \iff w\vDash\neg\alpha $$
The first equivalence is due to the previous lemma, and the second is due to the induction hypothesis.
And therefore $w\vDash X$, meaning $X$ is satisfiable.
\qed

\bthrm[title=The Completeness Theorem, name=propcompletethrm]

    Let $X$ and $\alpha$ be an arbitrary set of formulas and formula respectively.
    Then $X\vdash\alpha$ if and only if $X\vDash\alpha$.
    More suggestively,
    $$ {\vdash}={\vDash} $$

\ethrm

We have already shown that ${\vdash}\subseteq{\vDash}$ and so all that remains is to show the converse.
Suppose that $X\nvdash\alpha$, then $X,\neg\alpha$ is consistent by ${\tt C}^+$.
Thus it can be extended to a maximally consistent set $X,\neg\alpha\subseteq X'$ which is satisfiable.
Therefore so is $X,\neg\alpha$, which means that $X\nvDash\alpha$.
\qed

We get the following theorem as an immediate result from \refmath{propcompletethrm} and \refmath[theorem]{vdashfinite}:

\bthrm

    $X\vDash\alpha$ if and only if $X_0\vDash\alpha$ for a finite $X_0\subseteq X$.

\ethrm

\bthrm[title=The Compactness Theorem, name=propcompactthrm]

    A set $X\subseteq{\cal F}$ is satisfiable if and only if every finite $X_0\subseteq X$ is satisfiable.

\ethrm

Obviously if $X$ is satisfiable, so is $X_0\subseteq X$.
Now if $X$ is not satisfiable, then $X\vdash\bot$ and so there exists a finite $X_0\subseteq X$ such that $X_0\vdash\bot$ (and so $X_0\vDash\bot$) by the previous theorem.
And so if $X$ is not satisfiable, there exists a finite $X_0\subseteq X$ which is not satisfiable.
\qed

Let us now give some examples of applications of the compactness theorem.

\bprop

    Every set $M$ can be linearly (also known as totally) ordered.

\eprop

If $M$ is finite, this is trivial: if $M=\set{m_1,\dots,m_n}$ simply define $m_1<\cdots<m_n$.
Now let $M$ be any set, let us define the propositional variable (aka prime formula) $p_{ab}$ for every $(a,b)\in M\times M$.
This will represent $a<b$.
So we define $X$ to be the set of the following formulas, which represents $M$ being linearly ordered,
$$ \eqalign{
    \neg p_{aa} &\quad (a\in M),\cr
    p_{ab}\land p_{bc}\to p_{ac} &\quad (a,b,c\in M),\cr
    p_{ab}\lor p_{ba} &\quad (a\neq b\in M)\cr
} $$
If $X$ is satisfiable, suppose $w\vDash X$, then we define the linear order $a<b$ if and only if $w\vDash p_{ab}$.
Thus $X$ is precisely the set of conditions necessary for $<$ to be a linear order: the first condition is irreflexivity, the second is transitivity, and the third totality (antisymmetry is gained through
the combination of irreflexitivity and transitivity).

So if $X$ is satisfiable, then $M$ can be linearly ordered.
By the compactness theorem, we need only to show that every finite $X_0\subseteq X$ is satisfiable.
If $X_0\subseteq X$ is finite, then let us define $M_0$ to be the set of all symbols in $M$ which occur in formulas in $X_0$.
Since $X_0$ is finite, so is $M_0$ and therefore $M_0$ can be linearly ordered.
Let us define $w_0\vDash p_{ab}\iff a<b$ in $M_0$, then $w_0\vDash X_0$.
So by the compactness theorem $X$ is satisfiable, as required.
\qed

Recall that showing that every set can be well-ordered (the well-ordering theorem) is equivalent to the axiom of choice.
Since the compactness theorem is actually weaker than the axiom of choice, the linear ordering theorem (what we just showed) is weaker than the well-ordering theorem.
Which is not surprising.

\bprop

    A graph is $k$-colorable if and only if every finite subgraph is $k$-colorable.

\eprop

A {\it graph} is a pair $G=(V,E)$ where $V$ is a set of {\it vertices} and $E$ is a set of {\it edges}.
$E$ is a subset of $\set{\set{v,u}}[v\neq u\in V]$.
The graph $G$ is $k$-colorable if $V$ can be partitioned into $k$ {\it color classes}: $V=C_1\dcup\cdots\dcup C_k$ such that if $a,b\in C_i$ then $\set{a,b}\notin E$, meaning two neighboring vertices do
not have the same color.

Obviously if a graph is $k$-colorable, so is every subgraph.
To show the converse, let $G=(V,E)$ be a graph, then let us define the set of formulas $X$, where prime formulas are of the form $p_{a,i}$ where $a\in V$ and $1\leq i\leq k$:

$$ \eqalign{
    p_{a,1}\lor\cdots\lor p_{a,k} & (a\in V)\cr
    \neg(p_{a,i}\land p_{a,j}) & (a\in V,\,1\leq i<j\leq k)\cr
    \neg(p_{a,i}\land p_{b,i}) & (\set{a,b}\in E,\,1\leq i\leq k)\cr
} $$

If $X$ is satisfiable, $w\vDash X$, then we define $C_i=\set{a\in V}[w\vDash p_{a,i}]$, ie. we color $a\in V$ with the color $i$ if and only if $p_{a,i}$ is satisfied.
Then $V=C_1\cup\cdots\cup C_k$ since for every $a\in V$, $w\vDash p_{a,1}\lor\cdots\lor p_{a,k}$, so for every $a\in V$ there exists an $1\leq i\leq k$ such that $w\vDash p_{a,i}$ so $a\in C_i$.
And $C_i\cap C_j=\varnothing$ in lieu of $\neg(p_{a,i}\land p_{a,j})$.
And if $\set{a,b}\in E$ then $a$ and $b$ cannot be in the same color class by $\neg(p_{a,i}\land p_{b,i})$.
So the $C_i$s give a valid $k$-coloring of $G$.

Let $X_0\subseteq X$ be finite, then let us define $G_0=(V_0,E_0)$ where $V_0$ is the set of vertices appearing in formulas in $X_0$, and $E_0$ be the edges connecting them.
By assumption, $G_0$ is $k$-colorable since it is finite.
Now we define the valuation $w_0$ such that $w_0\vDash p_{a,i}$ if and only if $a$ is in the $i$th color class for $a\in V_0$.
This must model $X_0$ since $X_0$ includes only statements saying that $G_0$ can be $k$-colored.
So by the compactness theorem, $X$ is satisfiable, as required.
\qed

There are more examples of applications of the compactness theorem.
For example, the ultrafilter theorem, which we will visit later on.
