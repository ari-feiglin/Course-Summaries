\subsection{Definitions}

We now define {\it Probably Approximately Correct (PAC)} Learning.

\bdefn

    Let $\c X$ be the {\emphcolor input space}, the set of all possible examples or instances.
    The set of all {\emphcolor labels} or {\emphcolor target values} is $\c Y$.
    For now, we restrict our view to be binary: $\c Y=\set{0,1}$.
    A {\emphcolor concept} is a map $c\colon\c X\longto\c Y$, or equivalently a subset of $\c X$ (the set $\set{x\in\c X}[c(x)=1]$).
    A {\emphcolor concept class} is a class of concepts which we would like to learn (approximate) and is denoted $\c C$.

\edefn

The idea of PAC learning is as follows: the learner considers a fixed set of concepts $\c H$ called the {\it hypothesis set}, which may or may not coincide with $\c C$.
The learner then receives a sequence of samples $S=(x_1,\dots,x_n)$ which are independent and distribute according to some distribution $\c D$.
The learner also receives labels $(c(x_1),\dots,c(x_n))$ according to some concept $c\in\c C$ which it is tasked with learning.
Using this information the learner attempts to choose a hypothesis $h_S\in\c H$ which minimizes the {\it generalization error} (or {\it risk}):

\bdefn

    Given a hypothesis $h\in\c H$, target concept $c\in\c C$, and an underlying distribution $\c D$, the {\emphcolor generalized error} (or {\emphcolor risk}) is:
    $$ R(h) = \probof{h(x)\neq c(x)}[x\sim\c D] = \expecof{\b1\set{h(x)\neq c(x)}}[x\sim\c D] $$
    i.e. it is the probability that $h(x)$ differs from $c(x)$ when $x$ is chosen randomly with a distribution of $\c D$.

\edefn

But the generalized error cannot be known to the learner, as it knows not the target concept nor the underlying distribution.
So instead the learner minimizes the {\it empirical error} (or {\it risk}):

\bdefn

    Given a hypothesis $h\in\c H$, a target concept $c\in\c C$, and a sample $S=(x_1,\dots,x_n)$, define the {\emphcolor empirical error} (or {\emphcolor risk}) to be:
    $$ \hat R_S(h) = \frac1n\sum_{i=1}^n\b1\set{h(x_i)\neq c(x_i)} $$

\edefn

Notice that
$$ \eqalign{
    \expecof{\hat R_S(h)}[S\sim\c D^n] &= \frac1n\sum_{i=1}^n\expecof{\b1\set{h(x_i)\neq c(x_i)}}[S\sim\c D^n]\cr
    &= \frac1n\sum_{i=1}^n\expecof{\b1\set{h(x)\neq c(x)}}[x\sim\c D] = \frac1n\sum_{i=1}^nR(h) =
    R(h)
} $$

We now formally define what PAC learning is.
Let $n$ be a number such that the size of every $x\in\c X$ can be represented in $O(n)$ space, for $c\in\c C$ let $\size c$ be the maximal computational cost of $c$.
We focus on algorithms $\c A$ which take as input a sample $S$ and return a hypothesis $h_S$.

\bdefn

    A concept class $\c C$ is {\emphcolor PAC-learnable} if there exists an algorithm $\c A$ and a polynomial function $\poly(\bullet,\bullet,\bullet,\bullet)$ such that for all $\epsilon,\delta>0$,
    distribution $\c D$ on $\c X$ and target concept $c\in\c C$, for every sample size $n\geq\poly(1/\epsilon,1/\delta,n,\size c)$,
    $$ \probof{R(h_S)\leq\epsilon}[S\sim\c D^n] \geq 1 - \delta $$
    If $\c A$ runs in $\poly(1/\epsilon,1/\delta,n,\size c)$ time then $\c C$ is {\emphcolor efficiently PAC-learnable}.
    An algorithm $\c A$, if one exists, is called a {\emphcolor PAC-learning algorithm} for $\c C$.

\edefn

The intuition is as follows: a concept class $\c C$ is PAC-learnable if there exists an algorithm $\c A$ where given a sample size at least polynomial in $1/\epsilon$ and $1/\delta$, it returns a hypothesis
with an error bound by $\epsilon$ at least $1-\delta$ of the time.
Note that if the running time is polynomial in $1/\epsilon$ and $1/\delta$, then assuming the total input is read by the algorithm, the input must too be polynomial in $1/\epsilon$ and $1/\delta$.

\subsection{The Finite Case}

Given a concept $c\in\c C$ and a sample $S=(x_1,\dots,x_n)$, call an hypothesis $h\in\c H$ {\it consistent} if $h(x_i)=c(x_i)$ for all $1\leq i\leq n$.
Equivalently, $\hat R_S(h)=0$.
We will assume that our hypotheses are consistent, and so we can always assume that our target concept is in $\c H$.

\bthrm

    Let $\c H$ be a finite set of hypotheses, and $\c A$ an algorithm such that for any target concept $c\in\c H$, $\c A$ returns a consistent hypothesis $h_S$ for any input sample $S$ (where $S\sim\c D^n$).
    Then for every every $\epsilon,\delta>0$, the inequality $\probof{R(h_S)\leq\epsilon}[S\sim\c D^n]\geq1-\delta$ holds if
    $$ n\geq\frac1\epsilon\cdot\log\frac{\abs{\c H}}\delta $$

\ethrm

\Proof let $\epsilon>0$, and define $\c H_\epsilon=\set{h\in\c H}[R(h)>\epsilon]$.
The probability that $h\in\c H_\epsilon$ is consistent is
\multlines{
    \probof{\hat R_S(h)=0} = \probof{h(x_1)=c(x_1),\dots,h(x_n)=c(x_n)}[x_i\sim\c D]\cr
    &= \prod_{i=1}^n\probof{h(x_i)=c(x_i)}[x_i\sim\c D] = \probof{h(x)=c(x)}[x\sim\c D]^n = \bigl(1-\probof{h(x)\neq c(x)}[x\sim\c D]\bigr)^n
}
this is since the samples are independent and distributively equal.
Now recall that by definition, we have that $\probof{h(x)\neq c(x)}[x\sim\c D]=R(h)$ which is greater than $\epsilon$ by definition, so
$$ \probof{\hat R_S(h)=0} \leq (1-\epsilon)^n $$
Thus the probability that a ``bad'' hypothesis tricks us by being consistent is bound by $(1-\epsilon)^n$.
Now, we want to show that the probability that there exists a bad consistent hypothesis is bound by $\delta$.
This means that with probability at least $1-\delta$, there exist no bad consistent hypotheses and so $h_S$ is necessarily a ``good'' hypothesis with $R(h_S)\leq\epsilon$.
The probability of there existing a consistent bad hypothesis is
$$ \probof{\exists h\in\c H_\epsilon.\;\hat R_S(h)=0} \leq \sum_{h\in\c H_\epsilon}\probof{\hat R_S(h)=0} \leq \sum_{h\in\c H_\epsilon}(1-\epsilon)^n \leq \abs{\c H}(1-\epsilon)^n $$
We also know that $(1-\epsilon)^n\leq e^{-n\epsilon}$, so this probability is bound by $\abs{\c H}e^{-n\epsilon}$.
If $n\geq\frac1\epsilon\cdot\log\frac{\abs{\c H}}\delta$ then this is bound by $\delta$, as required.
\qed

Since $\frac1\epsilon\cdot\log\frac{\abs{\c H}}\delta$ is bound by some polynomial in $\frac1\epsilon,\frac1\delta$, this means that when $\c H$ is finite a consistent algorithm $\c A$ is PAC-learning.
But what if our algorithm isn't consistent, i.e. the target concept is not in $\c H$?

\bthrm

    For every $\epsilon,\delta>0$ if $n>\frac1{\epsilon^2}\log\frac{2\abs{\c H}}\delta$ then
    $$ \probof{R(h_S) - \min_{h\in\c H}R(h)\leq\epsilon} \geq 1 - \delta $$

\ethrm

\Proof similar to before, but using Hoeffding's inequality $\probof{S-\expecof S\geq\epsilon}\leq\exp(-2\epsilon^2/n)$.\qed

This is a generalization of the previous theorem, since if $c$ is taken from $\c H$ then $\min_{h\in\c H}R(h)=0$.
Call a concept class $\c C$ which satisfies this probability bound {\it Agonistically PAC-learnable}:

\bdefn

    A concept class $\c C$ is {\emphcolor agonistically PAC-learnable} if there exists an {\emphcolor agnostic PAC-learner} $\c A$ such that for every $\epsilon,\delta>0$ distribution $\c D$ and 
    $n\geq{\it poly}(1/\epsilon,1/\delta,n,\size c)$ for some ${\it poly}$,
    $$ \probof{R(h_S) - \min_{h\in\c H}R(h)\leq\epsilon} \geq 1 - \delta $$

\edefn

So assuming the hypothesis class is finite, every concept class is PAC-learnable in the agnostic sense.

