\subsection{$k$-Means}

Suppose we have samples $\set{\b x_i}_{i=1}^n$ from ${\bb R}^d$ and a number of clusters $K$.
We want to assign to every $\b x_i$ a cluster in $\set{1,\dots,K}$.
The idea behind our algorithm is to find a set of {\it prototypes} $\fb\mu=\set{\fb\mu_i}_{i=1}^n\subseteq{\bb R}^d$ and we cluster $\b x_i$ according to which prototype it is closest to.
That is, we want to find the $j$ which minimizes $d(\b x_i,\fb\mu)=\min_j\norm{\b x_i-\fb\mu_j}^2$, and we assign to $\b x_i$ the cluster $j$.

The problem is that we want to minimize $D(\fb\mu)=\sum_id(\b x_i,\fb\mu)$, i.e. find $\fb\mu^*=\argmin_{\fb\mu}D(\fb\mu)$.
Unfortunately this problem is NP-hard, so instead we present an algorithm to iteratively improve $D(\fb\mu)$.

\algorithm
\Function{$k$-Means}{$\set{\b x_i}_{i=1}^n$}
    \State let $\set{\fb\mu_j^0}_{j=1}^K$ be randomly initialized
    \For{$t=0,\dots,M$}
        \State for each $i$, ${\rm cluster}(\b x_i)\gets\argmin_j\norm{\b x_i,\fb\mu_j^t}^2$
        \State for each $j$, $\fb\mu_j^{t+1}={\rm mean}\set{\b x_i}[{\rm cluster}(\b x_i)=j]$
    \EndFor
    \State\Return $\set{\fb\mu_j}_{j=1}^K$
\EndFunc
\ealgorithm

\blemm

    Let $\set{\b x_i}_{i=1}^n\subseteq{\bb R}^d$, then
    $$ {\rm mean}\set{\b x_i}_{i=1}^n = \argmin\c L(\fb\mu) = \argmin\sum_i\norm{\b x_i-\fb\mu}^2 $$

\elemm

\Proof we differentiate $\c L(\fb\mu)$ and get
$$ \frac\partial{\partial\fb\mu}\c L = 2\sum_i(\b x_i-\fb\mu) $$
This is zero if and only if
$$ \fb\mu = \frac1n\sum_i\b x_i = {\rm mean}\set{\b x_i}_{i=1}^n \qed $$

\bprop

    In the above algorithm, $D(\fb\mu)$ improves every iteration.

\eprop

\Proof let $\fb\mu^t=\set{\fb\mu_j^t}_{j=1}^K$, so we want to show that $D(\fb\mu^t)\geq D(\fb\mu^{t+1})$.
Let us define $r_{ij}=1$ if $\b x_i$ is in cluster $j$ and zero otherwise.
Notice then that
$$ d(\b x_i,\fb\mu) = \min_j\norm{\b x_i-\fb\mu_j}^2 = \min_{r_{ij}}\sum_jr_{ij}\norm{\b x_i-\fb\mu_j}^2 $$
Thus
$$ \min_{\fb\mu}D(\fb\mu) = \min_{\fb\mu}\sum_id(\b x_i,\fb\mu) = \min_{\fb\mu}\min_{r_{ij}}\sum_{i,j}r_{ij}\norm{\b x_i-\fb\mu_j}^2 $$
let us denote this as $\min_{\fb\mu}\min_{\b r}g(\fb\mu,\b r)$, so $D(\fb\mu)=\min_{\b r}g(\fb\mu,\b r)$.
Now let us define a new procedure where every iteration takes two steps:
\benum
    \item $\b r^{t+1}=\argmin_{\b r}g(\fb\mu^t,\b r)$
    \item $\fb\mu^{t+1}=\argmin_{\fb\mu}g(\fb\mu,\b r^{t+1})$
\eenum
After these two steps we have that
$$ g(\fb\mu^{t+1},\b r^{t+1}) = \min_{\b r}g(\fb\mu^{t+1},\b r) = D(\fb\mu^{t+1}) $$
And since $g(\fb\mu^t,\b r^t)\geq g(\fb\mu^{t+1},\b r^{t+1})$ we have that $D(\fb\mu^{t+1})\leq D(\fb\mu^t)$.

Now we will show that this procedure is equivalent to {\tencsc $k$-Means}.
\benum
    \item $\b r^{t+1}=\argmin_{\b r}g(\fb\mu^t,\b r)$: this minimizes $\min_{\b r}\sum_jr_{ij}\norm{\b x_i-\fb\mu_j^t}^2$, i.e. we find assignments $r_{ij}$ which minimizes $\norm{\b x_i-\fb\mu_j^t}^2$.
    This is what line 4 does in {\tencsc $k$-Means}.
    \item $\fb\mu^{t+1}=\argmin_{\fb\mu}g(\fb\mu,\b r^{t+1})$, this finds $\fb\mu_j$ which minimizes $\min_{\fb\mu_j}\sum_{{\rm cluster}(\b x_i)=j}\norm{\b x_i-\fb\mu_j}^2$.
    By the above lemma, this just means $\fb\mu_j={\rm mean}\set{\b x_i}[{\rm cluster}(\b x_i)=j]$, which is what line 5 does.
\eenum
\qed

\subsection{Expectation Maximization}

Recall the following result:

\bthrm[title=Jensen's Inequality]

    Let $X$ be a random variable and $\phi\colon{\bb R}\longto{\bb R}$ a convex function then
    $$ \phi(\expecof X) \leq \expecof{\phi X} $$

\ethrm

\bdefn

    Let $P,Q$ be two distributions on $\c X$ with pdfs $p,q$ respectively.
    Then their {\emphcolor Kullback-Leibler divergence} (KL divergence) is defined as
    $$ \klof{P\|Q} = \int_{\c X}p(x)\log\frac{p(x)}{q(x)}\,dx = \expecof[x\sim P]{\log\frac{p(x)}{q(x)}} $$

\edefn

\bthrm

    $\klof{P\|Q}\geq0$

\ethrm

\Proof by flipping the fraction, we have
$$ \klof{P\|Q} = \expecof[x\sim P]{-\log\frac{q(x)}{p(x)}} \geq -\log\expecof[x\sim P]{\frac{q(x)}{p(x)}} $$
where the inequality is due to Jensen.
Continuing, we have
$$ = -\log\int_{\c X}p(x)\frac{q(x)}{p(x)}\,dx = -\log\int_{\c X}q(x) = -\log1 = 0 \qed $$

Suppose we have samples $\b X\subseteq\c X$ whose distribution is dependent on some latent variables $\b Z\subseteq\c Z$.
Suppose we know the joint distribution of $(\b X,\b Z)$ up to some unkown $\fb\theta$, $(\b X,\b Z)\sim P(\fb\theta)$, and let the distribution of $\b Z$ be $Q$.
Let the pdf of $Q$ be $q$ and the pdf of $P(\fb\theta)$ be $p_{\fb\theta}$.
Given that we've seen $\b X$ we want to find the $\fb\theta$ which maximizes $\probof{\fb\theta}[\b X]$, this can be written as
$$ \probof{\b X}[\fb\theta] = \int_{\c Z}\probof{\b X,\b Z}[\fb\theta]\,d\b Z = \int_{\c Z}p_{\fb\theta}(\b X,\b Z)\,d\b Z $$
This is unfortunately hard to maximize.

So instead let us focus on maximizing $\expecof{\log\probof{\b X}[\fb\theta]}$.
We know that $(\b X,\b Z)\sim P(\fb\theta)$ and $\b Z\sim Q$ so
$$ \expecof{\log\probof{\b X}[\fb\theta]} = \int_{\c Z}q(\b Z)\log\probof{\b X}[\fb\theta,\b Z]\,d\b Z = \int_{\c Z}q(\b Z)\log\frac{p_{\fb\theta}(\b X,\b Z)}{q(\b Z)}\,d\b Z $$
Let us denote this

\bdefn

    Define the {\emphcolor elbo} function as
    $$ \c L(q,\fb\theta) = \expecof{\log\probof{\b X}[\fb\theta]} = \int_{\c Z}q(\b Z)\log\frac{p_{\fb\theta}(\b X,\b Z)}{q(\b Z)}\,d\b Z $$

\edefn

\noindent Notice that by Jensen
$$ \c L(q,\fb\theta) \leq \log\expecof{\probof{\b X}[\fb\theta]} = \log\int_{\c Z}q(\b Z)\probof{\b X}[\fb\theta,\b Z]\,d\b Z = \log\int_{\c Z}p_{\fb\theta}(\b X,\b Z)\,d\b Z =
\log\probof{\b X}[\fb\theta] $$
So the elbo function is a lower bound for $\log\probof{\b X}[\fb\theta]$, thus maximizing it should give us an approximation for the maximum of $\log\probof{\b X}[\fb\theta]$.

Also notice that
$$ \eqalign{
    \c L(q,\fb\theta) &= \int_{\c Z}q(\b Z)\log\frac{p_{\fb\theta}(\b X,\b Z)}{q(\b Z)}\,d\b Z = \int_{\c Z}q(\b Z)\log\frac{\probof{\b Z}[\b X,\fb\theta]\probof{\b X}[\fb\theta]}{q(\b Z)}\,d\b Z\cr
    &= \int_{\c Z}q(\b Z)\log\frac{\probof{\b Z}[\b X,\fb\theta]}{q(\b Z)}\,d\b Z + \int_{\c Z}q(\b Z)\log\probof{\b X}[\fb\theta]\,d\b Z\cr
    &= -\klof{\b Z,(\b Z\,|\,\b X,\fb\theta)} + \log\probof{\b X}[\fb\theta]\cr
} $$
Similarly we have
$$ \c L(q,\fb\theta) = \int_{\c Z}q(\b Z)\log\frac{p_{\fb\theta}(\b X,\b Z)}{q(\b Z)}\,d\b Z = \int_{\c Z}q(\b Z)\log\bigl(p_{\fb\theta}(\b X,\b Z)\bigr) - \int_{\c Z}q(\b Z)\log q(\b Z) 
= \int_{\c Z}q(\b Z)\log\bigl(p_{\fb\theta}(\b X,\b Z)\bigr) - H(q) $$ 
The expectation-maximization (EM) algorithm works in two steps: the E step and the M step.
First in the E-step we maximize $\c L(q,\fb\theta)$ with respect to $q$, holding $\fb\theta$ constant.
In the M-step we maximize $\c L(q,\fb\theta)$ with respect to $\fb\theta$, holding $q$ constant.

\benum
    \item E-step: we want to maximize $-\klof{\b Z,(\b Z\,|\,\b X,\fb\theta)} + \log\probof{\b X}[\fb\theta]$ with respect to $q$.
    Notice that the second term is independent on $q$, and the first term is non-positive.
    So $\c L(q,\fb\theta)$ is maximized when the first term is zero, which is when $\b Z$ distributes the same as $\b Z\,|\,\b X,\fb\theta$, i.e. $q(\b Z)=\probof{\b Z}[\b X,\fb\theta]$.
    And in such a case we have $\max_q\c L(q,\fb\theta)=\log\probof{\b X}[\fb\theta]$.
    \item M-step: we want to maximize $\int_{\c Z}q(\b Z)\log\bigl(p_{\fb\theta}(\b X,\b Z)\bigr)-H(q)$ with respect to $\fb\theta$.
    Since $H(q)$ does not depend on $\fb\theta$, we want to maximize $\int_{\c Z}q(\b Z)\log p_{\fb\theta}(\b X,\b Z)$.
    This can be easier than optimizing $\probof{\b X}[\fb\theta]$ if $p_{\fb\theta}$ is a nice function.
\eenum

Since at each E-step we have $\c L(q,\fb\theta)=\log\probof{\b X}[\fb\theta]$, and at each M-step we increase $\c L(q,\fb\theta)$, it follows that $\probof{\b X}[\fb\theta]$ is increased each iteration.

\bexam

    Suppose we have two biased coins, one with a bias of $\theta_A$ and the other with a bias of $\theta_B$.
    Meaning when we flip coin $A$, its result distributes $\Berof{\theta_A}$.
    Now suppose we get a sequence of flips, where the sequence is constructed as follows: we choose a coin randomly with equal probability, then flip it ten times, and repeat this process five times total.
    Can we estimate $\theta_A,\theta_B$?
    If we know which coin was flipped for each flip then this is simple (suppose coin $A$ gave $24$ heads and $6$ tails, then $\theta_A$ can be approximated as $24/(24+6)$, this is the MLE estimator).
    But suppose we don't know which coin was flipped, then we have a latent variable $Z$ which has a bernoulli distribution which determines which coin we flip.

    Our space $\c X$ consists of $5$-tuples of $10$-tuples of flips (i.e. $(\set{0,1}^{10})^5$), and $\c Z$ is $\set{A,B}^5$.
    Suppose our flips are (ignoring the order):
    
    \vfill\break
    \centerline{\vbox{\tabskip=.25cm\halign{\strut$\b X_{#}$\hfil\ \vrule\tabskip=.25cm&#\hfil\tabskip=.25cm&#\hfil\cr
        \omit\strut Flip \#\hfil\ \vrule& \# Heads & \# Tails\cr\noalign{\hrule}
        1 & 5 & 5\cr
        2 & 9 & 1\cr
        3 & 8 & 2\cr
        4 & 4 & 6\cr
        5 & 7 & 3\cr
    }}}

    So we want to maximize $\c L(q,\fb\theta)=\sum_{\b Z}q(\b Z)\log\frac{p_\theta(\b X,\b Z)}{q(\b Z)}$.
    We begin with an estimate for $\fb\theta$: $\fb\theta^0$ which we can choose randomly.
    Suppose we choose $\theta_A^0=0.6$ and $\theta_B^0=0.5$.
    Now we want to compute $\probof{\b Z}[\b X,\fb\theta^0]$ for the E-step.

    Note that in the first row we have $5$ heads and $5$ tails, the probability of this occurring with a bias of $\theta^0_A=0.6$ is $0.6^5\cdot0.4^5\approx0.0008$ and the probability of this occurring with
    a bias of $\theta^0_B=0.5$ is $0.5^5\cdot0.5^5\approx0.001$.
    That is $\probof{\b X_1}[\b Z_1=A,\fb\theta^0]=0.0008$ and $\probof{\b X_2}[\b Z_1=B,\fb\theta^0]=0.001$.
    Now we know that $\probof{\b Z_1=A}=\probof{\b Z_1=B}=0.5$ (and this is independent of $\fb\theta$) so we get that
    $$ \probof{\b Z_1=A}[\b X_1,\fb\theta^0] = \probof{\b X_1}[\b Z_1=A,\fb\theta] \cdot \frac{\probof{\b Z_1=A}[\fb\theta^0]}{\probof{\b X_1}[\fb\theta^0]} $$
    Now we know that
    $$ \probof{\b X_1}[\fb\theta^0] = \sum_{i=A,B}\probof{\b X_1}[\b Z_1=i,\fb\theta^0]\cdot\probof{\b Z_1=i} = 0.5(0.0008 + 0.001) $$
    So we get that
    $$ \probof{\b Z_1=A}[\b X_1,\fb\theta^0] = \frac{0.0008\cdot0.5}{0.5(0.0008+0.001)} = \frac{0.0008}{0.0008 + 0.001} \approx 0.45 $$
    And similarly
    $$ \probof{\b Z_1=B}[\b X_1,\fb\theta^0] = \frac{0.001}{0.0008 + 0.001} \approx 0.55 $$
    Continuing this computation we get

    \centerline{\vbox{\tabskip=.25cm\halign{\strut$#$\hfil\ \vrule&$#$\hfil&$#$\hfil\cr
        i& \probof{\b Z_i=A}[\b X_i,\fb\theta^0] & \probof{\b Z_i=B}[\b X_i,\fb\theta^0]\cr\noalign{\hrule}
        1 & 0.45 & 0.55\cr
        2 & 0.8 & 0.2\cr
        3 & 0.73 & 0.27\cr
        4 & 0.35 & 0.65\cr
        5 & 0.65 & 0.35\cr
    }}}

    So what this tells us is that we can view $0.45$ of the first ten flips to belong to coin A and $0.55$ of the first ten flips to belong to coin B and so on.
    Since in the first ten flips we have $5$ heads and $5$ tails, $0.45\cdot5\approx2.2$ of them are from coin A, etc.
    So we get the following table

    \centerline{\vbox{\tabskip=.25cm\halign{\strut#\hfil\ \vrule&&\hfil#\cr
        Flip \# & \omit\span\hfil Coin A\hfil &\omit\span \hfil Coin B\hfil\cr\noalign{\hrule}
        1 & 2.2 H& 2.2 T & 2.8 H& 2.8 T\cr
        2 & 7.2 H& 0.8 T & 1.8 H& 0.2 T\cr
        3 & 5.9 H& 1.5 T & 2.1 H& 0.5 T\cr
        4 & 1.4 H& 2.1 T & 2.6 H& 3.9 T\cr
        5 & 4.5 H& 1.9 T & 2.5 H& 1.1 T\cr\noalign{\hrule}
        Total &21.3 H&8.6 T &11.7 H& 8.4 T\cr
    }}}

    For the M-step we will use the MLE on these values.
    We get that
    $$ \theta_A^1 = \frac{21.3}{21.3 + 8.6} \approx 0.71,\qquad \theta_B^1 = \frac{11.7}{11.7 + 8.4} \approx 0.58 $$
    And then we continue the next iteration with $\fb\theta^1$.

\eexam

